2023-01-07 09:05:24,434 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:05:24,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:24,470 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:24,470 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:24,470 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 09:05:24,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,188 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,188 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,188 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 09:05:25,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,190 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,190 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,190 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 09:05:25,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,191 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,191 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,191 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 09:05:25,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,192 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,192 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,192 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 09:05:25,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,227 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,227 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,227 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 09:05:25,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,228 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,229 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,229 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 09:05:25,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,230 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,230 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,230 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 09:05:25,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,231 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,231 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,231 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 09:05:25,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,231 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,232 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,232 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 09:05:25,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,232 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,233 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,233 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 09:05:25,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,233 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,233 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,233 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 09:05:25,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,234 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,234 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,234 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 09:05:25,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,235 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,235 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,235 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 09:05:25,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,236 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,236 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,236 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 09:05:25,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,237 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,237 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,237 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 09:05:25,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,238 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,238 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,238 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 09:05:25,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,239 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,239 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,239 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 09:05:25,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,240 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,240 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,240 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 09:05:25,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,241 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,241 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,241 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 09:05:25,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,242 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,242 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,242 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 09:05:25,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,243 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,243 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,243 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 09:05:25,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,244 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,244 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,244 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 09:05:25,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,245 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,245 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,245 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 09:05:25,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,246 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,246 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,246 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 09:05:25,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,247 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,247 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,247 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 09:05:25,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,248 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,248 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,248 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 09:05:25,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,249 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,249 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,249 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 09:05:25,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,250 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,250 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,250 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 09:05:25,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,251 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,251 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,251 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 09:05:25,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,252 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,252 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,252 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 09:05:25,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,253 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,253 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,253 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 09:05:25,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,254 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,254 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,254 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 09:05:25,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,255 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,255 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,255 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 09:05:25,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,256 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,256 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,256 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 09:05:25,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,256 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,257 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,257 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 09:05:25,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,258 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,258 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,258 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 09:05:25,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,258 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,258 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,259 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 09:05:25,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,260 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,260 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,260 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 09:05:25,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,260 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,260 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,260 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 09:05:25,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,261 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,261 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,261 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 09:05:25,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,262 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,262 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,262 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 09:05:25,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,263 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,263 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,263 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 09:05:25,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,264 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,264 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,264 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 09:05:25,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,265 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,265 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,265 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 09:05:25,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,266 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,266 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,266 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 09:05:25,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,267 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,267 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,267 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 09:05:25,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,268 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,268 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,268 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 09:05:25,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,269 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,269 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,269 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 09:05:25,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,270 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,270 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,270 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 09:05:25,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,271 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,271 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,271 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 09:05:25,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,272 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,272 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,272 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 09:05:25,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,273 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,273 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,273 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 09:05:25,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,274 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,274 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,274 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 09:05:25,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,274 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,274 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,275 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 09:05:25,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,275 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,275 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,275 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 09:05:25,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,276 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,276 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,276 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 09:05:25,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,277 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,277 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,277 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 09:05:25,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,278 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,278 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,278 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 09:05:25,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,279 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,279 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,279 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 09:05:25,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,280 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,280 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,280 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 09:05:25,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,281 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,281 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,281 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 09:05:25,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,282 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,282 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,282 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 09:05:25,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,283 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,283 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,283 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 09:05:25,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,284 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,284 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,284 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 09:05:25,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,285 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,285 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,285 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 09:05:25,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,286 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,286 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,286 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 09:05:25,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,286 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,286 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,286 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 09:05:25,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,287 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,287 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,287 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 09:05:25,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,288 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,288 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,288 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 09:05:25,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,289 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,289 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,289 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 09:05:25,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,290 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,290 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,290 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 09:05:25,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,291 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,291 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,291 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 09:05:25,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,292 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,292 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,292 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 09:05:25,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,293 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,293 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,293 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 09:05:25,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,294 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,294 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,294 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 09:05:25,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,295 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,295 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,295 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 09:05:25,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,295 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,295 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,295 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 09:05:25,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,296 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,296 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,296 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 09:05:25,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,297 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,297 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,297 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 09:05:25,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,298 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,298 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,298 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 09:05:25,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,299 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,299 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,299 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 09:05:25,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,300 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,300 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,300 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 09:05:25,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,301 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,301 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,301 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 09:05:25,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,302 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,302 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,302 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 09:05:25,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,302 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,303 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,303 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 09:05:25,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,304 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,304 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,304 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 09:05:25,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,304 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,304 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,304 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 09:05:25,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,305 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,305 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,305 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 09:05:25,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,306 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,306 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,306 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 09:05:25,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,307 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,307 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,307 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 09:05:25,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,308 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,308 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,308 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 09:05:25,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,309 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,309 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,309 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 09:05:25,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,310 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,310 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,310 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 09:05:25,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,311 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,311 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,311 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 09:05:25,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,312 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,312 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,312 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 09:05:25,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,313 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,313 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,313 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 09:05:25,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,314 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,314 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,314 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 09:05:25,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,314 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,315 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,315 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 09:05:25,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,315 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,315 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,316 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 09:05:25,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,316 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,316 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,316 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 09:05:25,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,317 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,317 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,317 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 09:05:25,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,318 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,318 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,318 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 09:05:25,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,319 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,319 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,319 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 09:05:25,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,320 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,320 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,320 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 09:05:25,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,321 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,321 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,321 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 09:05:25,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:05:25,322 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:25,322 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:05:25,322 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 09:05:25,323 > [DEBUG] 0 :: 7.102209091186523
2023-01-07 09:05:25,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,327 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,327 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0233154296875
2023-01-07 09:05:25,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -363.87066650390625
2023-01-07 09:05:25,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,331 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,331 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.23943135142326355
2023-01-07 09:05:25,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,332 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -199.63702392578125
2023-01-07 09:05:25,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -342.74822998046875
2023-01-07 09:05:25,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,341 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,341 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -2.1992530822753906
2023-01-07 09:05:25,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.016813039779663086
2023-01-07 09:05:25,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,342 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,343 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.028796354308724403
2023-01-07 09:05:25,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,343 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -162.34396362304688
2023-01-07 09:05:25,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9329181909561157
2023-01-07 09:05:25,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,345 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,345 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3.126215934753418
2023-01-07 09:05:25,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005256236530840397
2023-01-07 09:05:25,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,346 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,346 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.06889761984348297
2023-01-07 09:05:25,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,346 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -362.9087219238281
2023-01-07 09:05:25,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7596614956855774
2023-01-07 09:05:25,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,347 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,347 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7.581072807312012
2023-01-07 09:05:25,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12773528695106506
2023-01-07 09:05:25,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,348 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,348 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.11430400609970093
2023-01-07 09:05:25,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,349 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -405.34967041015625
2023-01-07 09:05:25,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8635971546173096
2023-01-07 09:05:25,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,350 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,350 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -5.249309062957764
2023-01-07 09:05:25,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6599739789962769
2023-01-07 09:05:25,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,350 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,351 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.07533429563045502
2023-01-07 09:05:25,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,351 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -212.3069305419922
2023-01-07 09:05:25,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2741811275482178
2023-01-07 09:05:25,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,352 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,352 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 27.54668426513672
2023-01-07 09:05:25,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39860790967941284
2023-01-07 09:05:25,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,353 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,353 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.09555405378341675
2023-01-07 09:05:25,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,353 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 27.54668426513672
2023-01-07 09:05:25,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3098524510860443
2023-01-07 09:05:25,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,354 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,354 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.247152328491211
2023-01-07 09:05:25,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8778071999549866
2023-01-07 09:05:25,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,355 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,355 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.07042206823825836
2023-01-07 09:05:25,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,356 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -503.241455078125
2023-01-07 09:05:25,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11360228061676025
2023-01-07 09:05:25,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,357 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,357 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 12.847424507141113
2023-01-07 09:05:25,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.776761531829834
2023-01-07 09:05:25,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,358 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,358 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1350473016500473
2023-01-07 09:05:25,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,358 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -503.16387939453125
2023-01-07 09:05:25,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.180330753326416
2023-01-07 09:05:25,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,359 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,359 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 8.957572937011719
2023-01-07 09:05:25,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7324082851409912
2023-01-07 09:05:25,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,360 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,360 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.053535133600234985
2023-01-07 09:05:25,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,360 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -257.1473388671875
2023-01-07 09:05:25,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.880110263824463
2023-01-07 09:05:25,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,362 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,362 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 5.222621917724609
2023-01-07 09:05:25,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30516427755355835
2023-01-07 09:05:25,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,363 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,363 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.16551369428634644
2023-01-07 09:05:25,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,363 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -279.7253112792969
2023-01-07 09:05:25,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.2901787757873535
2023-01-07 09:05:25,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,365 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,365 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.676584243774414
2023-01-07 09:05:25,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004998490214347839
2023-01-07 09:05:25,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,366 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,366 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.03446776047348976
2023-01-07 09:05:25,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,366 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -379.3546142578125
2023-01-07 09:05:25,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0624111145734787
2023-01-07 09:05:25,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,367 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.541527271270752
2023-01-07 09:05:25,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20370978116989136
2023-01-07 09:05:25,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,368 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,368 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.0011730492115020752
2023-01-07 09:05:25,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,368 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -255.34542846679688
2023-01-07 09:05:25,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9037575721740723
2023-01-07 09:05:25,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,370 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 13.362224578857422
2023-01-07 09:05:25,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4100409150123596
2023-01-07 09:05:25,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,371 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.23788027465343475
2023-01-07 09:05:25,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -240.60833740234375
2023-01-07 09:05:25,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49798765778541565
2023-01-07 09:05:25,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,372 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 5.3547186851501465
2023-01-07 09:05:25,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4029298424720764
2023-01-07 09:05:25,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,373 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.22076267004013062
2023-01-07 09:05:25,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -355.8108825683594
2023-01-07 09:05:25,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2866140604019165
2023-01-07 09:05:25,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,374 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 5.4065093994140625
2023-01-07 09:05:25,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.48416560888290405
2023-01-07 09:05:25,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,375 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.037322938442230225
2023-01-07 09:05:25,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -251.30044555664062
2023-01-07 09:05:25,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9143000245094299
2023-01-07 09:05:25,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,377 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 9.786354064941406
2023-01-07 09:05:25,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29371023178100586
2023-01-07 09:05:25,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,378 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.057270973920822144
2023-01-07 09:05:25,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -237.88967895507812
2023-01-07 09:05:25,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3331685066223145
2023-01-07 09:05:25,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,379 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -7.233682632446289
2023-01-07 09:05:25,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5925500392913818
2023-01-07 09:05:25,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,380 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.02945469319820404
2023-01-07 09:05:25,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -359.97491455078125
2023-01-07 09:05:25,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6940792798995972
2023-01-07 09:05:25,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,381 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 3.2542335987091064
2023-01-07 09:05:25,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5935215950012207
2023-01-07 09:05:25,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,382 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.1389205902814865
2023-01-07 09:05:25,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -246.5437469482422
2023-01-07 09:05:25,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23932909965515137
2023-01-07 09:05:25,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,384 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 42.505470275878906
2023-01-07 09:05:25,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7429462671279907
2023-01-07 09:05:25,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,384 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.022182419896125793
2023-01-07 09:05:25,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -206.82693481445312
2023-01-07 09:05:25,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.370121955871582
2023-01-07 09:05:25,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,386 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.4083585739135742
2023-01-07 09:05:25,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33693933486938477
2023-01-07 09:05:25,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,387 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.139268159866333
2023-01-07 09:05:25,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -349.2897644042969
2023-01-07 09:05:25,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6811836957931519
2023-01-07 09:05:25,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,388 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,388 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.2373056411743164
2023-01-07 09:05:25,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4096033275127411
2023-01-07 09:05:25,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,389 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.11562687158584595
2023-01-07 09:05:25,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -233.3858184814453
2023-01-07 09:05:25,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.021573781967163
2023-01-07 09:05:25,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,390 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.136938095092773
2023-01-07 09:05:25,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1551176309585571
2023-01-07 09:05:25,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,391 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.21690057218074799
2023-01-07 09:05:25,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -189.80776977539062
2023-01-07 09:05:25,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.873449325561523
2023-01-07 09:05:25,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,392 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 18.273895263671875
2023-01-07 09:05:25,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.397649347782135
2023-01-07 09:05:25,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,393 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.6762872934341431
2023-01-07 09:05:25,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,394 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -304.5385437011719
2023-01-07 09:05:25,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.073676347732544
2023-01-07 09:05:25,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,395 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -10.766319274902344
2023-01-07 09:05:25,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.489316463470459
2023-01-07 09:05:25,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,396 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,396 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.2707584500312805
2023-01-07 09:05:25,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,396 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -196.06704711914062
2023-01-07 09:05:25,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005898982286453247
2023-01-07 09:05:25,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,397 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 57.006832122802734
2023-01-07 09:05:25,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5615895986557007
2023-01-07 09:05:25,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,398 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -126.16070556640625
2023-01-07 09:05:25,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2452893257141113
2023-01-07 09:05:25,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,399 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,399 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 33.678810119628906
2023-01-07 09:05:25,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2224459648132324
2023-01-07 09:05:25,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,400 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,400 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.2052035331726074
2023-01-07 09:05:25,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,401 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -248.82272338867188
2023-01-07 09:05:25,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.399552345275879
2023-01-07 09:05:25,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,402 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 6.174324989318848
2023-01-07 09:05:25,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.473564863204956
2023-01-07 09:05:25,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,403 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7549589276313782
2023-01-07 09:05:25,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -106.10099792480469
2023-01-07 09:05:25,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0470834970474243
2023-01-07 09:05:25,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,404 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,404 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 33.93189239501953
2023-01-07 09:05:25,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04503670334815979
2023-01-07 09:05:25,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,405 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.1237793117761612
2023-01-07 09:05:25,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -104.8665542602539
2023-01-07 09:05:25,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.77294921875
2023-01-07 09:05:25,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,407 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,407 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 61.013267517089844
2023-01-07 09:05:25,407 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.302628755569458
2023-01-07 09:05:25,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,408 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -72.3058090209961
2023-01-07 09:05:25,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7580816149711609
2023-01-07 09:05:25,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,409 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,409 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -44.47346878051758
2023-01-07 09:05:25,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8075656890869141
2023-01-07 09:05:25,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -224.09872436523438
2023-01-07 09:05:25,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.699891090393066
2023-01-07 09:05:25,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,411 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,411 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -0.1343250274658203
2023-01-07 09:05:25,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.819071888923645
2023-01-07 09:05:25,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -127.33438110351562
2023-01-07 09:05:25,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.338728904724121
2023-01-07 09:05:25,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,413 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,413 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 79.43632507324219
2023-01-07 09:05:25,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.151410102844238
2023-01-07 09:05:25,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,414 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -47.062782287597656
2023-01-07 09:05:25,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1461334228515625
2023-01-07 09:05:25,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,415 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,415 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 13.907672882080078
2023-01-07 09:05:25,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.042506217956543
2023-01-07 09:05:25,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,416 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -175.6147003173828
2023-01-07 09:05:25,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6486126184463501
2023-01-07 09:05:25,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,417 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,417 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -5.3245110511779785
2023-01-07 09:05:25,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3501478135585785
2023-01-07 09:05:25,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,418 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -131.4146270751953
2023-01-07 09:05:25,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8130710124969482
2023-01-07 09:05:25,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,419 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,419 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 23.146316528320312
2023-01-07 09:05:25,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.179450035095215
2023-01-07 09:05:25,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,420 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -109.88887786865234
2023-01-07 09:05:25,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.613075256347656
2023-01-07 09:05:25,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,421 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,421 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 26.37493133544922
2023-01-07 09:05:25,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.513834476470947
2023-01-07 09:05:25,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,422 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,422 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.7792770266532898
2023-01-07 09:05:25,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,422 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -196.7453155517578
2023-01-07 09:05:25,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8589487075805664
2023-01-07 09:05:25,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,423 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -13.24463939666748
2023-01-07 09:05:25,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.719488143920898
2023-01-07 09:05:25,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -175.05905151367188
2023-01-07 09:05:25,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7557969093322754
2023-01-07 09:05:25,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,425 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,426 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -20.93495750427246
2023-01-07 09:05:25,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.877776622772217
2023-01-07 09:05:25,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,427 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -191.59153747558594
2023-01-07 09:05:25,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.57928466796875
2023-01-07 09:05:25,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,428 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,428 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -24.013980865478516
2023-01-07 09:05:25,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.648242950439453
2023-01-07 09:05:25,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,429 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,429 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.7392725944519043
2023-01-07 09:05:25,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,429 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -273.0406799316406
2023-01-07 09:05:25,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0690314769744873
2023-01-07 09:05:25,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,430 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,430 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.088863372802734
2023-01-07 09:05:25,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.814751148223877
2023-01-07 09:05:25,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,431 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -98.66703796386719
2023-01-07 09:05:25,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.131399154663086
2023-01-07 09:05:25,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,432 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,432 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -4.8087005615234375
2023-01-07 09:05:25,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5746781826019287
2023-01-07 09:05:25,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,433 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -173.08685302734375
2023-01-07 09:05:25,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.654756546020508
2023-01-07 09:05:25,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,434 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,435 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 186.48239135742188
2023-01-07 09:05:25,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.014554977416992
2023-01-07 09:05:25,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,435 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,435 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.4065079092979431
2023-01-07 09:05:25,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,436 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.461074829101562
2023-01-07 09:05:25,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.463687896728516
2023-01-07 09:05:25,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -766.3103637695312
2023-01-07 09:05:25,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.79447078704834
2023-01-07 09:05:25,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,438 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,438 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.5618388652801514
2023-01-07 09:05:25,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,438 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,438 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -47.160743713378906
2023-01-07 09:05:25,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,438 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1125.2548828125
2023-01-07 09:05:25,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.806819915771484
2023-01-07 09:05:25,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,440 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1221.637451171875
2023-01-07 09:05:25,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.13496208190918
2023-01-07 09:05:25,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,441 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -907.1405029296875
2023-01-07 09:05:25,441 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.467042922973633
2023-01-07 09:05:25,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,442 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,442 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 309.95172119140625
2023-01-07 09:05:25,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 68.85045623779297
2023-01-07 09:05:25,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,443 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,443 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.082258939743042
2023-01-07 09:05:25,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,444 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 92.03811645507812
2023-01-07 09:05:25,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.98014831542969
2023-01-07 09:05:25,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,445 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -482.6320495605469
2023-01-07 09:05:25,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.834468841552734
2023-01-07 09:05:25,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,446 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,446 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.0002602338790894
2023-01-07 09:05:25,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,446 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,446 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 39.469058990478516
2023-01-07 09:05:25,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.29606628417969
2023-01-07 09:05:25,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,447 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -741.7694702148438
2023-01-07 09:05:25,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.549508094787598
2023-01-07 09:05:25,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,448 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,448 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 14.988929748535156
2023-01-07 09:05:25,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1443.8046875
2023-01-07 09:05:25,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.93642044067383
2023-01-07 09:05:25,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,449 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -913.30615234375
2023-01-07 09:05:25,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.745183944702148
2023-01-07 09:05:25,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,450 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,450 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -9.380165100097656
2023-01-07 09:05:25,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.19974899291992
2023-01-07 09:05:25,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,451 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1088.78515625
2023-01-07 09:05:25,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 85.54837036132812
2023-01-07 09:05:25,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,452 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,452 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -2.2670350074768066
2023-01-07 09:05:25,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,452 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,452 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -7.221967697143555
2023-01-07 09:05:25,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,453 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1323.201416015625
2023-01-07 09:05:25,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1625.126708984375
2023-01-07 09:05:25,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.989029884338379
2023-01-07 09:05:25,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,454 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1462.69921875
2023-01-07 09:05:25,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.172487258911133
2023-01-07 09:05:25,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,456 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,456 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -178.53775024414062
2023-01-07 09:05:25,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.18067932128906
2023-01-07 09:05:25,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,457 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1511.990234375
2023-01-07 09:05:25,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.369516372680664
2023-01-07 09:05:25,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,458 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -860.419189453125
2023-01-07 09:05:25,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,458 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1649.5068359375
2023-01-07 09:05:25,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -117.40609741210938
2023-01-07 09:05:25,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,459 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,459 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 210.5579833984375
2023-01-07 09:05:25,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.93092346191406
2023-01-07 09:05:25,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,460 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,460 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -3.5609686374664307
2023-01-07 09:05:25,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,461 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 139.72698974609375
2023-01-07 09:05:25,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17050743103027344
2023-01-07 09:05:25,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,462 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -558.5252685546875
2023-01-07 09:05:25,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.592464447021484
2023-01-07 09:05:25,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,463 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1010.7669067382812
2023-01-07 09:05:25,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.161027908325195
2023-01-07 09:05:25,471 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:05:25,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,471 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:25,471 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 4139.96533203125
2023-01-07 09:05:25,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,471 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1433.151611328125
2023-01-07 09:05:25,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,472 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1672.457275390625
2023-01-07 09:05:25,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,472 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -971.0958862304688
2023-01-07 09:05:25,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,472 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -700.77734375
2023-01-07 09:05:25,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,472 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -947.9360961914062
2023-01-07 09:05:25,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -962.9689331054688
2023-01-07 09:05:25,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -823.3934936523438
2023-01-07 09:05:25,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -656.1469116210938
2023-01-07 09:05:25,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,474 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -809.0530395507812
2023-01-07 09:05:25,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,474 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -573.3741455078125
2023-01-07 09:05:25,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,474 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -556.88720703125
2023-01-07 09:05:25,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,474 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -735.72021484375
2023-01-07 09:05:25,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,475 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -691.2246704101562
2023-01-07 09:05:25,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -713.4649047851562
2023-01-07 09:05:25,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -753.127197265625
2023-01-07 09:05:25,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -956.8754272460938
2023-01-07 09:05:25,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -665.29150390625
2023-01-07 09:05:25,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -955.6712646484375
2023-01-07 09:05:25,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -809.190185546875
2023-01-07 09:05:25,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -986.3848876953125
2023-01-07 09:05:25,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -916.7750244140625
2023-01-07 09:05:25,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -861.43798828125
2023-01-07 09:05:25,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1028.4105224609375
2023-01-07 09:05:25,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -979.4140625
2023-01-07 09:05:25,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -930.1229248046875
2023-01-07 09:05:25,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 5090.9609375
2023-01-07 09:05:25,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1055.1824951171875
2023-01-07 09:05:25,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -993.8816528320312
2023-01-07 09:05:25,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -960.1334228515625
2023-01-07 09:05:25,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1056.6842041015625
2023-01-07 09:05:25,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -994.043212890625
2023-01-07 09:05:25,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -967.9187622070312
2023-01-07 09:05:25,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1061.563720703125
2023-01-07 09:05:25,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,481 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1016.4871826171875
2023-01-07 09:05:25,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,482 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1017.3604125976562
2023-01-07 09:05:25,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.34241485595703
2023-01-07 09:05:25,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -781.634765625
2023-01-07 09:05:25,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -996.98876953125
2023-01-07 09:05:25,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -998.740478515625
2023-01-07 09:05:25,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -954.6646118164062
2023-01-07 09:05:25,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.18812942504883
2023-01-07 09:05:25,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,484 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -744.5270385742188
2023-01-07 09:05:25,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,484 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -900.0194702148438
2023-01-07 09:05:25,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1013.1504516601562
2023-01-07 09:05:25,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5916414260864258
2023-01-07 09:05:25,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -707.8209228515625
2023-01-07 09:05:25,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:25,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:25,485 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -350.32269287109375
2023-01-07 09:05:25,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 703.3070068359375
2023-01-07 09:05:26,343 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -1061.6173095703125 param sum :: 0.22188237309455872
2023-01-07 09:05:26,343 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:26,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,343 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,344 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -4.273125171661377
2023-01-07 09:05:26,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,344 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1715.416748046875
2023-01-07 09:05:26,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 203.07241821289062
2023-01-07 09:05:26,345 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -2.5994873046875 param sum :: 64.0
2023-01-07 09:05:26,345 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,345 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 5.294054985046387
2023-01-07 09:05:26,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,345 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6439666748046875
2023-01-07 09:05:26,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1762.847900390625
2023-01-07 09:05:26,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2755584716796875
2023-01-07 09:05:26,347 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 48.01000213623047 param sum :: 5.294054985046387
2023-01-07 09:05:26,347 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:26,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,347 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1761.9483642578125
2023-01-07 09:05:26,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -91.0488052368164
2023-01-07 09:05:26,348 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -12.993379592895508 param sum :: 64.0
2023-01-07 09:05:26,348 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:26,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,349 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1983.63232421875
2023-01-07 09:05:26,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.85887145996094
2023-01-07 09:05:26,350 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -1985.4456787109375 param sum :: 5.982826232910156
2023-01-07 09:05:26,350 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:26,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,350 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,350 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -5.059881210327148
2023-01-07 09:05:26,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,350 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,350 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -6.493122100830078
2023-01-07 09:05:26,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,351 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1605.360595703125
2023-01-07 09:05:26,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.8270149230957
2023-01-07 09:05:26,352 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -7.363888740539551 param sum :: 64.0
2023-01-07 09:05:26,352 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:26,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,352 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1648.0068359375
2023-01-07 09:05:26,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.51078796386719
2023-01-07 09:05:26,353 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -134.724853515625 param sum :: 11.617742538452148
2023-01-07 09:05:26,353 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:26,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,354 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 2.1933841705322266
2023-01-07 09:05:26,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -578.376220703125
2023-01-07 09:05:26,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.72945785522461
2023-01-07 09:05:26,355 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -1.9985761642456055 param sum :: 256.0
2023-01-07 09:05:26,355 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:26,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,355 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -481.670654296875
2023-01-07 09:05:26,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 90.52774047851562
2023-01-07 09:05:26,356 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1564.34130859375 param sum :: -10.471250534057617
2023-01-07 09:05:26,356 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:26,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -554.8446655273438
2023-01-07 09:05:26,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.439231872558594
2023-01-07 09:05:26,358 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -31.157459259033203 param sum :: 256.0
2023-01-07 09:05:26,358 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:26,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,358 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -578.567138671875
2023-01-07 09:05:26,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.799631118774414
2023-01-07 09:05:26,359 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -486.734619140625 param sum :: -38.087059020996094
2023-01-07 09:05:26,359 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:26,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,359 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 2.4737725257873535
2023-01-07 09:05:26,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -534.54541015625
2023-01-07 09:05:26,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.44366455078125
2023-01-07 09:05:26,361 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.37518978118896484 param sum :: 64.0
2023-01-07 09:05:26,361 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:26,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,362 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -487.6098937988281
2023-01-07 09:05:26,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,362 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1976.1788330078125
2023-01-07 09:05:26,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.00434112548828
2023-01-07 09:05:26,363 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -500.19537353515625 param sum :: -9.83746337890625
2023-01-07 09:05:26,363 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:26,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,363 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.9651751518249512
2023-01-07 09:05:26,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -610.7056274414062
2023-01-07 09:05:26,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.16746139526367
2023-01-07 09:05:26,365 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 2.4665794372558594 param sum :: 64.0
2023-01-07 09:05:26,365 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:26,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,365 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1669.07666015625
2023-01-07 09:05:26,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6728687286376953
2023-01-07 09:05:26,366 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -576.250244140625 param sum :: 13.821467399597168
2023-01-07 09:05:26,366 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:26,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -734.3123779296875
2023-01-07 09:05:26,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.335615158081055
2023-01-07 09:05:26,368 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 12.667900085449219 param sum :: 256.0
2023-01-07 09:05:26,368 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:26,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,368 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -745.6917724609375
2023-01-07 09:05:26,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,368 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1731.9449462890625
2023-01-07 09:05:26,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.234574317932129
2023-01-07 09:05:26,369 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -730.5912475585938 param sum :: -16.297849655151367
2023-01-07 09:05:26,369 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:26,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,370 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1777.319580078125
2023-01-07 09:05:26,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.127518653869629
2023-01-07 09:05:26,370 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -20.65947723388672 param sum :: 64.0
2023-01-07 09:05:26,370 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:26,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,371 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1716.3963623046875
2023-01-07 09:05:26,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.14598846435547
2023-01-07 09:05:26,372 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -1716.339111328125 param sum :: -1.7540884017944336
2023-01-07 09:05:26,372 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:26,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,372 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,372 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -1.0424144268035889
2023-01-07 09:05:26,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,372 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,372 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 11.7921142578125
2023-01-07 09:05:26,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,373 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1867.02978515625
2023-01-07 09:05:26,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.082403182983398
2023-01-07 09:05:26,374 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -1.112406611442566 param sum :: 64.0
2023-01-07 09:05:26,374 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:26,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1971.515625
2023-01-07 09:05:26,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.962562561035156
2023-01-07 09:05:26,375 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -21.314363479614258 param sum :: -0.8034350872039795
2023-01-07 09:05:26,376 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,376 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -17.25152015686035
2023-01-07 09:05:26,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1965.42236328125
2023-01-07 09:05:26,376 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.617450714111328
2023-01-07 09:05:26,377 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -65.74671936035156 param sum :: 256.0
2023-01-07 09:05:26,377 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:26,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1948.8170166015625
2023-01-07 09:05:26,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.954692840576172
2023-01-07 09:05:26,378 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1975.2694091796875 param sum :: -17.25152015686035
2023-01-07 09:05:26,378 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:26,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,379 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1985.080810546875
2023-01-07 09:05:26,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.968449354171753
2023-01-07 09:05:26,380 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -40.276527404785156 param sum :: 128.0
2023-01-07 09:05:26,380 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:26,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,380 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2008.010009765625
2023-01-07 09:05:26,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.19438362121582
2023-01-07 09:05:26,381 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -1953.7608642578125 param sum :: -10.020125389099121
2023-01-07 09:05:26,381 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,382 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.8941804766654968
2023-01-07 09:05:26,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -999.3153076171875
2023-01-07 09:05:26,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.056657791137695
2023-01-07 09:05:26,383 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.4417407512664795 param sum :: 128.0
2023-01-07 09:05:26,383 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -4.991181373596191
2023-01-07 09:05:26,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,383 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1020.27880859375
2023-01-07 09:05:26,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8671271800994873
2023-01-07 09:05:26,384 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -1029.4854736328125 param sum :: -4.991181373596191
2023-01-07 09:05:26,384 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:26,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,385 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,385 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7484780550003052
2023-01-07 09:05:26,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,385 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -654.7416381835938
2023-01-07 09:05:26,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.02235221862793
2023-01-07 09:05:26,386 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.6496423482894897 param sum :: 512.0
2023-01-07 09:05:26,386 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -18.34212875366211
2023-01-07 09:05:26,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,386 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -673.4241943359375
2023-01-07 09:05:26,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.659857273101807
2023-01-07 09:05:26,387 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -680.425537109375 param sum :: -18.34212875366211
2023-01-07 09:05:26,387 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:26,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,388 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -926.7616577148438
2023-01-07 09:05:26,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.692251682281494
2023-01-07 09:05:26,389 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -126.05240631103516 param sum :: 512.0
2023-01-07 09:05:26,389 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:26,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,389 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -923.8704833984375
2023-01-07 09:05:26,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.667520999908447
2023-01-07 09:05:26,390 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -916.22509765625 param sum :: 37.5141487121582
2023-01-07 09:05:26,390 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,390 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,390 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.3320736885070801
2023-01-07 09:05:26,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,391 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -982.79248046875
2023-01-07 09:05:26,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.51100158691406
2023-01-07 09:05:26,392 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.6766185760498047 param sum :: 128.0
2023-01-07 09:05:26,392 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 8.519919395446777
2023-01-07 09:05:26,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,392 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -923.1829223632812
2023-01-07 09:05:26,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.65371322631836
2023-01-07 09:05:26,393 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -918.89990234375 param sum :: 8.519919395446777
2023-01-07 09:05:26,393 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,393 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,393 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.294705331325531
2023-01-07 09:05:26,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,394 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -799.391845703125
2023-01-07 09:05:26,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.5402021408081055
2023-01-07 09:05:26,395 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.45646634697914124 param sum :: 128.0
2023-01-07 09:05:26,395 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -17.605867385864258
2023-01-07 09:05:26,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,395 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -795.9945068359375
2023-01-07 09:05:26,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.965251445770264
2023-01-07 09:05:26,396 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -789.3873291015625 param sum :: -17.605867385864258
2023-01-07 09:05:26,396 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:26,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,397 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -655.5277099609375
2023-01-07 09:05:26,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5537257194519043
2023-01-07 09:05:26,398 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -118.85181427001953 param sum :: 512.0
2023-01-07 09:05:26,398 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:26,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,398 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -665.788818359375
2023-01-07 09:05:26,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7850308418273926
2023-01-07 09:05:26,399 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -658.056640625 param sum :: -58.115535736083984
2023-01-07 09:05:26,399 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,399 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,399 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.013425350189208984
2023-01-07 09:05:26,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,400 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -798.734130859375
2023-01-07 09:05:26,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.1451520919799805
2023-01-07 09:05:26,401 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.14542913436889648 param sum :: 128.0
2023-01-07 09:05:26,401 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -14.622330665588379
2023-01-07 09:05:26,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,401 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -844.6343383789062
2023-01-07 09:05:26,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6558241844177246
2023-01-07 09:05:26,402 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -846.4265747070312 param sum :: -14.622330665588379
2023-01-07 09:05:26,402 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,402 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,402 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.2692795991897583
2023-01-07 09:05:26,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,403 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -570.87353515625
2023-01-07 09:05:26,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.799698829650879
2023-01-07 09:05:26,404 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.3094344139099121 param sum :: 128.0
2023-01-07 09:05:26,404 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.833906173706055
2023-01-07 09:05:26,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,404 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -563.4525146484375
2023-01-07 09:05:26,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.1016950607299805
2023-01-07 09:05:26,405 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -556.6737060546875 param sum :: 20.833906173706055
2023-01-07 09:05:26,405 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:26,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,405 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,406 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -1.2846331596374512
2023-01-07 09:05:26,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,406 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -556.234130859375
2023-01-07 09:05:26,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.100649833679199
2023-01-07 09:05:26,407 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.4867110848426819 param sum :: 512.0
2023-01-07 09:05:26,407 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -48.606842041015625
2023-01-07 09:05:26,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,407 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -560.259521484375
2023-01-07 09:05:26,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.864284038543701
2023-01-07 09:05:26,408 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -562.9956665039062 param sum :: -48.606842041015625
2023-01-07 09:05:26,408 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,409 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,409 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.2615179121494293
2023-01-07 09:05:26,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,409 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -734.7756958007812
2023-01-07 09:05:26,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.927245140075684
2023-01-07 09:05:26,410 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.14381322264671326 param sum :: 128.0
2023-01-07 09:05:26,410 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -34.13706588745117
2023-01-07 09:05:26,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,410 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -784.7289428710938
2023-01-07 09:05:26,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.039036750793457
2023-01-07 09:05:26,411 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -791.09423828125 param sum :: -34.13706588745117
2023-01-07 09:05:26,411 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:26,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,412 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.000828176736831665
2023-01-07 09:05:26,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -690.646484375
2023-01-07 09:05:26,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.585117340087891
2023-01-07 09:05:26,413 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.38725101947784424 param sum :: 128.0
2023-01-07 09:05:26,413 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 11.814714431762695
2023-01-07 09:05:26,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,414 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -701.0545654296875
2023-01-07 09:05:26,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9602155685424805
2023-01-07 09:05:26,414 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -698.4271240234375 param sum :: 11.814714431762695
2023-01-07 09:05:26,415 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:26,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,415 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,415 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.6675233244895935
2023-01-07 09:05:26,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,415 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -708.8582153320312
2023-01-07 09:05:26,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2506217956542969
2023-01-07 09:05:26,416 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.603853166103363 param sum :: 512.0
2023-01-07 09:05:26,416 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -26.576377868652344
2023-01-07 09:05:26,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,417 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -703.660400390625
2023-01-07 09:05:26,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.866364002227783
2023-01-07 09:05:26,418 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -700.2384033203125 param sum :: -26.576377868652344
2023-01-07 09:05:26,418 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:26,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,418 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.6074534058570862
2023-01-07 09:05:26,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1152.0509033203125
2023-01-07 09:05:26,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7406601905822754
2023-01-07 09:05:26,419 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.8084116578102112 param sum :: 256.0
2023-01-07 09:05:26,419 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.7762942314147949
2023-01-07 09:05:26,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1120.2803955078125
2023-01-07 09:05:26,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.857701778411865
2023-01-07 09:05:26,421 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1102.7181396484375 param sum :: -0.7762942314147949
2023-01-07 09:05:26,421 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,421 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,421 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:26,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,421 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -834.5745849609375
2023-01-07 09:05:26,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.465230464935303
2023-01-07 09:05:26,422 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -43.24110412597656 param sum :: 256.0
2023-01-07 09:05:26,422 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,423 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:26,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,423 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -834.6240844726562
2023-01-07 09:05:26,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.022778034210205
2023-01-07 09:05:26,424 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -833.3939208984375 param sum :: 10.802434921264648
2023-01-07 09:05:26,424 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:26,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,424 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -258.9660949707031
2023-01-07 09:05:26,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.192389965057373
2023-01-07 09:05:26,425 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -190.62399291992188 param sum :: 1024.0
2023-01-07 09:05:26,425 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:26,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,426 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -251.96826171875
2023-01-07 09:05:26,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9101920127868652
2023-01-07 09:05:26,427 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -256.26165771484375 param sum :: -34.55324935913086
2023-01-07 09:05:26,427 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,427 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:26,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,427 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -762.715576171875
2023-01-07 09:05:26,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0357117652893066
2023-01-07 09:05:26,428 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -189.9053955078125 param sum :: 1024.0
2023-01-07 09:05:26,428 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:26,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,428 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -765.8572998046875
2023-01-07 09:05:26,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.026513397693634033
2023-01-07 09:05:26,429 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -768.8953857421875 param sum :: -82.27286529541016
2023-01-07 09:05:26,430 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,430 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,430 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:26,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,430 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:05:26,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.23630596697330475
2023-01-07 09:05:26,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -281.5235595703125
2023-01-07 09:05:26,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0977253913879395
2023-01-07 09:05:26,431 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.1881597638130188 param sum :: 256.0
2023-01-07 09:05:26,431 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -26.810012817382812
2023-01-07 09:05:26,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,431 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -278.519287109375
2023-01-07 09:05:26,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8876900672912598
2023-01-07 09:05:26,432 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -278.26922607421875 param sum :: -26.810012817382812
2023-01-07 09:05:26,433 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:26,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,433 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -840.9962158203125
2023-01-07 09:05:26,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.355302333831787
2023-01-07 09:05:26,434 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -42.568504333496094 param sum :: 256.0
2023-01-07 09:05:26,434 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,434 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,434 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:26,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,434 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -839.4718017578125
2023-01-07 09:05:26,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0061274766921997
2023-01-07 09:05:26,435 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -838.45751953125 param sum :: 23.764698028564453
2023-01-07 09:05:26,435 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,436 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:26,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,436 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -932.4249877929688
2023-01-07 09:05:26,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.210467338562012
2023-01-07 09:05:26,437 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -189.8809814453125 param sum :: 1024.0
2023-01-07 09:05:26,437 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:26,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,437 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -930.4628295898438
2023-01-07 09:05:26,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8518959283828735
2023-01-07 09:05:26,438 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -930.65869140625 param sum :: -33.003875732421875
2023-01-07 09:05:26,438 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:26,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,439 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -894.9677734375
2023-01-07 09:05:26,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.284939765930176
2023-01-07 09:05:26,440 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -43.41731262207031 param sum :: 256.0
2023-01-07 09:05:26,440 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:26,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,440 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -901.2294921875
2023-01-07 09:05:26,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.299253225326538
2023-01-07 09:05:26,441 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -905.2135620117188 param sum :: 39.414920806884766
2023-01-07 09:05:26,441 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:26,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,442 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1014.07568359375
2023-01-07 09:05:26,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28539779782295227
2023-01-07 09:05:26,443 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -43.66685485839844 param sum :: 256.0
2023-01-07 09:05:26,443 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:26,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,443 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1009.1168823242188
2023-01-07 09:05:26,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.859046995639801
2023-01-07 09:05:26,444 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1011.3130493164062 param sum :: 7.077023506164551
2023-01-07 09:05:26,444 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:26,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,445 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -978.4622802734375
2023-01-07 09:05:26,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.732966661453247
2023-01-07 09:05:26,446 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -186.64149475097656 param sum :: 1024.0
2023-01-07 09:05:26,446 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,446 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,446 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:26,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,446 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -979.0338745117188
2023-01-07 09:05:26,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8140976428985596
2023-01-07 09:05:26,447 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -974.291259765625 param sum :: -1.5680561065673828
2023-01-07 09:05:26,448 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,448 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,448 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:26,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,448 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3225.5546875
2023-01-07 09:05:26,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1697239875793457
2023-01-07 09:05:26,449 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -44.64369583129883 param sum :: 256.0
2023-01-07 09:05:26,449 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:26,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3229.51171875
2023-01-07 09:05:26,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9076274633407593
2023-01-07 09:05:26,450 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -3226.94775390625 param sum :: -28.745033264160156
2023-01-07 09:05:26,450 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,451 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:26,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,451 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1056.888427734375
2023-01-07 09:05:26,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35720863938331604
2023-01-07 09:05:26,452 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -44.9560546875 param sum :: 256.0
2023-01-07 09:05:26,452 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:26,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1058.8509521484375
2023-01-07 09:05:26,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5271584987640381
2023-01-07 09:05:26,453 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1059.2373046875 param sum :: -5.068355560302734
2023-01-07 09:05:26,454 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,454 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:26,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -904.8229370117188
2023-01-07 09:05:26,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.70000422000885
2023-01-07 09:05:26,455 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -188.9644775390625 param sum :: 1024.0
2023-01-07 09:05:26,455 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,455 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:26,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -906.0438842773438
2023-01-07 09:05:26,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5990104675292969
2023-01-07 09:05:26,456 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -905.4479370117188 param sum :: -63.574039459228516
2023-01-07 09:05:26,456 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:26,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,457 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -576.1233520507812
2023-01-07 09:05:26,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.781238317489624
2023-01-07 09:05:26,458 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -45.21514129638672 param sum :: 256.0
2023-01-07 09:05:26,458 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:26,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,458 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -581.4937133789062
2023-01-07 09:05:26,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8514575958251953
2023-01-07 09:05:26,459 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -584.8504028320312 param sum :: 41.2066535949707
2023-01-07 09:05:26,459 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:26,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,460 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2202.811767578125
2023-01-07 09:05:26,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.404074192047119
2023-01-07 09:05:26,461 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -45.311214447021484 param sum :: 256.0
2023-01-07 09:05:26,461 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:26,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,461 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2204.733154296875
2023-01-07 09:05:26,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08127650618553162
2023-01-07 09:05:26,462 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2206.522216796875 param sum :: -6.652504920959473
2023-01-07 09:05:26,462 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:26,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,463 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -504.58966064453125
2023-01-07 09:05:26,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6738256216049194
2023-01-07 09:05:26,464 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -186.79855346679688 param sum :: 1024.0
2023-01-07 09:05:26,464 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:26,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,464 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -502.245361328125
2023-01-07 09:05:26,464 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7141146659851074
2023-01-07 09:05:26,465 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -502.42999267578125 param sum :: -52.449466705322266
2023-01-07 09:05:26,465 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,465 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,465 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:26,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -365.12274169921875
2023-01-07 09:05:26,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17152830958366394
2023-01-07 09:05:26,467 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -45.283775329589844 param sum :: 256.0
2023-01-07 09:05:26,467 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:26,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,467 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -364.13616943359375
2023-01-07 09:05:26,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11066174507141113
2023-01-07 09:05:26,468 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -363.18951416015625 param sum :: -38.22235870361328
2023-01-07 09:05:26,468 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,468 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:26,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -582.0935668945312
2023-01-07 09:05:26,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8203291893005371
2023-01-07 09:05:26,470 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -45.506324768066406 param sum :: 256.0
2023-01-07 09:05:26,470 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,470 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:26,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,470 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -581.5654296875
2023-01-07 09:05:26,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24823226034641266
2023-01-07 09:05:26,471 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -582.5955810546875 param sum :: -3.3066205978393555
2023-01-07 09:05:26,471 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,471 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:26,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,471 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 21.148223876953125
2023-01-07 09:05:26,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6777586936950684
2023-01-07 09:05:26,472 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -186.76593017578125 param sum :: 1024.0
2023-01-07 09:05:26,473 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,473 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,473 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:26,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,473 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 20.145782470703125
2023-01-07 09:05:26,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0443828105926514
2023-01-07 09:05:26,474 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 20.862091064453125 param sum :: 70.22351837158203
2023-01-07 09:05:26,474 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:26,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,474 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -4609.39208984375
2023-01-07 09:05:26,474 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5174368619918823
2023-01-07 09:05:26,475 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -95.35724639892578 param sum :: 512.0
2023-01-07 09:05:26,475 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:26,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,476 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -4610.87841796875
2023-01-07 09:05:26,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1474188566207886
2023-01-07 09:05:26,477 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -4611.5009765625 param sum :: 8.939680099487305
2023-01-07 09:05:26,477 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,477 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,477 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:26,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,477 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 257.6527099609375
2023-01-07 09:05:26,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24270206689834595
2023-01-07 09:05:26,478 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.08687535673379898 param sum :: 512.0
2023-01-07 09:05:26,478 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,479 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:26,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,479 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 257.75250244140625
2023-01-07 09:05:26,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35507088899612427
2023-01-07 09:05:26,480 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 257.19439697265625 param sum :: 20.04633331298828
2023-01-07 09:05:26,480 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:26,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,480 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2431.078857421875
2023-01-07 09:05:26,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7313766479492188
2023-01-07 09:05:26,481 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -408.9066467285156 param sum :: 2048.0
2023-01-07 09:05:26,481 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:26,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,482 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2430.75341796875
2023-01-07 09:05:26,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.878608226776123
2023-01-07 09:05:26,482 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2430.08837890625 param sum :: -41.879310607910156
2023-01-07 09:05:26,483 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:26,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 98.39892578125
2023-01-07 09:05:26,483 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8132133483886719
2023-01-07 09:05:26,484 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -412.2991027832031 param sum :: 2048.0
2023-01-07 09:05:26,484 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:26,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,484 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 98.43280029296875
2023-01-07 09:05:26,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3773801624774933
2023-01-07 09:05:26,485 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 97.40869140625 param sum :: 1.8718643188476562
2023-01-07 09:05:26,485 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,485 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:26,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,486 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8029.8447265625
2023-01-07 09:05:26,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2581603527069092
2023-01-07 09:05:26,487 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -98.25335693359375 param sum :: 512.0
2023-01-07 09:05:26,487 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:26,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,487 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8029.794921875
2023-01-07 09:05:26,487 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1017539501190186
2023-01-07 09:05:26,488 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -8029.26171875 param sum :: -16.520038604736328
2023-01-07 09:05:26,488 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:26,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,489 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -804.0308837890625
2023-01-07 09:05:26,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7095068693161011
2023-01-07 09:05:26,490 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.08229658752679825 param sum :: 512.0
2023-01-07 09:05:26,490 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,490 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:26,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,490 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -804.1929931640625
2023-01-07 09:05:26,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014114804565906525
2023-01-07 09:05:26,491 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -804.3963623046875 param sum :: -14.060759544372559
2023-01-07 09:05:26,491 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:26,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,492 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7403.302734375
2023-01-07 09:05:26,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35639727115631104
2023-01-07 09:05:26,493 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -413.33685302734375 param sum :: 2048.0
2023-01-07 09:05:26,493 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:26,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,493 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7403.63427734375
2023-01-07 09:05:26,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42453694343566895
2023-01-07 09:05:26,494 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 7403.73388671875 param sum :: 35.38753890991211
2023-01-07 09:05:26,494 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:26,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,494 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -17861.337890625
2023-01-07 09:05:26,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12053646147251129
2023-01-07 09:05:26,495 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -98.5147705078125 param sum :: 512.0
2023-01-07 09:05:26,496 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:26,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,496 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -17861.087890625
2023-01-07 09:05:26,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12691539525985718
2023-01-07 09:05:26,497 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -17861.115234375 param sum :: -33.75589370727539
2023-01-07 09:05:26,497 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:26,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,497 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3150.78515625
2023-01-07 09:05:26,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22489871084690094
2023-01-07 09:05:26,498 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.025485865771770477 param sum :: 512.0
2023-01-07 09:05:26,498 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:26,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,499 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3150.7353515625
2023-01-07 09:05:26,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.049971193075180054
2023-01-07 09:05:26,500 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 3150.54638671875 param sum :: 18.47730255126953
2023-01-07 09:05:26,500 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,500 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,500 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:26,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,500 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17597.89453125
2023-01-07 09:05:26,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.418729782104492
2023-01-07 09:05:26,501 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -412.7421875 param sum :: 2048.0
2023-01-07 09:05:26,501 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:26,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,502 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17600.9765625
2023-01-07 09:05:26,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7025525569915771
2023-01-07 09:05:26,503 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 17601.5546875 param sum :: 9.326160430908203
2023-01-07 09:05:26,503 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:26,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:26,504 > [DEBUG] 0 :: 7.079259872436523
2023-01-07 09:05:26,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,506 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0157470703125
2023-01-07 09:05:26,506 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,507 > [DEBUG] 0 :: before allreduce fusion buffer :: -272.91436767578125
2023-01-07 09:05:26,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1892790049314499
2023-01-07 09:05:26,509 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,509 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -283.20147705078125
2023-01-07 09:05:26,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -302.96746826171875
2023-01-07 09:05:26,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 0.30467402935028076
2023-01-07 09:05:26,511 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08901423215866089
2023-01-07 09:05:26,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,512 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.004839971661567688
2023-01-07 09:05:26,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,513 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -111.8712387084961
2023-01-07 09:05:26,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2230154275894165
2023-01-07 09:05:26,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,514 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.0635414123535156
2023-01-07 09:05:26,514 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0961962342262268
2023-01-07 09:05:26,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,515 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.03220411762595177
2023-01-07 09:05:26,515 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,515 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.048142910003662
2023-01-07 09:05:26,515 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02048683539032936
2023-01-07 09:05:26,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,516 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.526388645172119
2023-01-07 09:05:26,517 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45754575729370117
2023-01-07 09:05:26,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,517 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.06576204299926758
2023-01-07 09:05:26,518 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,518 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -70.15904998779297
2023-01-07 09:05:26,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4295191764831543
2023-01-07 09:05:26,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,519 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -0.49655139446258545
2023-01-07 09:05:26,519 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3331928849220276
2023-01-07 09:05:26,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,520 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.015430537052452564
2023-01-07 09:05:26,520 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,520 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -0.08405590057373047
2023-01-07 09:05:26,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9098766446113586
2023-01-07 09:05:26,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,521 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 11.754831314086914
2023-01-07 09:05:26,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09778603166341782
2023-01-07 09:05:26,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,522 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.04177350550889969
2023-01-07 09:05:26,523 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,523 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 11.540596961975098
2023-01-07 09:05:26,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29996976256370544
2023-01-07 09:05:26,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,524 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.7596988677978516
2023-01-07 09:05:26,524 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06949730217456818
2023-01-07 09:05:26,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,525 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.09526114165782928
2023-01-07 09:05:26,525 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,525 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.9168026447296143
2023-01-07 09:05:26,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5954681634902954
2023-01-07 09:05:26,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,526 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.010260581970215
2023-01-07 09:05:26,526 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7722539901733398
2023-01-07 09:05:26,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,527 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.10145003348588943
2023-01-07 09:05:26,527 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,528 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.843892097473145
2023-01-07 09:05:26,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6820386648178101
2023-01-07 09:05:26,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,529 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.343647480010986
2023-01-07 09:05:26,529 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06990198791027069
2023-01-07 09:05:26,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,530 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.015817776322364807
2023-01-07 09:05:26,530 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,530 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.01467227935791
2023-01-07 09:05:26,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.282939434051514
2023-01-07 09:05:26,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,531 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -12.790822982788086
2023-01-07 09:05:26,531 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,531 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.056914784014225006
2023-01-07 09:05:26,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,532 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.0829811543226242
2023-01-07 09:05:26,532 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,532 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -12.790822982788086
2023-01-07 09:05:26,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.095020055770874
2023-01-07 09:05:26,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,533 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -0.0029096603393554688
2023-01-07 09:05:26,533 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.615366518497467
2023-01-07 09:05:26,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,534 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.045931827276945114
2023-01-07 09:05:26,534 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,535 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -0.6953868865966797
2023-01-07 09:05:26,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01784542202949524
2023-01-07 09:05:26,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,536 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.9027908444404602
2023-01-07 09:05:26,536 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5811425447463989
2023-01-07 09:05:26,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,537 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.1335192322731018
2023-01-07 09:05:26,537 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,537 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.03269100189209
2023-01-07 09:05:26,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15876618027687073
2023-01-07 09:05:26,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,538 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -8.03001880645752
2023-01-07 09:05:26,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9059925675392151
2023-01-07 09:05:26,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,539 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.07051998376846313
2023-01-07 09:05:26,539 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,540 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -5.458811283111572
2023-01-07 09:05:26,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25672855973243713
2023-01-07 09:05:26,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,541 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.916023254394531
2023-01-07 09:05:26,541 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9490057826042175
2023-01-07 09:05:26,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.23099662363529205
2023-01-07 09:05:26,542 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.088425636291504
2023-01-07 09:05:26,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11090700328350067
2023-01-07 09:05:26,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,543 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 3.2328834533691406
2023-01-07 09:05:26,543 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03460855782032013
2023-01-07 09:05:26,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.1358790397644043
2023-01-07 09:05:26,544 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 3.8303396701812744
2023-01-07 09:05:26,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.156571626663208
2023-01-07 09:05:26,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,545 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -2.7317209243774414
2023-01-07 09:05:26,545 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2032827138900757
2023-01-07 09:05:26,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,546 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.036308273673057556
2023-01-07 09:05:26,546 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -3.172647476196289
2023-01-07 09:05:26,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2684233784675598
2023-01-07 09:05:26,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,548 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -6.263522624969482
2023-01-07 09:05:26,548 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6304976940155029
2023-01-07 09:05:26,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.0065546780824661255
2023-01-07 09:05:26,549 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -7.735929489135742
2023-01-07 09:05:26,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1020407676696777
2023-01-07 09:05:26,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 4.386966705322266
2023-01-07 09:05:26,550 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08690068125724792
2023-01-07 09:05:26,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,551 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.14389337599277496
2023-01-07 09:05:26,551 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,551 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 4.168581962585449
2023-01-07 09:05:26,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2378164529800415
2023-01-07 09:05:26,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 15.402743339538574
2023-01-07 09:05:26,552 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8777008056640625
2023-01-07 09:05:26,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,553 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.11212300509214401
2023-01-07 09:05:26,553 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,554 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 16.01152801513672
2023-01-07 09:05:26,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.356884956359863
2023-01-07 09:05:26,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.752260208129883
2023-01-07 09:05:26,555 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1816098988056183
2023-01-07 09:05:26,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,556 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2777940630912781
2023-01-07 09:05:26,556 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,556 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -7.560446739196777
2023-01-07 09:05:26,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.677431583404541
2023-01-07 09:05:26,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,557 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.9730167388916016
2023-01-07 09:05:26,557 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4225846827030182
2023-01-07 09:05:26,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.11916257441043854
2023-01-07 09:05:26,558 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 7.414768218994141
2023-01-07 09:05:26,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.537149429321289
2023-01-07 09:05:26,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,559 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 25.506244659423828
2023-01-07 09:05:26,559 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0941615104675293
2023-01-07 09:05:26,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,560 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.08647942543029785
2023-01-07 09:05:26,560 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,560 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 29.09504508972168
2023-01-07 09:05:26,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.144170761108398
2023-01-07 09:05:26,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,561 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -23.666717529296875
2023-01-07 09:05:26,562 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3610308766365051
2023-01-07 09:05:26,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.1334739625453949
2023-01-07 09:05:26,563 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,563 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -22.257904052734375
2023-01-07 09:05:26,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1291744709014893
2023-01-07 09:05:26,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,564 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 15.664779663085938
2023-01-07 09:05:26,564 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9905425310134888
2023-01-07 09:05:26,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.06967028975486755
2023-01-07 09:05:26,565 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 17.499826431274414
2023-01-07 09:05:26,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1135653257369995
2023-01-07 09:05:26,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,566 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -2.471019744873047
2023-01-07 09:05:26,567 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2831157445907593
2023-01-07 09:05:26,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -8.824620246887207
2023-01-07 09:05:26,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4902448356151581
2023-01-07 09:05:26,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -2.1207828521728516
2023-01-07 09:05:26,569 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3394124507904053
2023-01-07 09:05:26,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.21739532053470612
2023-01-07 09:05:26,569 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -9.78288459777832
2023-01-07 09:05:26,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5839648246765137
2023-01-07 09:05:26,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -25.331750869750977
2023-01-07 09:05:26,571 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43913209438323975
2023-01-07 09:05:26,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.2672543525695801
2023-01-07 09:05:26,572 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -27.129175186157227
2023-01-07 09:05:26,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1831324100494385
2023-01-07 09:05:26,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.7357873916625977
2023-01-07 09:05:26,573 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10930785536766052
2023-01-07 09:05:26,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.05043429136276245
2023-01-07 09:05:26,574 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.4193735122680664
2023-01-07 09:05:26,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.973292350769043
2023-01-07 09:05:26,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 111.29247283935547
2023-01-07 09:05:26,576 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08219890296459198
2023-01-07 09:05:26,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 107.88871002197266
2023-01-07 09:05:26,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7107391357421875
2023-01-07 09:05:26,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,577 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 11.08056926727295
2023-01-07 09:05:26,578 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.327675819396973
2023-01-07 09:05:26,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 11.08056926727295
2023-01-07 09:05:26,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.829646110534668
2023-01-07 09:05:26,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,579 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.818939208984375
2023-01-07 09:05:26,580 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.769622802734375
2023-01-07 09:05:26,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,580 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -15.460777282714844
2023-01-07 09:05:26,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.4206743240356445
2023-01-07 09:05:26,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,581 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 12.132247924804688
2023-01-07 09:05:26,581 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.028207778930664
2023-01-07 09:05:26,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,582 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 8.97391128540039
2023-01-07 09:05:26,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.7515668869018555
2023-01-07 09:05:26,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -18.643224716186523
2023-01-07 09:05:26,583 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.016101360321045
2023-01-07 09:05:26,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,584 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.415420532226562
2023-01-07 09:05:26,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3321834802627563
2023-01-07 09:05:26,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.444000720977783
2023-01-07 09:05:26,585 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.292390823364258
2023-01-07 09:05:26,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,586 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.543466567993164
2023-01-07 09:05:26,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.42237663269043
2023-01-07 09:05:26,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,587 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -86.9725570678711
2023-01-07 09:05:26,587 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.552685737609863
2023-01-07 09:05:26,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,588 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -82.67509460449219
2023-01-07 09:05:26,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4844706058502197
2023-01-07 09:05:26,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,589 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -64.09738159179688
2023-01-07 09:05:26,589 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1905040740966797
2023-01-07 09:05:26,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,590 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.3581295013427734
2023-01-07 09:05:26,590 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,591 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -79.5648193359375
2023-01-07 09:05:26,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.711552619934082
2023-01-07 09:05:26,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,592 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.987283706665039
2023-01-07 09:05:26,592 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.1974515914917
2023-01-07 09:05:26,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,593 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -1270.0382080078125
2023-01-07 09:05:26,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.669098854064941
2023-01-07 09:05:26,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,594 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 165.22451782226562
2023-01-07 09:05:26,594 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.80551815032959
2023-01-07 09:05:26,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1121.9913330078125
2023-01-07 09:05:26,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.512798309326172
2023-01-07 09:05:26,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,596 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -13.609310150146484
2023-01-07 09:05:26,596 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.741865634918213
2023-01-07 09:05:26,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,597 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.264336347579956
2023-01-07 09:05:26,597 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,597 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1224.4180908203125
2023-01-07 09:05:26,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.156904220581055
2023-01-07 09:05:26,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 32.02423858642578
2023-01-07 09:05:26,598 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.512270927429199
2023-01-07 09:05:26,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,599 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.32258415222168
2023-01-07 09:05:26,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.857666015625
2023-01-07 09:05:26,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,600 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 7.08113956451416
2023-01-07 09:05:26,600 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.084304809570312
2023-01-07 09:05:26,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,601 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -18.492202758789062
2023-01-07 09:05:26,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5700693130493164
2023-01-07 09:05:26,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,602 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -173.98843383789062
2023-01-07 09:05:26,602 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.2130279541015625
2023-01-07 09:05:26,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,603 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.2210421562194824
2023-01-07 09:05:26,603 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,603 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -213.81658935546875
2023-01-07 09:05:26,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.72047424316406
2023-01-07 09:05:26,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,605 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -231.77618408203125
2023-01-07 09:05:26,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.73431968688965
2023-01-07 09:05:26,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,605 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.1521847248077393
2023-01-07 09:05:26,605 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,606 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -30.835859298706055
2023-01-07 09:05:26,606 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,606 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -281.55328369140625
2023-01-07 09:05:26,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.767623901367188
2023-01-07 09:05:26,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -281.57086181640625
2023-01-07 09:05:26,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.122596740722656
2023-01-07 09:05:26,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.664045810699463
2023-01-07 09:05:26,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.141170501708984
2023-01-07 09:05:26,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,609 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 162.4173583984375
2023-01-07 09:05:26,609 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.23929786682129
2023-01-07 09:05:26,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,610 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.2178523540496826
2023-01-07 09:05:26,610 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,610 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 143.7918701171875
2023-01-07 09:05:26,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -90.64833068847656
2023-01-07 09:05:26,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,611 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 162.220947265625
2023-01-07 09:05:26,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -93.38386535644531
2023-01-07 09:05:26,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,612 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.6091346740722656
2023-01-07 09:05:26,612 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,613 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 66.98101806640625
2023-01-07 09:05:26,613 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.92966079711914
2023-01-07 09:05:26,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,614 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 124.68670654296875
2023-01-07 09:05:26,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.567968368530273
2023-01-07 09:05:26,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,615 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 12.314241409301758
2023-01-07 09:05:26,615 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -365.9300537109375
2023-01-07 09:05:26,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1849222183227539
2023-01-07 09:05:26,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,616 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -5.682899475097656
2023-01-07 09:05:26,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.733530044555664
2023-01-07 09:05:26,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,617 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -38.525489807128906
2023-01-07 09:05:26,617 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.325721740722656
2023-01-07 09:05:26,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,618 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -118.72064971923828
2023-01-07 09:05:26,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.929481506347656
2023-01-07 09:05:26,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -4.082846641540527
2023-01-07 09:05:26,619 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 95.92630767822266
2023-01-07 09:05:26,620 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,620 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -240.3509521484375
2023-01-07 09:05:26,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,620 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -373.07965087890625
2023-01-07 09:05:26,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.97589111328125
2023-01-07 09:05:26,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,621 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -241.45846557617188
2023-01-07 09:05:26,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.168235778808594
2023-01-07 09:05:26,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,622 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 133.20262145996094
2023-01-07 09:05:26,622 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.66766357421875
2023-01-07 09:05:26,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,623 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -350.953125
2023-01-07 09:05:26,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.65850830078125
2023-01-07 09:05:26,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,624 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 164.87649536132812
2023-01-07 09:05:26,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -356.2217712402344
2023-01-07 09:05:26,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 141.69708251953125
2023-01-07 09:05:26,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,625 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 955.70654296875
2023-01-07 09:05:26,625 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.233055114746094
2023-01-07 09:05:26,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,626 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -0.008759737014770508
2023-01-07 09:05:26,626 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,627 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 974.9249877929688
2023-01-07 09:05:26,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.666677474975586
2023-01-07 09:05:26,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,628 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1175.5494384765625
2023-01-07 09:05:26,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.5526580810546875
2023-01-07 09:05:26,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1175.5494384765625
2023-01-07 09:05:26,629 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.029108047485352
2023-01-07 09:05:26,631 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:05:26,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,632 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -3432.485107421875
2023-01-07 09:05:26,632 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:26,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,632 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1266.03125
2023-01-07 09:05:26,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,633 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -340.94415283203125
2023-01-07 09:05:26,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,633 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -52.767852783203125
2023-01-07 09:05:26,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,634 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 15.990964889526367
2023-01-07 09:05:26,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,634 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1226.8974609375
2023-01-07 09:05:26,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,635 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -625.0108032226562
2023-01-07 09:05:26,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,635 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 167.31761169433594
2023-01-07 09:05:26,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3.1130294799804688
2023-01-07 09:05:26,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -64.8103256225586
2023-01-07 09:05:26,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -10.334086418151855
2023-01-07 09:05:26,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.3655128479003906
2023-01-07 09:05:26,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 38.27156448364258
2023-01-07 09:05:26,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -9.904705047607422
2023-01-07 09:05:26,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 8.849478721618652
2023-01-07 09:05:26,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 86.46174621582031
2023-01-07 09:05:26,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 12.302948951721191
2023-01-07 09:05:26,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -18.03126335144043
2023-01-07 09:05:26,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 10.657621383666992
2023-01-07 09:05:26,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.5943603515625
2023-01-07 09:05:26,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 29.50104522705078
2023-01-07 09:05:26,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -31.980602264404297
2023-01-07 09:05:26,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.662181854248047
2023-01-07 09:05:26,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -2.6554336547851562
2023-01-07 09:05:26,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -9.299577713012695
2023-01-07 09:05:26,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 20.922409057617188
2023-01-07 09:05:26,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -2646.257080078125
2023-01-07 09:05:26,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 4.895179748535156
2023-01-07 09:05:26,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -11.144937515258789
2023-01-07 09:05:26,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -4.712774276733398
2023-01-07 09:05:26,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 2.477994441986084
2023-01-07 09:05:26,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.899059295654297
2023-01-07 09:05:26,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -4.637912750244141
2023-01-07 09:05:26,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.539081573486328
2023-01-07 09:05:26,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,645 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.0269269943237305
2023-01-07 09:05:26,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,645 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -14.33338451385498
2023-01-07 09:05:26,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.017120361328125
2023-01-07 09:05:26,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,646 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.561013221740723
2023-01-07 09:05:26,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,646 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.259562492370605
2023-01-07 09:05:26,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,647 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 0.4193429946899414
2023-01-07 09:05:26,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,647 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 11.084331512451172
2023-01-07 09:05:26,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.216150283813477
2023-01-07 09:05:26,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,648 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 0.0021669864654541016
2023-01-07 09:05:26,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,648 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -70.39927673339844
2023-01-07 09:05:26,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,648 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.0146563053131104
2023-01-07 09:05:26,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.400064468383789
2023-01-07 09:05:26,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,649 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -485.87017822265625
2023-01-07 09:05:26,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:26,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:26,649 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -353.68328857421875
2023-01-07 09:05:26,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 571.77685546875
2023-01-07 09:05:27,492 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 208.07347106933594 param sum :: 0.22188237309455872
2023-01-07 09:05:27,492 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,492 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:27,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,493 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -2.6254546642303467
2023-01-07 09:05:27,493 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,493 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 649.5830078125
2023-01-07 09:05:27,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 103.43144226074219
2023-01-07 09:05:27,494 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -1.399916648864746 param sum :: 64.0
2023-01-07 09:05:27,495 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,495 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 5.294054985046387
2023-01-07 09:05:27,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,495 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 109.68500518798828
2023-01-07 09:05:27,495 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,495 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 719.4085693359375
2023-01-07 09:05:27,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 116.70504760742188
2023-01-07 09:05:27,496 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 227.00205993652344 param sum :: 5.294054985046387
2023-01-07 09:05:27,496 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:27,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,497 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 714.452392578125
2023-01-07 09:05:27,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 115.59396362304688
2023-01-07 09:05:27,498 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 9.915860176086426 param sum :: 64.0
2023-01-07 09:05:27,498 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,498 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:27,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,498 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 649.6968994140625
2023-01-07 09:05:27,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 126.65750122070312
2023-01-07 09:05:27,499 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 739.9528198242188 param sum :: 5.982826232910156
2023-01-07 09:05:27,499 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:27,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,499 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -3.905116081237793
2023-01-07 09:05:27,500 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,500 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 71.00432586669922
2023-01-07 09:05:27,500 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,500 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -443.4429016113281
2023-01-07 09:05:27,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 70.20182800292969
2023-01-07 09:05:27,501 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -1.2889750003814697 param sum :: 64.0
2023-01-07 09:05:27,501 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,502 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:27,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,502 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -384.6385498046875
2023-01-07 09:05:27,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3747787475585938
2023-01-07 09:05:27,503 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 123.79723358154297 param sum :: 11.617742538452148
2023-01-07 09:05:27,503 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:27,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,503 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.7287865877151489
2023-01-07 09:05:27,503 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,503 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 89.17335510253906
2023-01-07 09:05:27,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.122337341308594
2023-01-07 09:05:27,504 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.6809801459312439 param sum :: 256.0
2023-01-07 09:05:27,505 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:27,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,505 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -183.9221649169922
2023-01-07 09:05:27,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9168891906738281
2023-01-07 09:05:27,506 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 244.8745574951172 param sum :: -10.471250534057617
2023-01-07 09:05:27,506 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,506 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:27,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,506 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -205.96673583984375
2023-01-07 09:05:27,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.56537628173828
2023-01-07 09:05:27,507 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -61.86608123779297 param sum :: 256.0
2023-01-07 09:05:27,507 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:27,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,508 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -40.043678283691406
2023-01-07 09:05:27,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.74101257324219
2023-01-07 09:05:27,509 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 96.11679077148438 param sum :: -38.087059020996094
2023-01-07 09:05:27,509 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:27,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,509 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.6349024772644043
2023-01-07 09:05:27,509 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,509 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -75.85528564453125
2023-01-07 09:05:27,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -124.34011840820312
2023-01-07 09:05:27,511 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 1.3880290985107422 param sum :: 64.0
2023-01-07 09:05:27,511 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:27,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,511 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -101.72389221191406
2023-01-07 09:05:27,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,511 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -228.67605590820312
2023-01-07 09:05:27,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.73445510864258
2023-01-07 09:05:27,512 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -111.80929565429688 param sum :: -9.83746337890625
2023-01-07 09:05:27,512 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:27,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,513 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -3.0139427185058594
2023-01-07 09:05:27,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,513 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -145.79092407226562
2023-01-07 09:05:27,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.952154159545898
2023-01-07 09:05:27,514 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -3.64841365814209 param sum :: 64.0
2023-01-07 09:05:27,514 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:27,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,515 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -445.0970458984375
2023-01-07 09:05:27,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 120.65599822998047
2023-01-07 09:05:27,516 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -54.019630432128906 param sum :: 13.821467399597168
2023-01-07 09:05:27,516 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:27,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,516 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -32.903587341308594
2023-01-07 09:05:27,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.837074279785156
2023-01-07 09:05:27,517 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -53.84716033935547 param sum :: 256.0
2023-01-07 09:05:27,517 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:27,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,518 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 40.524559020996094
2023-01-07 09:05:27,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,518 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -475.9090270996094
2023-01-07 09:05:27,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.96494197845459
2023-01-07 09:05:27,519 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 33.00107192993164 param sum :: -16.297849655151367
2023-01-07 09:05:27,519 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:27,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,519 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -414.9217529296875
2023-01-07 09:05:27,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 85.99537658691406
2023-01-07 09:05:27,520 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -7.934301853179932 param sum :: 64.0
2023-01-07 09:05:27,520 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:27,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,521 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -476.92205810546875
2023-01-07 09:05:27,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.092544555664062
2023-01-07 09:05:27,521 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -454.9266357421875 param sum :: -1.7540884017944336
2023-01-07 09:05:27,522 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:27,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,522 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -1.9909294843673706
2023-01-07 09:05:27,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,522 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 12.15025520324707
2023-01-07 09:05:27,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,522 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 23.934810638427734
2023-01-07 09:05:27,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.285839080810547
2023-01-07 09:05:27,524 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -1.6740925312042236 param sum :: 64.0
2023-01-07 09:05:27,524 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:27,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -243.24610900878906
2023-01-07 09:05:27,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.049299240112305
2023-01-07 09:05:27,525 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 29.910446166992188 param sum :: -0.8034350872039795
2023-01-07 09:05:27,525 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -17.25152015686035
2023-01-07 09:05:27,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 79.41795349121094
2023-01-07 09:05:27,526 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.00396728515625
2023-01-07 09:05:27,527 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -34.13127899169922 param sum :: 256.0
2023-01-07 09:05:27,527 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:27,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,527 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -218.25390625
2023-01-07 09:05:27,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.966033935546875
2023-01-07 09:05:27,528 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 81.06771850585938 param sum :: -17.25152015686035
2023-01-07 09:05:27,528 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:27,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,529 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -133.51605224609375
2023-01-07 09:05:27,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.92202758789062
2023-01-07 09:05:27,530 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -19.76892852783203 param sum :: 128.0
2023-01-07 09:05:27,530 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:27,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -109.63432312011719
2023-01-07 09:05:27,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.60476303100586
2023-01-07 09:05:27,531 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -127.36039733886719 param sum :: -10.020125389099121
2023-01-07 09:05:27,531 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,531 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -1.6678404808044434
2023-01-07 09:05:27,532 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -63.296756744384766
2023-01-07 09:05:27,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8847908973693848
2023-01-07 09:05:27,533 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.954531192779541 param sum :: 128.0
2023-01-07 09:05:27,533 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -4.991181373596191
2023-01-07 09:05:27,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,533 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -64.24861907958984
2023-01-07 09:05:27,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.858179807662964
2023-01-07 09:05:27,534 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -69.949462890625 param sum :: -4.991181373596191
2023-01-07 09:05:27,534 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:27,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.5781490802764893
2023-01-07 09:05:27,535 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -73.77327728271484
2023-01-07 09:05:27,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.822769165039062
2023-01-07 09:05:27,536 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.4477025270462036 param sum :: 512.0
2023-01-07 09:05:27,536 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -18.34212875366211
2023-01-07 09:05:27,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,537 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -64.3854751586914
2023-01-07 09:05:27,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.02999210357666
2023-01-07 09:05:27,537 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -60.133914947509766 param sum :: -18.34212875366211
2023-01-07 09:05:27,537 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:27,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,538 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1229.5931396484375
2023-01-07 09:05:27,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.793171882629395
2023-01-07 09:05:27,539 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -1207.0712890625 param sum :: 512.0
2023-01-07 09:05:27,539 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:27,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1226.020263671875
2023-01-07 09:05:27,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.010025978088379
2023-01-07 09:05:27,540 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -1229.474609375 param sum :: 37.5141487121582
2023-01-07 09:05:27,540 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -1.0526241064071655
2023-01-07 09:05:27,541 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -774.818603515625
2023-01-07 09:05:27,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.74494934082031
2023-01-07 09:05:27,542 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.7766958475112915 param sum :: 128.0
2023-01-07 09:05:27,542 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 8.519919395446777
2023-01-07 09:05:27,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,542 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -776.9584350585938
2023-01-07 09:05:27,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.222929000854492
2023-01-07 09:05:27,543 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -763.705078125 param sum :: 8.519919395446777
2023-01-07 09:05:27,543 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,544 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.16528242826461792
2023-01-07 09:05:27,544 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 136.13674926757812
2023-01-07 09:05:27,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.818641185760498
2023-01-07 09:05:27,545 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 1.771939992904663 param sum :: 128.0
2023-01-07 09:05:27,545 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -17.605867385864258
2023-01-07 09:05:27,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,545 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 141.57138061523438
2023-01-07 09:05:27,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2703016996383667
2023-01-07 09:05:27,546 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 138.38226318359375 param sum :: -17.605867385864258
2023-01-07 09:05:27,547 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:27,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,547 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3.5994319915771484
2023-01-07 09:05:27,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.2475385665893555
2023-01-07 09:05:27,548 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -14.193660736083984 param sum :: 512.0
2023-01-07 09:05:27,548 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:27,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -0.9186515808105469
2023-01-07 09:05:27,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35911399126052856
2023-01-07 09:05:27,549 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 7.56629753112793 param sum :: -58.115535736083984
2023-01-07 09:05:27,549 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,549 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,549 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.9614266753196716
2023-01-07 09:05:27,550 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -64.91458129882812
2023-01-07 09:05:27,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.253669738769531
2023-01-07 09:05:27,551 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 1.5253348350524902 param sum :: 128.0
2023-01-07 09:05:27,551 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,551 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -14.622330665588379
2023-01-07 09:05:27,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -98.32968139648438
2023-01-07 09:05:27,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.350934982299805
2023-01-07 09:05:27,552 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -90.90485382080078 param sum :: -14.622330665588379
2023-01-07 09:05:27,553 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -1.58034086227417
2023-01-07 09:05:27,553 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -9.494617462158203
2023-01-07 09:05:27,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23960471153259277
2023-01-07 09:05:27,554 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -1.8693610429763794 param sum :: 128.0
2023-01-07 09:05:27,554 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.833906173706055
2023-01-07 09:05:27,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -1.2594480514526367
2023-01-07 09:05:27,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.459491491317749
2023-01-07 09:05:27,556 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -2.2534313201904297 param sum :: 20.833906173706055
2023-01-07 09:05:27,556 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:27,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.07076230645179749
2023-01-07 09:05:27,556 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.26195812225341797
2023-01-07 09:05:27,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1140007972717285
2023-01-07 09:05:27,557 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.02901628613471985 param sum :: 512.0
2023-01-07 09:05:27,557 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -48.606842041015625
2023-01-07 09:05:27,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 6.767419815063477
2023-01-07 09:05:27,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.253061294555664
2023-01-07 09:05:27,559 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 5.2940216064453125 param sum :: -48.606842041015625
2023-01-07 09:05:27,559 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,559 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.05157113075256348
2023-01-07 09:05:27,559 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,559 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 38.55685806274414
2023-01-07 09:05:27,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.370092391967773
2023-01-07 09:05:27,561 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.09600305557250977 param sum :: 128.0
2023-01-07 09:05:27,561 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,561 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,561 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -34.13706588745117
2023-01-07 09:05:27,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 56.78287887573242
2023-01-07 09:05:27,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.970855236053467
2023-01-07 09:05:27,562 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 49.787105560302734 param sum :: -34.13706588745117
2023-01-07 09:05:27,562 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:27,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.4713442325592041
2023-01-07 09:05:27,562 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -9.520626068115234
2023-01-07 09:05:27,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.795194149017334
2023-01-07 09:05:27,564 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.6602859497070312 param sum :: 128.0
2023-01-07 09:05:27,564 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 11.814714431762695
2023-01-07 09:05:27,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -1.2440686225891113
2023-01-07 09:05:27,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.142592191696167
2023-01-07 09:05:27,565 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -2.1126861572265625 param sum :: 11.814714431762695
2023-01-07 09:05:27,565 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,565 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:27,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.5115025043487549
2023-01-07 09:05:27,566 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,566 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 7.553481101989746
2023-01-07 09:05:27,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9183222055435181
2023-01-07 09:05:27,567 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.2745370864868164 param sum :: 512.0
2023-01-07 09:05:27,567 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,567 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -26.576377868652344
2023-01-07 09:05:27,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 1.7083091735839844
2023-01-07 09:05:27,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.072365760803223
2023-01-07 09:05:27,568 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -1.34869384765625 param sum :: -26.576377868652344
2023-01-07 09:05:27,568 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:27,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.39463549852371216
2023-01-07 09:05:27,569 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -607.7521362304688
2023-01-07 09:05:27,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.537210464477539
2023-01-07 09:05:27,570 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.2916923761367798 param sum :: 256.0
2023-01-07 09:05:27,570 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,570 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,570 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.7762942314147949
2023-01-07 09:05:27,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -610.1920776367188
2023-01-07 09:05:27,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.321929931640625
2023-01-07 09:05:27,572 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -607.3749389648438 param sum :: -0.7762942314147949
2023-01-07 09:05:27,572 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:27,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 136.953369140625
2023-01-07 09:05:27,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4009480476379395
2023-01-07 09:05:27,573 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.981257975101471 param sum :: 256.0
2023-01-07 09:05:27,573 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:27,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 135.92367553710938
2023-01-07 09:05:27,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.680821418762207
2023-01-07 09:05:27,575 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 134.01504516601562 param sum :: 10.802434921264648
2023-01-07 09:05:27,575 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:27,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 412.3270568847656
2023-01-07 09:05:27,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.112208127975464
2023-01-07 09:05:27,576 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -6.237510681152344 param sum :: 1024.0
2023-01-07 09:05:27,576 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,576 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,576 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:27,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 417.8958740234375
2023-01-07 09:05:27,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1696479320526123
2023-01-07 09:05:27,578 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 412.76123046875 param sum :: -34.55324935913086
2023-01-07 09:05:27,578 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:27,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 253.89022827148438
2023-01-07 09:05:27,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2151474952697754
2023-01-07 09:05:27,579 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -3.502209424972534 param sum :: 1024.0
2023-01-07 09:05:27,579 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,579 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,579 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:27,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,580 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 252.73150634765625
2023-01-07 09:05:27,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9447810649871826
2023-01-07 09:05:27,580 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 248.92800903320312 param sum :: -82.27286529541016
2023-01-07 09:05:27,581 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,581 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:27,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.11218541860580444
2023-01-07 09:05:27,581 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:05:27,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 570.09619140625
2023-01-07 09:05:27,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.140560150146484
2023-01-07 09:05:27,582 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.14835256338119507 param sum :: 256.0
2023-01-07 09:05:27,582 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -26.810012817382812
2023-01-07 09:05:27,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,583 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 565.1771240234375
2023-01-07 09:05:27,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.600461483001709
2023-01-07 09:05:27,584 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 565.903564453125 param sum :: -26.810012817382812
2023-01-07 09:05:27,584 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,584 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:27,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,584 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 148.088623046875
2023-01-07 09:05:27,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.762285053730011
2023-01-07 09:05:27,585 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -1.631866455078125 param sum :: 256.0
2023-01-07 09:05:27,585 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:27,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,586 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 149.708251953125
2023-01-07 09:05:27,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1639233827590942
2023-01-07 09:05:27,587 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 150.43885803222656 param sum :: 23.764698028564453
2023-01-07 09:05:27,587 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:27,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.733506202697754
2023-01-07 09:05:27,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6272110939025879
2023-01-07 09:05:27,588 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.021064937114715576 param sum :: 1024.0
2023-01-07 09:05:27,588 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:27,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -10.757987976074219
2023-01-07 09:05:27,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4164061546325684
2023-01-07 09:05:27,590 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -11.153127670288086 param sum :: -33.003875732421875
2023-01-07 09:05:27,590 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,590 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:27,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 16.26455307006836
2023-01-07 09:05:27,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0641766786575317
2023-01-07 09:05:27,591 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.730178713798523 param sum :: 256.0
2023-01-07 09:05:27,591 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,591 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:27,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 14.255295753479004
2023-01-07 09:05:27,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1736743450164795
2023-01-07 09:05:27,593 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 8.919455528259277 param sum :: 39.414920806884766
2023-01-07 09:05:27,593 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:27,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 3.2196435928344727
2023-01-07 09:05:27,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8756527900695801
2023-01-07 09:05:27,594 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.3065292537212372 param sum :: 256.0
2023-01-07 09:05:27,594 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,594 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,594 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:27,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 6.072601318359375
2023-01-07 09:05:27,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6836608648300171
2023-01-07 09:05:27,596 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 7.4306488037109375 param sum :: 7.077023506164551
2023-01-07 09:05:27,596 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:27,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -6.334691047668457
2023-01-07 09:05:27,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5189392566680908
2023-01-07 09:05:27,597 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.6075690984725952 param sum :: 1024.0
2023-01-07 09:05:27,597 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:27,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,597 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -6.825799942016602
2023-01-07 09:05:27,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.023860454559326
2023-01-07 09:05:27,599 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -6.459719657897949 param sum :: -1.5680561065673828
2023-01-07 09:05:27,599 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:27,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 421.46527099609375
2023-01-07 09:05:27,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18028464913368225
2023-01-07 09:05:27,600 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.1310305893421173 param sum :: 256.0
2023-01-07 09:05:27,600 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,600 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:27,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,600 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 422.1246032714844
2023-01-07 09:05:27,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6444256901741028
2023-01-07 09:05:27,602 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 421.1495361328125 param sum :: -28.745033264160156
2023-01-07 09:05:27,602 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,602 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,602 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:27,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 8.36670207977295
2023-01-07 09:05:27,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36643773317337036
2023-01-07 09:05:27,603 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.2407076358795166 param sum :: 256.0
2023-01-07 09:05:27,603 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:27,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 8.920166969299316
2023-01-07 09:05:27,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.508256435394287
2023-01-07 09:05:27,604 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 8.636372566223145 param sum :: -5.068355560302734
2023-01-07 09:05:27,605 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:27,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -36.906883239746094
2023-01-07 09:05:27,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11375700682401657
2023-01-07 09:05:27,606 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.3096533715724945 param sum :: 1024.0
2023-01-07 09:05:27,606 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,606 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:27,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,606 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -35.47657012939453
2023-01-07 09:05:27,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35614365339279175
2023-01-07 09:05:27,607 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -34.465660095214844 param sum :: -63.574039459228516
2023-01-07 09:05:27,608 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,608 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:27,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -196.45101928710938
2023-01-07 09:05:27,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.780979633331299
2023-01-07 09:05:27,609 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.20743024349212646 param sum :: 256.0
2023-01-07 09:05:27,609 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:27,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -196.01287841796875
2023-01-07 09:05:27,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9493681192398071
2023-01-07 09:05:27,610 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -192.38568115234375 param sum :: 41.2066535949707
2023-01-07 09:05:27,610 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:27,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1617.054443359375
2023-01-07 09:05:27,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2373783588409424
2023-01-07 09:05:27,612 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.09028920531272888 param sum :: 256.0
2023-01-07 09:05:27,612 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:27,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1616.9443359375
2023-01-07 09:05:27,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1939961910247803
2023-01-07 09:05:27,613 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1617.122314453125 param sum :: -6.652504920959473
2023-01-07 09:05:27,613 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,614 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:27,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 472.4508361816406
2023-01-07 09:05:27,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7681522369384766
2023-01-07 09:05:27,615 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.19691482186317444 param sum :: 1024.0
2023-01-07 09:05:27,615 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:27,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 474.4598388671875
2023-01-07 09:05:27,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2507489025592804
2023-01-07 09:05:27,616 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 474.01312255859375 param sum :: -52.449466705322266
2023-01-07 09:05:27,616 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:27,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 563.0032348632812
2023-01-07 09:05:27,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18140561878681183
2023-01-07 09:05:27,618 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.405887633562088 param sum :: 256.0
2023-01-07 09:05:27,618 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:27,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 560.65234375
2023-01-07 09:05:27,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3705127239227295
2023-01-07 09:05:27,619 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 559.6693725585938 param sum :: -38.22235870361328
2023-01-07 09:05:27,619 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,619 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:27,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 503.9757995605469
2023-01-07 09:05:27,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3090873658657074
2023-01-07 09:05:27,621 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.4658384919166565 param sum :: 256.0
2023-01-07 09:05:27,621 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,621 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:27,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,621 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 503.7493896484375
2023-01-07 09:05:27,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7204861640930176
2023-01-07 09:05:27,622 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 503.39398193359375 param sum :: -3.3066205978393555
2023-01-07 09:05:27,622 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,622 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:27,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1053.4254150390625
2023-01-07 09:05:27,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.567455768585205
2023-01-07 09:05:27,624 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.07597704231739044 param sum :: 1024.0
2023-01-07 09:05:27,624 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,624 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:27,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1054.48046875
2023-01-07 09:05:27,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2285962104797363
2023-01-07 09:05:27,625 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1054.235107421875 param sum :: 70.22351837158203
2023-01-07 09:05:27,625 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:27,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1730.29150390625
2023-01-07 09:05:27,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7185949087142944
2023-01-07 09:05:27,627 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5169562101364136 param sum :: 512.0
2023-01-07 09:05:27,627 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:27,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,627 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1729.5048828125
2023-01-07 09:05:27,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9110180139541626
2023-01-07 09:05:27,628 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 1729.754150390625 param sum :: 8.939680099487305
2023-01-07 09:05:27,628 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:27,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,628 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -469.000732421875
2023-01-07 09:05:27,629 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1968790292739868
2023-01-07 09:05:27,630 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.1355060189962387 param sum :: 512.0
2023-01-07 09:05:27,630 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:27,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,630 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -469.0049133300781
2023-01-07 09:05:27,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1402927190065384
2023-01-07 09:05:27,631 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -469.84747314453125 param sum :: 20.04633331298828
2023-01-07 09:05:27,631 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:27,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,632 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2553.6083984375
2023-01-07 09:05:27,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2132818102836609
2023-01-07 09:05:27,633 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.8091170787811279 param sum :: 2048.0
2023-01-07 09:05:27,633 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:27,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,633 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2553.2890625
2023-01-07 09:05:27,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.048097047954797745
2023-01-07 09:05:27,634 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2552.49755859375 param sum :: -41.879310607910156
2023-01-07 09:05:27,634 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:27,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,634 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1172.751708984375
2023-01-07 09:05:27,634 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.549850344657898
2023-01-07 09:05:27,635 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.8180601596832275 param sum :: 2048.0
2023-01-07 09:05:27,635 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,636 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:27,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,636 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1172.54296875
2023-01-07 09:05:27,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23320844769477844
2023-01-07 09:05:27,637 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1173.0028076171875 param sum :: 1.8718643188476562
2023-01-07 09:05:27,637 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:27,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 758.138427734375
2023-01-07 09:05:27,637 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42586708068847656
2023-01-07 09:05:27,638 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.15424905717372894 param sum :: 512.0
2023-01-07 09:05:27,638 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:27,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,639 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 758.7711181640625
2023-01-07 09:05:27,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8679866194725037
2023-01-07 09:05:27,640 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 759.052978515625 param sum :: -16.520038604736328
2023-01-07 09:05:27,640 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:27,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,640 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2266.29931640625
2023-01-07 09:05:27,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24704602360725403
2023-01-07 09:05:27,641 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.036151424050331116 param sum :: 512.0
2023-01-07 09:05:27,641 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:27,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,642 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2266.4248046875
2023-01-07 09:05:27,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1491144895553589
2023-01-07 09:05:27,642 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2266.76220703125 param sum :: -14.060759544372559
2023-01-07 09:05:27,643 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:27,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,643 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8471.53515625
2023-01-07 09:05:27,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3353496789932251
2023-01-07 09:05:27,644 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.41684412956237793 param sum :: 2048.0
2023-01-07 09:05:27,644 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:27,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,644 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8471.1484375
2023-01-07 09:05:27,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3757462799549103
2023-01-07 09:05:27,645 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8471.0634765625 param sum :: 35.38753890991211
2023-01-07 09:05:27,645 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:27,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,646 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4515.005859375
2023-01-07 09:05:27,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2540707588195801
2023-01-07 09:05:27,647 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -66.55006408691406 param sum :: 512.0
2023-01-07 09:05:27,647 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:27,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,647 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4515.1416015625
2023-01-07 09:05:27,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0232849083840847
2023-01-07 09:05:27,648 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 4515.19677734375 param sum :: -33.75589370727539
2023-01-07 09:05:27,648 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:27,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,649 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -2332.2060546875
2023-01-07 09:05:27,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05442424863576889
2023-01-07 09:05:27,650 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.06873747706413269 param sum :: 512.0
2023-01-07 09:05:27,650 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,650 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,650 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:27,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,650 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -2332.33935546875
2023-01-07 09:05:27,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05011966824531555
2023-01-07 09:05:27,651 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -2332.137451171875 param sum :: 18.47730255126953
2023-01-07 09:05:27,651 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:27,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,652 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13797.7666015625
2023-01-07 09:05:27,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5997939109802246
2023-01-07 09:05:27,653 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -348.4742736816406 param sum :: 2048.0
2023-01-07 09:05:27,653 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:27,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,653 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13800.634765625
2023-01-07 09:05:27,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.023226261138916
2023-01-07 09:05:27,654 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 13801.142578125 param sum :: 9.326160430908203
2023-01-07 09:05:27,654 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:27,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:27,655 > [DEBUG] 0 :: 6.940236568450928
2023-01-07 09:05:27,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,657 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.043975830078125
2023-01-07 09:05:27,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -314.593505859375
2023-01-07 09:05:27,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,660 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.11794581264257431
2023-01-07 09:05:27,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,660 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -232.078125
2023-01-07 09:05:27,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -376.3130187988281
2023-01-07 09:05:27,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,663 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 0.7230198383331299
2023-01-07 09:05:27,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06802335381507874
2023-01-07 09:05:27,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,665 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.019381150603294373
2023-01-07 09:05:27,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -61.947479248046875
2023-01-07 09:05:27,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8383772969245911
2023-01-07 09:05:27,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 9.290266990661621
2023-01-07 09:05:27,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.398341417312622
2023-01-07 09:05:27,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.029377620667219162
2023-01-07 09:05:27,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 9.381290435791016
2023-01-07 09:05:27,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12213217467069626
2023-01-07 09:05:27,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,672 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 22.365558624267578
2023-01-07 09:05:27,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.995616614818573
2023-01-07 09:05:27,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,673 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.04442031681537628
2023-01-07 09:05:27,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 23.522186279296875
2023-01-07 09:05:27,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07704035937786102
2023-01-07 09:05:27,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,675 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 8.174276351928711
2023-01-07 09:05:27,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12322483211755753
2023-01-07 09:05:27,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0270382147282362
2023-01-07 09:05:27,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 9.708904266357422
2023-01-07 09:05:27,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6234382390975952
2023-01-07 09:05:27,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,677 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.188814163208008
2023-01-07 09:05:27,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.469819575548172
2023-01-07 09:05:27,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,678 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.0982591062784195
2023-01-07 09:05:27,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,678 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.188814163208008
2023-01-07 09:05:27,678 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07573135197162628
2023-01-07 09:05:27,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,679 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.682002305984497
2023-01-07 09:05:27,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.308203101158142
2023-01-07 09:05:27,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.20858031511306763
2023-01-07 09:05:27,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.0074331760406494
2023-01-07 09:05:27,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.327412486076355
2023-01-07 09:05:27,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,682 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.082889556884766
2023-01-07 09:05:27,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4397834241390228
2023-01-07 09:05:27,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,682 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.07831746339797974
2023-01-07 09:05:27,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,683 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.082889556884766
2023-01-07 09:05:27,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.396423816680908
2023-01-07 09:05:27,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,684 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.280233383178711
2023-01-07 09:05:27,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18117685616016388
2023-01-07 09:05:27,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,685 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.03383062779903412
2023-01-07 09:05:27,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,685 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 11.525077819824219
2023-01-07 09:05:27,685 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.934087038040161
2023-01-07 09:05:27,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,686 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 66.92056274414062
2023-01-07 09:05:27,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4848187565803528
2023-01-07 09:05:27,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,687 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.016269274055957794
2023-01-07 09:05:27,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,687 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 66.92056274414062
2023-01-07 09:05:27,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.374113082885742
2023-01-07 09:05:27,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,688 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 20.29613494873047
2023-01-07 09:05:27,689 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3276980221271515
2023-01-07 09:05:27,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,689 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.010118681937456131
2023-01-07 09:05:27,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,690 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 20.78828239440918
2023-01-07 09:05:27,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.912334442138672
2023-01-07 09:05:27,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,691 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 9.234964370727539
2023-01-07 09:05:27,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.41646048426628113
2023-01-07 09:05:27,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,692 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.2366281896829605
2023-01-07 09:05:27,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,692 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 10.411675453186035
2023-01-07 09:05:27,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3004615306854248
2023-01-07 09:05:27,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,693 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 20.70054054260254
2023-01-07 09:05:27,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5636554956436157
2023-01-07 09:05:27,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,694 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.18198101222515106
2023-01-07 09:05:27,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,694 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 20.780757904052734
2023-01-07 09:05:27,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.586832284927368
2023-01-07 09:05:27,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,695 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 13.012977600097656
2023-01-07 09:05:27,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8536241054534912
2023-01-07 09:05:27,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,696 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.0031237229704856873
2023-01-07 09:05:27,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,697 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 14.296685218811035
2023-01-07 09:05:27,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.709801197052002
2023-01-07 09:05:27,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,698 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 15.06430721282959
2023-01-07 09:05:27,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1580318957567215
2023-01-07 09:05:27,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.006114702671766281
2023-01-07 09:05:27,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 15.0946683883667
2023-01-07 09:05:27,699 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9043582677841187
2023-01-07 09:05:27,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,700 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 31.459625244140625
2023-01-07 09:05:27,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7406504154205322
2023-01-07 09:05:27,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,701 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.046581968665122986
2023-01-07 09:05:27,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,701 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 28.21991729736328
2023-01-07 09:05:27,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7585277557373047
2023-01-07 09:05:27,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.04951286315918
2023-01-07 09:05:27,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5321968793869019
2023-01-07 09:05:27,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,703 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.1099218875169754
2023-01-07 09:05:27,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,703 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 7.338380336761475
2023-01-07 09:05:27,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1460041999816895
2023-01-07 09:05:27,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,704 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 12.724542617797852
2023-01-07 09:05:27,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5652785301208496
2023-01-07 09:05:27,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,705 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.006187915802001953
2023-01-07 09:05:27,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,705 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 7.641622543334961
2023-01-07 09:05:27,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2740463018417358
2023-01-07 09:05:27,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,707 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 50.274070739746094
2023-01-07 09:05:27,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02423398196697235
2023-01-07 09:05:27,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,707 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.21060073375701904
2023-01-07 09:05:27,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 44.797706604003906
2023-01-07 09:05:27,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.403614044189453
2023-01-07 09:05:27,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,709 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 10.831812858581543
2023-01-07 09:05:27,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.226918563246727
2023-01-07 09:05:27,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,710 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1416534036397934
2023-01-07 09:05:27,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,710 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.711055755615234
2023-01-07 09:05:27,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.188955068588257
2023-01-07 09:05:27,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,711 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 18.311595916748047
2023-01-07 09:05:27,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8132762908935547
2023-01-07 09:05:27,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.08598655462265015
2023-01-07 09:05:27,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 14.23717212677002
2023-01-07 09:05:27,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5151190757751465
2023-01-07 09:05:27,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,713 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 23.282752990722656
2023-01-07 09:05:27,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5017818212509155
2023-01-07 09:05:27,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,714 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.10490112006664276
2023-01-07 09:05:27,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,714 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 22.136157989501953
2023-01-07 09:05:27,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.99452018737793
2023-01-07 09:05:27,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 14.055520057678223
2023-01-07 09:05:27,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.39905047416687
2023-01-07 09:05:27,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,716 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.0692431628704071
2023-01-07 09:05:27,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 16.667312622070312
2023-01-07 09:05:27,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0120067596435547
2023-01-07 09:05:27,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,718 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 44.142433166503906
2023-01-07 09:05:27,718 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.52264142036438
2023-01-07 09:05:27,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.22679638862609863
2023-01-07 09:05:27,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 37.07511901855469
2023-01-07 09:05:27,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7287702560424805
2023-01-07 09:05:27,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 45.27415084838867
2023-01-07 09:05:27,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2090117931365967
2023-01-07 09:05:27,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,721 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 53.2636604309082
2023-01-07 09:05:27,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31553149223327637
2023-01-07 09:05:27,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -9.879922866821289
2023-01-07 09:05:27,722 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9969176054000854
2023-01-07 09:05:27,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,723 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.0465781688690186
2023-01-07 09:05:27,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,723 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -2.021777868270874
2023-01-07 09:05:27,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.815637588500977
2023-01-07 09:05:27,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 88.67593383789062
2023-01-07 09:05:27,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35599541664123535
2023-01-07 09:05:27,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.2617010474205017
2023-01-07 09:05:27,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 86.43448638916016
2023-01-07 09:05:27,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.180159091949463
2023-01-07 09:05:27,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 70.4431381225586
2023-01-07 09:05:27,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0110905170440674
2023-01-07 09:05:27,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.29087504744529724
2023-01-07 09:05:27,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,728 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 65.48187255859375
2023-01-07 09:05:27,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.337821960449219
2023-01-07 09:05:27,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,729 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 152.21945190429688
2023-01-07 09:05:27,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.751714706420898
2023-01-07 09:05:27,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 150.21791076660156
2023-01-07 09:05:27,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.820606231689453
2023-01-07 09:05:27,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,731 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 14.731975555419922
2023-01-07 09:05:27,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.322732448577881
2023-01-07 09:05:27,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -21.962305068969727
2023-01-07 09:05:27,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.6691813468933105
2023-01-07 09:05:27,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 11.719680786132812
2023-01-07 09:05:27,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7224137187004089
2023-01-07 09:05:27,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,734 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -24.959304809570312
2023-01-07 09:05:27,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7755675315856934
2023-01-07 09:05:27,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.25550079345703
2023-01-07 09:05:27,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.530817031860352
2023-01-07 09:05:27,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,736 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 49.80484390258789
2023-01-07 09:05:27,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.664928436279297
2023-01-07 09:05:27,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -9.748085021972656
2023-01-07 09:05:27,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7817678451538086
2023-01-07 09:05:27,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,738 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -33.932518005371094
2023-01-07 09:05:27,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.313575744628906
2023-01-07 09:05:27,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,739 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 8.492960929870605
2023-01-07 09:05:27,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.069692373275757
2023-01-07 09:05:27,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -25.0870361328125
2023-01-07 09:05:27,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.5634918212890625
2023-01-07 09:05:27,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,741 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 26.163524627685547
2023-01-07 09:05:27,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.561588287353516
2023-01-07 09:05:27,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,741 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -35.19588088989258
2023-01-07 09:05:27,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.878283977508545
2023-01-07 09:05:27,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -70.42009735107422
2023-01-07 09:05:27,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.280316352844238
2023-01-07 09:05:27,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.08407402038574219
2023-01-07 09:05:27,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -219.46890258789062
2023-01-07 09:05:27,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.908913612365723
2023-01-07 09:05:27,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 38.618553161621094
2023-01-07 09:05:27,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3171337842941284
2023-01-07 09:05:27,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -66.99899291992188
2023-01-07 09:05:27,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1572089195251465
2023-01-07 09:05:27,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,747 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 42.34014892578125
2023-01-07 09:05:27,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.090471267700195
2023-01-07 09:05:27,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,748 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -63.71800231933594
2023-01-07 09:05:27,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.539491653442383
2023-01-07 09:05:27,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,749 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 46.011878967285156
2023-01-07 09:05:27,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.277419090270996
2023-01-07 09:05:27,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,750 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.8925861120223999
2023-01-07 09:05:27,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,750 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -115.4919662475586
2023-01-07 09:05:27,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.123117446899414
2023-01-07 09:05:27,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,751 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -14.834024429321289
2023-01-07 09:05:27,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5051398277282715
2023-01-07 09:05:27,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,752 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -49.79990005493164
2023-01-07 09:05:27,752 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.764688491821289
2023-01-07 09:05:27,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,753 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 34.371849060058594
2023-01-07 09:05:27,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.01388168334961
2023-01-07 09:05:27,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,754 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -6.166255950927734
2023-01-07 09:05:27,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.431859970092773
2023-01-07 09:05:27,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,755 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 126.88900756835938
2023-01-07 09:05:27,755 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.59128761291504
2023-01-07 09:05:27,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,756 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.30482816696167
2023-01-07 09:05:27,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,756 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 88.0736083984375
2023-01-07 09:05:27,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.3619499206543
2023-01-07 09:05:27,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,757 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 88.70010375976562
2023-01-07 09:05:27,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.435031890869141
2023-01-07 09:05:27,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,758 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.7302095890045166
2023-01-07 09:05:27,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 61.688377380371094
2023-01-07 09:05:27,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,759 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 48.29813766479492
2023-01-07 09:05:27,759 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.418045043945312
2023-01-07 09:05:27,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,760 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 48.01552963256836
2023-01-07 09:05:27,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.863380432128906
2023-01-07 09:05:27,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,761 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 127.18724822998047
2023-01-07 09:05:27,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.373313903808594
2023-01-07 09:05:27,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,762 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 49.0009651184082
2023-01-07 09:05:27,762 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.300716400146484
2023-01-07 09:05:27,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,763 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.7360851168632507
2023-01-07 09:05:27,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,763 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 38.122772216796875
2023-01-07 09:05:27,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 103.50958251953125
2023-01-07 09:05:27,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,764 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 108.32818603515625
2023-01-07 09:05:27,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.12156677246094
2023-01-07 09:05:27,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,765 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.4401679039001465
2023-01-07 09:05:27,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,765 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 9.609277725219727
2023-01-07 09:05:27,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.11524200439453
2023-01-07 09:05:27,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,766 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 193.90626525878906
2023-01-07 09:05:27,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.38684844970703
2023-01-07 09:05:27,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,767 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 72.68482971191406
2023-01-07 09:05:27,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,767 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 9.112850189208984
2023-01-07 09:05:27,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.248647689819336
2023-01-07 09:05:27,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,769 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 227.38584899902344
2023-01-07 09:05:27,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.142575740814209
2023-01-07 09:05:27,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,769 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 165.77230834960938
2023-01-07 09:05:27,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.412097930908203
2023-01-07 09:05:27,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,771 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 256.6944580078125
2023-01-07 09:05:27,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.76012420654297
2023-01-07 09:05:27,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,772 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.6858582496643066
2023-01-07 09:05:27,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,772 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 12.091901779174805
2023-01-07 09:05:27,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,772 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 261.8126525878906
2023-01-07 09:05:27,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,773 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 53.53121566772461
2023-01-07 09:05:27,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.75811767578125
2023-01-07 09:05:27,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,774 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 280.21649169921875
2023-01-07 09:05:27,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0108330249786377
2023-01-07 09:05:27,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,775 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 43.97501754760742
2023-01-07 09:05:27,775 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.485130310058594
2023-01-07 09:05:27,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,776 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 325.6922607421875
2023-01-07 09:05:27,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.254626989364624
2023-01-07 09:05:27,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,777 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 239.95303344726562
2023-01-07 09:05:27,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,777 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 57.82223129272461
2023-01-07 09:05:27,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.64139556884766
2023-01-07 09:05:27,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,778 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 304.9456481933594
2023-01-07 09:05:27,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.478710174560547
2023-01-07 09:05:27,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,779 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 1.6051671504974365
2023-01-07 09:05:27,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,779 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 216.19558715820312
2023-01-07 09:05:27,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.572982788085938
2023-01-07 09:05:27,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,780 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 183.1798858642578
2023-01-07 09:05:27,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 299.7302551269531
2023-01-07 09:05:27,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,781 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -18.527366638183594
2023-01-07 09:05:27,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.124317169189453
2023-01-07 09:05:27,784 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:05:27,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,784 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -4518.60986328125
2023-01-07 09:05:27,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,785 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -0.5111846923828125
2023-01-07 09:05:27,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 57.965755462646484
2023-01-07 09:05:27,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 10.883634567260742
2023-01-07 09:05:27,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -82.13499450683594
2023-01-07 09:05:27,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1651.0755615234375
2023-01-07 09:05:27,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -2879.259033203125
2023-01-07 09:05:27,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -831.452392578125
2023-01-07 09:05:27,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,787 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -307.59637451171875
2023-01-07 09:05:27,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,787 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -80.11328125
2023-01-07 09:05:27,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,787 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -88.43106079101562
2023-01-07 09:05:27,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -92.44609069824219
2023-01-07 09:05:27,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 9.181739807128906
2023-01-07 09:05:27,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -90.78643798828125
2023-01-07 09:05:27,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -55.81407165527344
2023-01-07 09:05:27,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 152.4049072265625
2023-01-07 09:05:27,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 65.02581787109375
2023-01-07 09:05:27,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 91.50657653808594
2023-01-07 09:05:27,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 16.14059066772461
2023-01-07 09:05:27,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 134.69970703125
2023-01-07 09:05:27,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 48.330726623535156
2023-01-07 09:05:27,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 55.07781982421875
2023-01-07 09:05:27,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 22.50135612487793
2023-01-07 09:05:27,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 7.828677177429199
2023-01-07 09:05:27,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 19.14632797241211
2023-01-07 09:05:27,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 42.97890853881836
2023-01-07 09:05:27,792 > [DEBUG] 0 :: before allreduce fusion buffer :: -4039.372314453125
2023-01-07 09:05:27,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 7.186142444610596
2023-01-07 09:05:27,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.81557846069336
2023-01-07 09:05:27,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 29.926069259643555
2023-01-07 09:05:27,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 17.142253875732422
2023-01-07 09:05:27,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.604546546936035
2023-01-07 09:05:27,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 23.49546241760254
2023-01-07 09:05:27,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.868302345275879
2023-01-07 09:05:27,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,794 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 23.765823364257812
2023-01-07 09:05:27,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 63.658660888671875
2023-01-07 09:05:27,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 194.7095489501953
2023-01-07 09:05:27,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.38686752319336
2023-01-07 09:05:27,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 15.073354721069336
2023-01-07 09:05:27,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -5.871025085449219
2023-01-07 09:05:27,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.112544059753418
2023-01-07 09:05:27,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.324722290039062
2023-01-07 09:05:27,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 12.260711669921875
2023-01-07 09:05:27,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 23.780473709106445
2023-01-07 09:05:27,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 10.05099868774414
2023-01-07 09:05:27,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.50584411621094
2023-01-07 09:05:27,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -615.6720581054688
2023-01-07 09:05:27,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:27,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:27,798 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -408.1085205078125
2023-01-07 09:05:27,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 689.55126953125
2023-01-07 09:05:28,642 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 537.0623168945312 param sum :: 0.22188237309455872
2023-01-07 09:05:28,642 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:28,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,643 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -2.23862886428833
2023-01-07 09:05:28,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,643 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 16.435108184814453
2023-01-07 09:05:28,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.940839767456055
2023-01-07 09:05:28,645 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -7.1597065925598145 param sum :: 64.0
2023-01-07 09:05:28,645 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 5.294054985046387
2023-01-07 09:05:28,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,645 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 154.1877899169922
2023-01-07 09:05:28,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,645 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 6.052410125732422
2023-01-07 09:05:28,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 151.40139770507812
2023-01-07 09:05:28,647 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 159.42578125 param sum :: 5.294054985046387
2023-01-07 09:05:28,647 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:28,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 3.2724952697753906
2023-01-07 09:05:28,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 147.7830047607422
2023-01-07 09:05:28,648 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 2.2848148345947266 param sum :: 64.0
2023-01-07 09:05:28,648 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:28,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -27.026962280273438
2023-01-07 09:05:28,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.11980438232422
2023-01-07 09:05:28,649 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 8.551856994628906 param sum :: 5.982826232910156
2023-01-07 09:05:28,649 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,650 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,650 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:28,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,650 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.7020590305328369
2023-01-07 09:05:28,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,650 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 41.75640869140625
2023-01-07 09:05:28,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,650 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 254.54208374023438
2023-01-07 09:05:28,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.640331268310547
2023-01-07 09:05:28,652 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -0.5115184783935547 param sum :: 64.0
2023-01-07 09:05:28,652 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:28,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,652 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 219.58460998535156
2023-01-07 09:05:28,652 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.31251335144043
2023-01-07 09:05:28,653 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -7.78179931640625 param sum :: 11.617742538452148
2023-01-07 09:05:28,653 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:28,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,653 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -3.6648716926574707
2023-01-07 09:05:28,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,654 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 139.05398559570312
2023-01-07 09:05:28,654 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.445028305053711
2023-01-07 09:05:28,655 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -8.225821495056152 param sum :: 256.0
2023-01-07 09:05:28,655 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:28,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,655 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 286.4220275878906
2023-01-07 09:05:28,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3904266357421875
2023-01-07 09:05:28,656 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 377.8894958496094 param sum :: -10.471250534057617
2023-01-07 09:05:28,656 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:28,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,657 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 382.9000244140625
2023-01-07 09:05:28,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.546451568603516
2023-01-07 09:05:28,658 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 10.939453125 param sum :: 256.0
2023-01-07 09:05:28,658 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:28,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 386.2628173828125
2023-01-07 09:05:28,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.86461067199707
2023-01-07 09:05:28,659 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 190.70777893066406 param sum :: -38.087059020996094
2023-01-07 09:05:28,659 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:28,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,659 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -2.3722026348114014
2023-01-07 09:05:28,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 365.9538269042969
2023-01-07 09:05:28,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 223.08340454101562
2023-01-07 09:05:28,661 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -2.392103672027588 param sum :: 64.0
2023-01-07 09:05:28,661 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,661 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,661 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:28,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,661 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 293.3308410644531
2023-01-07 09:05:28,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,662 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 75.57479858398438
2023-01-07 09:05:28,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.54098892211914
2023-01-07 09:05:28,663 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 273.16082763671875 param sum :: -9.83746337890625
2023-01-07 09:05:28,663 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:28,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -2.1209046840667725
2023-01-07 09:05:28,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 236.41868591308594
2023-01-07 09:05:28,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.17277526855469
2023-01-07 09:05:28,664 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -5.683833122253418 param sum :: 64.0
2023-01-07 09:05:28,664 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:28,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,665 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 145.95933532714844
2023-01-07 09:05:28,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 91.16000366210938
2023-01-07 09:05:28,666 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 301.3375244140625 param sum :: 13.821467399597168
2023-01-07 09:05:28,666 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:28,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,666 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 185.09835815429688
2023-01-07 09:05:28,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.078002452850342
2023-01-07 09:05:28,667 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -1.784162998199463 param sum :: 256.0
2023-01-07 09:05:28,667 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:28,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,668 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 185.3809356689453
2023-01-07 09:05:28,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,668 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 106.515869140625
2023-01-07 09:05:28,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5293288230895996
2023-01-07 09:05:28,669 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 184.63571166992188 param sum :: -16.297849655151367
2023-01-07 09:05:28,669 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:28,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,669 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 171.75201416015625
2023-01-07 09:05:28,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.7127685546875
2023-01-07 09:05:28,670 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -13.934762954711914 param sum :: 64.0
2023-01-07 09:05:28,671 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:28,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,671 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 175.86610412597656
2023-01-07 09:05:28,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.011873245239258
2023-01-07 09:05:28,672 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 176.63674926757812 param sum :: -1.7540884017944336
2023-01-07 09:05:28,672 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,672 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.6569365859031677
2023-01-07 09:05:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,672 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4383156299591064
2023-01-07 09:05:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 169.88262939453125
2023-01-07 09:05:28,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.48226547241211
2023-01-07 09:05:28,674 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.2805534601211548 param sum :: 64.0
2023-01-07 09:05:28,674 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:28,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 83.18118286132812
2023-01-07 09:05:28,675 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.171839714050293
2023-01-07 09:05:28,676 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -2.2804994583129883 param sum :: -0.8034350872039795
2023-01-07 09:05:28,676 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -17.25152015686035
2023-01-07 09:05:28,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,676 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 152.21240234375
2023-01-07 09:05:28,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.3541059494018555
2023-01-07 09:05:28,677 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -34.18772888183594 param sum :: 256.0
2023-01-07 09:05:28,677 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,677 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:28,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 24.79477310180664
2023-01-07 09:05:28,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.8281450271606445
2023-01-07 09:05:28,678 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 118.22874450683594 param sum :: -17.25152015686035
2023-01-07 09:05:28,678 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,679 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:28,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,679 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 35.207481384277344
2023-01-07 09:05:28,679 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.389631271362305
2023-01-07 09:05:28,680 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -19.426712036132812 param sum :: 128.0
2023-01-07 09:05:28,680 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:28,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 45.27177047729492
2023-01-07 09:05:28,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.71345329284668
2023-01-07 09:05:28,681 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 34.36907958984375 param sum :: -10.020125389099121
2023-01-07 09:05:28,681 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.01441335678100586
2023-01-07 09:05:28,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -24.65725326538086
2023-01-07 09:05:28,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4152021408081055
2023-01-07 09:05:28,683 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -1.9807908535003662 param sum :: 128.0
2023-01-07 09:05:28,683 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,683 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -4.991181373596191
2023-01-07 09:05:28,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,683 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -15.802330017089844
2023-01-07 09:05:28,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.464276075363159
2023-01-07 09:05:28,684 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -25.734786987304688 param sum :: -4.991181373596191
2023-01-07 09:05:28,684 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:28,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7348625063896179
2023-01-07 09:05:28,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -55.819522857666016
2023-01-07 09:05:28,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.278685092926025
2023-01-07 09:05:28,686 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.41083377599716187 param sum :: 512.0
2023-01-07 09:05:28,686 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,686 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,686 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -18.34212875366211
2023-01-07 09:05:28,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -70.88844299316406
2023-01-07 09:05:28,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.720476150512695
2023-01-07 09:05:28,687 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -81.2579116821289 param sum :: -18.34212875366211
2023-01-07 09:05:28,687 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:28,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1679.9766845703125
2023-01-07 09:05:28,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8119580745697021
2023-01-07 09:05:28,689 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -87.58206176757812 param sum :: 512.0
2023-01-07 09:05:28,689 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:28,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1674.8782958984375
2023-01-07 09:05:28,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5794655084609985
2023-01-07 09:05:28,690 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -1676.94384765625 param sum :: 37.5141487121582
2023-01-07 09:05:28,690 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,690 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,690 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.6810164451599121
2023-01-07 09:05:28,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -2779.221923828125
2023-01-07 09:05:28,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.00926208496094
2023-01-07 09:05:28,692 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.9369025230407715 param sum :: 128.0
2023-01-07 09:05:28,692 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,692 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 8.519919395446777
2023-01-07 09:05:28,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -2807.634521484375
2023-01-07 09:05:28,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.950138092041016
2023-01-07 09:05:28,693 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -2839.577392578125 param sum :: 8.519919395446777
2023-01-07 09:05:28,693 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,693 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.4825226664543152
2023-01-07 09:05:28,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -853.9061279296875
2023-01-07 09:05:28,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.574467658996582
2023-01-07 09:05:28,695 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.04450786113739014 param sum :: 128.0
2023-01-07 09:05:28,695 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -17.605867385864258
2023-01-07 09:05:28,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -855.957275390625
2023-01-07 09:05:28,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3057528734207153
2023-01-07 09:05:28,696 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -852.0172119140625 param sum :: -17.605867385864258
2023-01-07 09:05:28,696 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,696 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:28,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -308.6730651855469
2023-01-07 09:05:28,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7284362316131592
2023-01-07 09:05:28,698 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -69.98463439941406 param sum :: 512.0
2023-01-07 09:05:28,698 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,698 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:28,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -330.108642578125
2023-01-07 09:05:28,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0436277389526367
2023-01-07 09:05:28,699 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -328.44305419921875 param sum :: -58.115535736083984
2023-01-07 09:05:28,699 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.1407759189605713
2023-01-07 09:05:28,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -81.9757080078125
2023-01-07 09:05:28,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.580171585083008
2023-01-07 09:05:28,701 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.2886723279953003 param sum :: 128.0
2023-01-07 09:05:28,701 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -14.622330665588379
2023-01-07 09:05:28,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -109.92178344726562
2023-01-07 09:05:28,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.019637107849121
2023-01-07 09:05:28,702 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -121.44447326660156 param sum :: -14.622330665588379
2023-01-07 09:05:28,702 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,702 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.4332992434501648
2023-01-07 09:05:28,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -89.05384063720703
2023-01-07 09:05:28,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0013638734817505
2023-01-07 09:05:28,704 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.7262365818023682 param sum :: 128.0
2023-01-07 09:05:28,704 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.833906173706055
2023-01-07 09:05:28,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,704 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -74.06776428222656
2023-01-07 09:05:28,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8707265853881836
2023-01-07 09:05:28,705 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -73.18451690673828 param sum :: 20.833906173706055
2023-01-07 09:05:28,705 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:28,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,706 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.20516712963581085
2023-01-07 09:05:28,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,706 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -91.30528259277344
2023-01-07 09:05:28,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.95931625366211
2023-01-07 09:05:28,707 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.08035200834274292 param sum :: 512.0
2023-01-07 09:05:28,707 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -48.606842041015625
2023-01-07 09:05:28,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -93.80818176269531
2023-01-07 09:05:28,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.526548385620117
2023-01-07 09:05:28,708 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -88.22193145751953 param sum :: -48.606842041015625
2023-01-07 09:05:28,708 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.12867891788482666
2023-01-07 09:05:28,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 2.1403961181640625
2023-01-07 09:05:28,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.962247848510742
2023-01-07 09:05:28,710 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.03157365322113037 param sum :: 128.0
2023-01-07 09:05:28,710 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,710 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,710 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -34.13706588745117
2023-01-07 09:05:28,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,710 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -22.517221450805664
2023-01-07 09:05:28,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.913078308105469
2023-01-07 09:05:28,712 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -8.476078033447266 param sum :: -34.13706588745117
2023-01-07 09:05:28,712 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,712 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:28,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,712 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.3180128335952759
2023-01-07 09:05:28,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,712 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -93.64811706542969
2023-01-07 09:05:28,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.005521297454834
2023-01-07 09:05:28,714 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.06088027358055115 param sum :: 128.0
2023-01-07 09:05:28,714 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,714 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 11.814714431762695
2023-01-07 09:05:28,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -75.83887481689453
2023-01-07 09:05:28,714 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2646774053573608
2023-01-07 09:05:28,715 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -70.99009704589844 param sum :: 11.814714431762695
2023-01-07 09:05:28,715 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:28,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,715 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.749335765838623
2023-01-07 09:05:28,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,716 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -31.653141021728516
2023-01-07 09:05:28,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22541436553001404
2023-01-07 09:05:28,717 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.657401978969574 param sum :: 512.0
2023-01-07 09:05:28,717 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -26.576377868652344
2023-01-07 09:05:28,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -44.00651931762695
2023-01-07 09:05:28,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.370368242263794
2023-01-07 09:05:28,718 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -39.24204635620117 param sum :: -26.576377868652344
2023-01-07 09:05:28,718 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:28,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,718 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.23660634458065033
2023-01-07 09:05:28,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -458.0701904296875
2023-01-07 09:05:28,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.516693115234375
2023-01-07 09:05:28,720 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.4101502597332001 param sum :: 256.0
2023-01-07 09:05:28,720 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.7762942314147949
2023-01-07 09:05:28,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -453.744384765625
2023-01-07 09:05:28,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.070111274719238
2023-01-07 09:05:28,721 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -448.10986328125 param sum :: -0.7762942314147949
2023-01-07 09:05:28,721 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,721 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:28,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 183.17593383789062
2023-01-07 09:05:28,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.623201370239258
2023-01-07 09:05:28,723 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -4.012960433959961 param sum :: 256.0
2023-01-07 09:05:28,723 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,723 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:28,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,723 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 174.89852905273438
2023-01-07 09:05:28,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7131717205047607
2023-01-07 09:05:28,724 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 180.60572814941406 param sum :: 10.802434921264648
2023-01-07 09:05:28,724 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:28,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 429.84442138671875
2023-01-07 09:05:28,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.286953926086426
2023-01-07 09:05:28,726 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 2.3193483352661133 param sum :: 1024.0
2023-01-07 09:05:28,726 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:28,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,726 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 428.8735656738281
2023-01-07 09:05:28,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0720781087875366
2023-01-07 09:05:28,727 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 433.8811950683594 param sum :: -34.55324935913086
2023-01-07 09:05:28,727 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,727 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:28,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 246.56568908691406
2023-01-07 09:05:28,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.516869068145752
2023-01-07 09:05:28,728 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 8.3895263671875 param sum :: 1024.0
2023-01-07 09:05:28,729 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,729 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:28,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,729 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 251.65927124023438
2023-01-07 09:05:28,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0366997718811035
2023-01-07 09:05:28,730 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 252.451904296875 param sum :: -82.27286529541016
2023-01-07 09:05:28,730 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:28,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.13556209206581116
2023-01-07 09:05:28,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,731 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 688.1221923828125
2023-01-07 09:05:28,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.0618896484375
2023-01-07 09:05:28,732 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.08543999493122101 param sum :: 256.0
2023-01-07 09:05:28,732 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -26.810012817382812
2023-01-07 09:05:28,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 690.1810913085938
2023-01-07 09:05:28,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.051782608032227
2023-01-07 09:05:28,733 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 686.1851806640625 param sum :: -26.810012817382812
2023-01-07 09:05:28,733 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:28,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 148.63829040527344
2023-01-07 09:05:28,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.896607875823975
2023-01-07 09:05:28,735 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -1.628204107284546 param sum :: 256.0
2023-01-07 09:05:28,735 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,735 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:28,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 146.27772521972656
2023-01-07 09:05:28,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8188567161560059
2023-01-07 09:05:28,736 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 149.38461303710938 param sum :: 23.764698028564453
2023-01-07 09:05:28,736 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:28,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 48.24664306640625
2023-01-07 09:05:28,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8463717699050903
2023-01-07 09:05:28,738 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.13293051719665527 param sum :: 1024.0
2023-01-07 09:05:28,738 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:28,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 47.09107971191406
2023-01-07 09:05:28,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2442886233329773
2023-01-07 09:05:28,739 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 52.9521484375 param sum :: -33.003875732421875
2023-01-07 09:05:28,739 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:28,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 8.20893669128418
2023-01-07 09:05:28,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.0471367835998535
2023-01-07 09:05:28,740 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -1.928551197052002 param sum :: 256.0
2023-01-07 09:05:28,741 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:28,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,741 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 9.016874313354492
2023-01-07 09:05:28,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9438257217407227
2023-01-07 09:05:28,742 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 3.855381965637207 param sum :: 39.414920806884766
2023-01-07 09:05:28,742 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:28,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.817899703979492
2023-01-07 09:05:28,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3316705226898193
2023-01-07 09:05:28,743 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -2.187438726425171 param sum :: 256.0
2023-01-07 09:05:28,743 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:28,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,744 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.464773178100586
2023-01-07 09:05:28,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.437681198120117
2023-01-07 09:05:28,745 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -0.8769435882568359 param sum :: 7.077023506164551
2023-01-07 09:05:28,745 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:28,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 5.41029167175293
2023-01-07 09:05:28,745 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31926432251930237
2023-01-07 09:05:28,746 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -3.805866241455078 param sum :: 1024.0
2023-01-07 09:05:28,746 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:28,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 6.933596611022949
2023-01-07 09:05:28,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3948771953582764
2023-01-07 09:05:28,748 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 4.04442834854126 param sum :: -1.5680561065673828
2023-01-07 09:05:28,748 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,748 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,748 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:28,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1086.7921142578125
2023-01-07 09:05:28,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1620612144470215
2023-01-07 09:05:28,749 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -2.1591227054595947 param sum :: 256.0
2023-01-07 09:05:28,749 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,750 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:28,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1081.4576416015625
2023-01-07 09:05:28,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11274296790361404
2023-01-07 09:05:28,751 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 1078.037109375 param sum :: -28.745033264160156
2023-01-07 09:05:28,751 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:28,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,751 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -2.7610244750976562
2023-01-07 09:05:28,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5491204261779785
2023-01-07 09:05:28,752 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -1.9983367919921875 param sum :: 256.0
2023-01-07 09:05:28,752 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,752 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,752 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:28,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.7602596282958984
2023-01-07 09:05:28,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17297953367233276
2023-01-07 09:05:28,754 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1.5761528015136719 param sum :: -5.068355560302734
2023-01-07 09:05:28,754 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:28,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,754 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -67.16767120361328
2023-01-07 09:05:28,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.232851266860962
2023-01-07 09:05:28,755 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -5.224057197570801 param sum :: 1024.0
2023-01-07 09:05:28,755 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,755 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,755 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:28,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -68.96458435058594
2023-01-07 09:05:28,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.844335675239563
2023-01-07 09:05:28,756 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -69.28754425048828 param sum :: -63.574039459228516
2023-01-07 09:05:28,757 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,757 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,757 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:28,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,757 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 295.5593566894531
2023-01-07 09:05:28,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.511793613433838
2023-01-07 09:05:28,758 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.9476801156997681 param sum :: 256.0
2023-01-07 09:05:28,758 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:28,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 301.6112976074219
2023-01-07 09:05:28,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2853305339813232
2023-01-07 09:05:28,759 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 299.8416748046875 param sum :: 41.2066535949707
2023-01-07 09:05:28,759 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,759 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:28,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1454.927490234375
2023-01-07 09:05:28,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0270540714263916
2023-01-07 09:05:28,761 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.7144672274589539 param sum :: 256.0
2023-01-07 09:05:28,761 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,761 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:28,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1454.2169189453125
2023-01-07 09:05:28,761 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7321354150772095
2023-01-07 09:05:28,762 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1453.7972412109375 param sum :: -6.652504920959473
2023-01-07 09:05:28,762 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:28,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,763 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 487.1328125
2023-01-07 09:05:28,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15024027228355408
2023-01-07 09:05:28,764 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.020631730556488037 param sum :: 1024.0
2023-01-07 09:05:28,764 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:28,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 491.70855712890625
2023-01-07 09:05:28,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.209822416305542
2023-01-07 09:05:28,765 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 490.7781982421875 param sum :: -52.449466705322266
2023-01-07 09:05:28,765 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,765 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,765 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:28,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 733.441650390625
2023-01-07 09:05:28,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.939250111579895
2023-01-07 09:05:28,767 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.49366462230682373 param sum :: 256.0
2023-01-07 09:05:28,767 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,767 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,767 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:28,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 728.1968994140625
2023-01-07 09:05:28,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33266615867614746
2023-01-07 09:05:28,768 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 726.7044067382812 param sum :: -38.22235870361328
2023-01-07 09:05:28,768 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:28,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,769 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 461.6203918457031
2023-01-07 09:05:28,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.007322907447814941
2023-01-07 09:05:28,770 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.28800347447395325 param sum :: 256.0
2023-01-07 09:05:28,770 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:28,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 462.1955261230469
2023-01-07 09:05:28,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47383955121040344
2023-01-07 09:05:28,771 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 463.06689453125 param sum :: -3.3066205978393555
2023-01-07 09:05:28,771 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:28,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,772 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1038.458740234375
2023-01-07 09:05:28,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3080900013446808
2023-01-07 09:05:28,773 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 1.6024128198623657 param sum :: 1024.0
2023-01-07 09:05:28,773 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:28,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,773 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1036.4893798828125
2023-01-07 09:05:28,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6418499946594238
2023-01-07 09:05:28,774 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1035.739013671875 param sum :: 70.22351837158203
2023-01-07 09:05:28,774 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,774 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:28,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,775 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3066.43359375
2023-01-07 09:05:28,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5340158939361572
2023-01-07 09:05:28,776 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.20653851330280304 param sum :: 512.0
2023-01-07 09:05:28,776 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:28,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,776 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3062.267578125
2023-01-07 09:05:28,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8697490692138672
2023-01-07 09:05:28,777 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 3063.7880859375 param sum :: 8.939680099487305
2023-01-07 09:05:28,777 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,777 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:28,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,777 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 373.14447021484375
2023-01-07 09:05:28,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7677849531173706
2023-01-07 09:05:28,779 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.08709798753261566 param sum :: 512.0
2023-01-07 09:05:28,779 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:28,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,779 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 371.298828125
2023-01-07 09:05:28,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5552818775177002
2023-01-07 09:05:28,780 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 372.4496154785156 param sum :: 20.04633331298828
2023-01-07 09:05:28,780 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,780 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,780 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:28,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,780 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3236.072509765625
2023-01-07 09:05:28,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.857588529586792
2023-01-07 09:05:28,781 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -0.30610036849975586 param sum :: 2048.0
2023-01-07 09:05:28,782 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,782 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:28,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,782 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3239.15478515625
2023-01-07 09:05:28,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6360819339752197
2023-01-07 09:05:28,783 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 3239.32763671875 param sum :: -41.879310607910156
2023-01-07 09:05:28,783 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:28,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,783 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1149.460693359375
2023-01-07 09:05:28,783 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46342992782592773
2023-01-07 09:05:28,784 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.044985756278038025 param sum :: 2048.0
2023-01-07 09:05:28,784 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,784 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:28,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,785 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1149.31201171875
2023-01-07 09:05:28,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8479232788085938
2023-01-07 09:05:28,786 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1150.3392333984375 param sum :: 1.8718643188476562
2023-01-07 09:05:28,786 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,786 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,786 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:28,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,786 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 5294.9248046875
2023-01-07 09:05:28,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7213500738143921
2023-01-07 09:05:28,787 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 1.0717779397964478 param sum :: 512.0
2023-01-07 09:05:28,787 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,787 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,787 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:28,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,788 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 5294.46337890625
2023-01-07 09:05:28,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1490263044834137
2023-01-07 09:05:28,789 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 5293.4560546875 param sum :: -16.520038604736328
2023-01-07 09:05:28,789 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:28,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,789 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -807.587646484375
2023-01-07 09:05:28,789 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17484444379806519
2023-01-07 09:05:28,790 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.10135148465633392 param sum :: 512.0
2023-01-07 09:05:28,790 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,790 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,790 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:28,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,791 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -807.331787109375
2023-01-07 09:05:28,791 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06101488322019577
2023-01-07 09:05:28,792 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -807.31640625 param sum :: -14.060759544372559
2023-01-07 09:05:28,792 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,792 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:28,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,792 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8273.978515625
2023-01-07 09:05:28,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16020989418029785
2023-01-07 09:05:28,793 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 2.1413722038269043 param sum :: 2048.0
2023-01-07 09:05:28,793 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,793 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,793 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:28,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,793 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8273.345703125
2023-01-07 09:05:28,794 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3327023386955261
2023-01-07 09:05:28,794 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8273.455078125 param sum :: 35.38753890991211
2023-01-07 09:05:28,794 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,795 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:28,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 9905.0458984375
2023-01-07 09:05:28,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21060225367546082
2023-01-07 09:05:28,796 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.4631311893463135 param sum :: 512.0
2023-01-07 09:05:28,796 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,796 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,796 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:28,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 9905.2021484375
2023-01-07 09:05:28,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39394548535346985
2023-01-07 09:05:28,797 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 9905.5234375 param sum :: -33.75589370727539
2023-01-07 09:05:28,797 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:28,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1272.2193603515625
2023-01-07 09:05:28,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25538772344589233
2023-01-07 09:05:28,799 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -62.60593795776367 param sum :: 512.0
2023-01-07 09:05:28,799 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:28,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,799 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1272.2528076171875
2023-01-07 09:05:28,799 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.031175147742033005
2023-01-07 09:05:28,800 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 1272.7381591796875 param sum :: 18.47730255126953
2023-01-07 09:05:28,800 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,800 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,800 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:28,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,801 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 16010.041015625
2023-01-07 09:05:28,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4258999824523926
2023-01-07 09:05:28,802 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -291.2587890625 param sum :: 2048.0
2023-01-07 09:05:28,802 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:28,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,802 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 16013.158203125
2023-01-07 09:05:28,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6063635349273682
2023-01-07 09:05:28,803 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 16013.6865234375 param sum :: 9.326160430908203
2023-01-07 09:05:28,803 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:28,803 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:28,804 > [DEBUG] 0 :: 6.756134033203125
2023-01-07 09:05:28,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,806 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0269775390625
2023-01-07 09:05:28,807 > [DEBUG] 0 :: before allreduce fusion buffer :: -320.3676452636719
2023-01-07 09:05:28,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,809 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: -0.020143158733844757
2023-01-07 09:05:28,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,809 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -352.21209716796875
2023-01-07 09:05:28,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -202.51687622070312
2023-01-07 09:05:28,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,812 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -5.044594764709473
2023-01-07 09:05:28,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17310798168182373
2023-01-07 09:05:28,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,814 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.040574394166469574
2023-01-07 09:05:28,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,815 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -79.55541229248047
2023-01-07 09:05:28,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.020648807287216187
2023-01-07 09:05:28,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,818 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.9243812561035156
2023-01-07 09:05:28,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6100801825523376
2023-01-07 09:05:28,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,819 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.026506979018449783
2023-01-07 09:05:28,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,820 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -70.35379028320312
2023-01-07 09:05:28,820 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2996107339859009
2023-01-07 09:05:28,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,821 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 13.405686378479004
2023-01-07 09:05:28,821 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6024465560913086
2023-01-07 09:05:28,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,822 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: -0.010113980621099472
2023-01-07 09:05:28,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,822 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -60.57398223876953
2023-01-07 09:05:28,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5662609338760376
2023-01-07 09:05:28,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,823 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 1.4912285804748535
2023-01-07 09:05:28,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2354728877544403
2023-01-07 09:05:28,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,824 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.026556946337223053
2023-01-07 09:05:28,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,824 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -72.77765655517578
2023-01-07 09:05:28,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41760826110839844
2023-01-07 09:05:28,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,825 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -37.786224365234375
2023-01-07 09:05:28,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17804820835590363
2023-01-07 09:05:28,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,826 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.06665444374084473
2023-01-07 09:05:28,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,826 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -37.786224365234375
2023-01-07 09:05:28,826 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4102349281311035
2023-01-07 09:05:28,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,827 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -8.342325210571289
2023-01-07 09:05:28,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.887161374092102
2023-01-07 09:05:28,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,828 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.0889308974146843
2023-01-07 09:05:28,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,829 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -8.618319511413574
2023-01-07 09:05:28,829 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.027312010526657104
2023-01-07 09:05:28,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,830 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 10.640278816223145
2023-01-07 09:05:28,830 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.022952597588300705
2023-01-07 09:05:28,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,831 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: -0.050495997071266174
2023-01-07 09:05:28,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,831 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 10.640278816223145
2023-01-07 09:05:28,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0614302158355713
2023-01-07 09:05:28,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,832 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.241884231567383
2023-01-07 09:05:28,832 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3816031813621521
2023-01-07 09:05:28,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,833 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.021024733781814575
2023-01-07 09:05:28,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,833 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.368358612060547
2023-01-07 09:05:28,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.5183000564575195
2023-01-07 09:05:28,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,834 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -72.73758697509766
2023-01-07 09:05:28,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8016610145568848
2023-01-07 09:05:28,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.06244664639234543
2023-01-07 09:05:28,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -72.73758697509766
2023-01-07 09:05:28,836 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.906613826751709
2023-01-07 09:05:28,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,836 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -10.027657508850098
2023-01-07 09:05:28,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8225384950637817
2023-01-07 09:05:28,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,837 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.1862102448940277
2023-01-07 09:05:28,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -9.210018157958984
2023-01-07 09:05:28,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9534451365470886
2023-01-07 09:05:28,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,839 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.7605466842651367
2023-01-07 09:05:28,839 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20421645045280457
2023-01-07 09:05:28,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,840 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.015592927113175392
2023-01-07 09:05:28,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,840 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.4595794677734375
2023-01-07 09:05:28,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44567304849624634
2023-01-07 09:05:28,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,841 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -20.102184295654297
2023-01-07 09:05:28,842 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030040670186281204
2023-01-07 09:05:28,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,842 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.10220063477754593
2023-01-07 09:05:28,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,843 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -20.00797462463379
2023-01-07 09:05:28,843 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9387669563293457
2023-01-07 09:05:28,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,844 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -16.112436294555664
2023-01-07 09:05:28,844 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5616940855979919
2023-01-07 09:05:28,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,845 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.060390904545784
2023-01-07 09:05:28,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,845 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -15.515920639038086
2023-01-07 09:05:28,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9733662605285645
2023-01-07 09:05:28,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,846 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 2.331441640853882
2023-01-07 09:05:28,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4567418098449707
2023-01-07 09:05:28,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,847 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.06555838882923126
2023-01-07 09:05:28,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,847 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 3.965404987335205
2023-01-07 09:05:28,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7823111414909363
2023-01-07 09:05:28,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,848 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.85535717010498
2023-01-07 09:05:28,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9071294069290161
2023-01-07 09:05:28,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,849 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.1249610185623169
2023-01-07 09:05:28,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,849 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -11.156637191772461
2023-01-07 09:05:28,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21738304197788239
2023-01-07 09:05:28,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,850 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -9.601056098937988
2023-01-07 09:05:28,850 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6864179372787476
2023-01-07 09:05:28,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,851 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.04610966145992279
2023-01-07 09:05:28,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,852 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -11.638388633728027
2023-01-07 09:05:28,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7997643947601318
2023-01-07 09:05:28,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,853 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -11.013113021850586
2023-01-07 09:05:28,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03703105449676514
2023-01-07 09:05:28,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,854 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.08601325005292892
2023-01-07 09:05:28,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,854 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -13.68308162689209
2023-01-07 09:05:28,854 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45698148012161255
2023-01-07 09:05:28,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,855 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1.765939712524414
2023-01-07 09:05:28,855 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0282957553863525
2023-01-07 09:05:28,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,856 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.12798362970352173
2023-01-07 09:05:28,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,856 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3.8342275619506836
2023-01-07 09:05:28,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8773488998413086
2023-01-07 09:05:28,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,857 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.4697906970977783
2023-01-07 09:05:28,857 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8665993213653564
2023-01-07 09:05:28,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,858 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.08035296201705933
2023-01-07 09:05:28,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,858 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1.6395211219787598
2023-01-07 09:05:28,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9094812870025635
2023-01-07 09:05:28,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,859 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -11.436046600341797
2023-01-07 09:05:28,860 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.524474024772644
2023-01-07 09:05:28,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,860 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.24277377128601074
2023-01-07 09:05:28,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,861 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -16.254653930664062
2023-01-07 09:05:28,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7163071632385254
2023-01-07 09:05:28,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -2.8005447387695312
2023-01-07 09:05:28,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8056278228759766
2023-01-07 09:05:28,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,863 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.017831027507781982
2023-01-07 09:05:28,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,863 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -5.624665260314941
2023-01-07 09:05:28,863 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.107307434082031
2023-01-07 09:05:28,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,864 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -17.398273468017578
2023-01-07 09:05:28,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.701369285583496
2023-01-07 09:05:28,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.888813853263855
2023-01-07 09:05:28,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -35.07307052612305
2023-01-07 09:05:28,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8334735035896301
2023-01-07 09:05:28,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,866 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -16.149860382080078
2023-01-07 09:05:28,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8171066641807556
2023-01-07 09:05:28,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,867 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.2966959476470947
2023-01-07 09:05:28,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,867 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -12.613027572631836
2023-01-07 09:05:28,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.051133394241333
2023-01-07 09:05:28,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,869 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 43.10881042480469
2023-01-07 09:05:28,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8615065813064575
2023-01-07 09:05:28,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,870 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 46.032142639160156
2023-01-07 09:05:28,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3127622604370117
2023-01-07 09:05:28,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,871 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -38.1062126159668
2023-01-07 09:05:28,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6596457958221436
2023-01-07 09:05:28,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.0679963082075119
2023-01-07 09:05:28,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -25.593416213989258
2023-01-07 09:05:28,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6038470268249512
2023-01-07 09:05:28,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,873 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -39.18352508544922
2023-01-07 09:05:28,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33878451585769653
2023-01-07 09:05:28,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.19097021222114563
2023-01-07 09:05:28,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -37.47654342651367
2023-01-07 09:05:28,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.502551078796387
2023-01-07 09:05:28,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -31.56627082824707
2023-01-07 09:05:28,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.383434534072876
2023-01-07 09:05:28,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.037417322397232056
2023-01-07 09:05:28,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,877 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -18.826030731201172
2023-01-07 09:05:28,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.13471508026123
2023-01-07 09:05:28,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 143.48707580566406
2023-01-07 09:05:28,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.905791997909546
2023-01-07 09:05:28,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,879 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 146.45108032226562
2023-01-07 09:05:28,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.184450149536133
2023-01-07 09:05:28,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,880 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 3.0925650596618652
2023-01-07 09:05:28,880 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.756699800491333
2023-01-07 09:05:28,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -5.341220378875732
2023-01-07 09:05:28,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.228036880493164
2023-01-07 09:05:28,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,882 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -6.000784397125244
2023-01-07 09:05:28,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1837215423583984
2023-01-07 09:05:28,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,883 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -9.508118629455566
2023-01-07 09:05:28,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5907780528068542
2023-01-07 09:05:28,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,884 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 13.973337173461914
2023-01-07 09:05:28,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.63140869140625
2023-01-07 09:05:28,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,885 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 7.209123611450195
2023-01-07 09:05:28,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.003924369812012
2023-01-07 09:05:28,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,886 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 26.01136016845703
2023-01-07 09:05:28,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9170265197753906
2023-01-07 09:05:28,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,887 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -16.853252410888672
2023-01-07 09:05:28,887 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.073511123657227
2023-01-07 09:05:28,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,888 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -9.274680137634277
2023-01-07 09:05:28,888 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.334317684173584
2023-01-07 09:05:28,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,889 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -31.624774932861328
2023-01-07 09:05:28,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.467125415802002
2023-01-07 09:05:28,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,890 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -161.741943359375
2023-01-07 09:05:28,890 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.948302268981934
2023-01-07 09:05:28,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,891 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -239.481201171875
2023-01-07 09:05:28,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.838902473449707
2023-01-07 09:05:28,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,892 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -24.962921142578125
2023-01-07 09:05:28,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9003591537475586
2023-01-07 09:05:28,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,893 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.3110662698745728
2023-01-07 09:05:28,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,894 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 228.587646484375
2023-01-07 09:05:28,894 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.9494194984436035
2023-01-07 09:05:28,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,895 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -17.165401458740234
2023-01-07 09:05:28,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.0408854484558105
2023-01-07 09:05:28,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,896 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -17.165401458740234
2023-01-07 09:05:28,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.102611064910889
2023-01-07 09:05:28,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,897 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 248.00564575195312
2023-01-07 09:05:28,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9509899616241455
2023-01-07 09:05:28,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,898 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -568.2096557617188
2023-01-07 09:05:28,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.00706481933594
2023-01-07 09:05:28,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,899 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -8.513222694396973
2023-01-07 09:05:28,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.415227890014648
2023-01-07 09:05:28,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,900 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.056371688842773
2023-01-07 09:05:28,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,900 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -56.65864944458008
2023-01-07 09:05:28,900 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.40888214111328
2023-01-07 09:05:28,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,901 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -70.13556671142578
2023-01-07 09:05:28,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1556423306465149
2023-01-07 09:05:28,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,902 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -88.91943359375
2023-01-07 09:05:28,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.577315330505371
2023-01-07 09:05:28,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,904 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 8.966014862060547
2023-01-07 09:05:28,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.915427207946777
2023-01-07 09:05:28,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,905 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -3.4517030715942383
2023-01-07 09:05:28,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.4776611328125
2023-01-07 09:05:28,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,906 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 230.63711547851562
2023-01-07 09:05:28,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.498807907104492
2023-01-07 09:05:28,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,907 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.4631006717681885
2023-01-07 09:05:28,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,907 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 219.4432373046875
2023-01-07 09:05:28,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.24339294433594
2023-01-07 09:05:28,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,908 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 233.9470977783203
2023-01-07 09:05:28,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.381195068359375
2023-01-07 09:05:28,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,909 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.096505045890808
2023-01-07 09:05:28,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,909 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -31.161012649536133
2023-01-07 09:05:28,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,910 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 223.6264190673828
2023-01-07 09:05:28,910 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.160736083984375
2023-01-07 09:05:28,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,911 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 223.7029266357422
2023-01-07 09:05:28,911 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.894161224365234
2023-01-07 09:05:28,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,912 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 22.552734375
2023-01-07 09:05:28,912 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.503089904785156
2023-01-07 09:05:28,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,913 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -259.83489990234375
2023-01-07 09:05:28,913 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.66029930114746
2023-01-07 09:05:28,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,914 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.203416347503662
2023-01-07 09:05:28,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,914 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -249.96621704101562
2023-01-07 09:05:28,914 > [DEBUG] 0 :: before allreduce fusion buffer :: -166.73072814941406
2023-01-07 09:05:28,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,915 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -265.1876220703125
2023-01-07 09:05:28,916 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.487394332885742
2023-01-07 09:05:28,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,916 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.40749549865722656
2023-01-07 09:05:28,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,917 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 24.921817779541016
2023-01-07 09:05:28,917 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.937557220458984
2023-01-07 09:05:28,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,918 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -360.5835266113281
2023-01-07 09:05:28,918 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.935760498046875
2023-01-07 09:05:28,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,919 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 13.605674743652344
2023-01-07 09:05:28,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,919 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 240.6099395751953
2023-01-07 09:05:28,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.23080444335938
2023-01-07 09:05:28,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,920 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -431.7481384277344
2023-01-07 09:05:28,920 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.172300338745117
2023-01-07 09:05:28,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,921 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 250.99087524414062
2023-01-07 09:05:28,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.003616333007812
2023-01-07 09:05:28,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,922 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -516.4353637695312
2023-01-07 09:05:28,923 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.931808471679688
2023-01-07 09:05:28,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,924 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.279597520828247
2023-01-07 09:05:28,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,924 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -21.854816436767578
2023-01-07 09:05:28,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,924 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -708.7869873046875
2023-01-07 09:05:28,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,925 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 251.11892700195312
2023-01-07 09:05:28,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.99284744262695
2023-01-07 09:05:28,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,926 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -726.8834228515625
2023-01-07 09:05:28,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.536241054534912
2023-01-07 09:05:28,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,927 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -79.67039489746094
2023-01-07 09:05:28,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -104.57318878173828
2023-01-07 09:05:28,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,928 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -747.3416748046875
2023-01-07 09:05:28,928 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.416057586669922
2023-01-07 09:05:28,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,929 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -375.50347900390625
2023-01-07 09:05:28,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,929 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 242.03082275390625
2023-01-07 09:05:28,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.446735382080078
2023-01-07 09:05:28,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,930 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 342.2106018066406
2023-01-07 09:05:28,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.078100681304932
2023-01-07 09:05:28,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,931 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.7467217445373535
2023-01-07 09:05:28,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,932 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 217.74720764160156
2023-01-07 09:05:28,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 80.28671264648438
2023-01-07 09:05:28,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,933 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -73.71449279785156
2023-01-07 09:05:28,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -98.3598403930664
2023-01-07 09:05:28,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,934 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -191.91015625
2023-01-07 09:05:28,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 86.89407348632812
2023-01-07 09:05:28,936 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:05:28,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,937 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 1933.578125
2023-01-07 09:05:28,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,937 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -369.46905517578125
2023-01-07 09:05:28,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,938 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 224.58534240722656
2023-01-07 09:05:28,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,939 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 9.498634338378906
2023-01-07 09:05:28,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,939 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -99.47789001464844
2023-01-07 09:05:28,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,940 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -51.1861572265625
2023-01-07 09:05:28,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,940 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2749.12255859375
2023-01-07 09:05:28,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,941 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2690.326416015625
2023-01-07 09:05:28,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,941 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3886.48974609375
2023-01-07 09:05:28,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,942 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -282.3317565917969
2023-01-07 09:05:28,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,943 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -48.24703598022461
2023-01-07 09:05:28,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,943 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -98.99449920654297
2023-01-07 09:05:28,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,943 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -2.145689010620117
2023-01-07 09:05:28,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,944 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -30.005037307739258
2023-01-07 09:05:28,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -0.8701615333557129
2023-01-07 09:05:28,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 100.37679290771484
2023-01-07 09:05:28,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -28.388835906982422
2023-01-07 09:05:28,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -17.612342834472656
2023-01-07 09:05:28,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -17.983976364135742
2023-01-07 09:05:28,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 43.578392028808594
2023-01-07 09:05:28,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8.966318130493164
2023-01-07 09:05:28,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -26.55694007873535
2023-01-07 09:05:28,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -12.149262428283691
2023-01-07 09:05:28,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -19.528148651123047
2023-01-07 09:05:28,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -6.280004978179932
2023-01-07 09:05:28,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,947 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 9.844161987304688
2023-01-07 09:05:28,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 2298.10595703125
2023-01-07 09:05:28,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -22.591888427734375
2023-01-07 09:05:28,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -12.239334106445312
2023-01-07 09:05:28,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -7.511066436767578
2023-01-07 09:05:28,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.294153690338135
2023-01-07 09:05:28,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.871252059936523
2023-01-07 09:05:28,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -17.81540298461914
2023-01-07 09:05:28,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,950 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 0.8310847282409668
2023-01-07 09:05:28,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -12.613417625427246
2023-01-07 09:05:28,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -67.6158676147461
2023-01-07 09:05:28,950 > [DEBUG] 0 :: before allreduce fusion buffer :: -148.54759216308594
2023-01-07 09:05:28,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,951 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 10.001900672912598
2023-01-07 09:05:28,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,951 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.658438682556152
2023-01-07 09:05:28,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,952 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.764917850494385
2023-01-07 09:05:28,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,952 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -37.9250602722168
2023-01-07 09:05:28,952 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.639554977416992
2023-01-07 09:05:28,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -73.42462158203125
2023-01-07 09:05:28,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -61.038108825683594
2023-01-07 09:05:28,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -69.33134460449219
2023-01-07 09:05:28,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.034010887145996
2023-01-07 09:05:28,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,954 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -122.17304229736328
2023-01-07 09:05:28,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:28,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:28,954 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -453.7823486328125
2023-01-07 09:05:28,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 208.3651123046875
2023-01-07 09:05:29,800 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -312.1806945800781 param sum :: 0.22188237309455872
2023-01-07 09:05:29,800 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,800 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,800 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:29,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,801 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -2.3963193893432617
2023-01-07 09:05:29,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,801 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -571.8746948242188
2023-01-07 09:05:29,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 77.46708679199219
2023-01-07 09:05:29,802 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.9762716293334961 param sum :: 64.0
2023-01-07 09:05:29,802 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 5.294054985046387
2023-01-07 09:05:29,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,802 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -81.4246826171875
2023-01-07 09:05:29,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,803 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -508.6138000488281
2023-01-07 09:05:29,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -73.24873352050781
2023-01-07 09:05:29,804 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -17.630966186523438 param sum :: 5.294054985046387
2023-01-07 09:05:29,804 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:29,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,804 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -493.6556701660156
2023-01-07 09:05:29,804 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.90034484863281
2023-01-07 09:05:29,805 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -24.92761993408203 param sum :: 64.0
2023-01-07 09:05:29,805 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,805 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,805 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.982826232910156
2023-01-07 09:05:29,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,806 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -569.001220703125
2023-01-07 09:05:29,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.27814865112305
2023-01-07 09:05:29,807 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -685.872314453125 param sum :: 5.982826232910156
2023-01-07 09:05:29,807 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,807 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:29,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,807 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.26246511936187744
2023-01-07 09:05:29,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,807 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 12.685359001159668
2023-01-07 09:05:29,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,807 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -886.4119262695312
2023-01-07 09:05:29,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9515495300292969
2023-01-07 09:05:29,809 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -1.0021703243255615 param sum :: 64.0
2023-01-07 09:05:29,809 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:29,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,809 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -886.8502197265625
2023-01-07 09:05:29,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7716999053955078
2023-01-07 09:05:29,810 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -15.217681884765625 param sum :: 11.617742538452148
2023-01-07 09:05:29,810 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,810 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,810 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:29,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,810 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -6.344569206237793
2023-01-07 09:05:29,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,811 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -318.0686950683594
2023-01-07 09:05:29,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.985931396484375
2023-01-07 09:05:29,812 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -4.855393409729004 param sum :: 256.0
2023-01-07 09:05:29,812 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,812 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,812 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:29,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,812 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -40.90398406982422
2023-01-07 09:05:29,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.36440372467041
2023-01-07 09:05:29,813 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -707.759765625 param sum :: -10.471250534057617
2023-01-07 09:05:29,813 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,813 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,813 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:29,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,814 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -95.65288543701172
2023-01-07 09:05:29,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.618331909179688
2023-01-07 09:05:29,815 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -12.31821060180664 param sum :: 256.0
2023-01-07 09:05:29,815 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,815 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,815 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:29,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,815 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -196.96820068359375
2023-01-07 09:05:29,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.70841217041016
2023-01-07 09:05:29,816 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -290.2882080078125 param sum :: -38.087059020996094
2023-01-07 09:05:29,816 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,816 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,816 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:29,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,816 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.8335782289505005
2023-01-07 09:05:29,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,817 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -248.01077270507812
2023-01-07 09:05:29,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 367.4107971191406
2023-01-07 09:05:29,818 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -0.010768890380859375 param sum :: 64.0
2023-01-07 09:05:29,818 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,818 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,818 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.83746337890625
2023-01-07 09:05:29,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,818 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -262.8083190917969
2023-01-07 09:05:29,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,819 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -7.5268707275390625
2023-01-07 09:05:29,819 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.455734252929688
2023-01-07 09:05:29,820 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -292.2250061035156 param sum :: -9.83746337890625
2023-01-07 09:05:29,820 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,820 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:29,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,820 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.48326241970062256
2023-01-07 09:05:29,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,820 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -234.20152282714844
2023-01-07 09:05:29,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.318336486816406
2023-01-07 09:05:29,821 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.2252134084701538 param sum :: 64.0
2023-01-07 09:05:29,822 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,822 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,822 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:29,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,822 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -876.3856201171875
2023-01-07 09:05:29,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.27654266357422
2023-01-07 09:05:29,823 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -218.14431762695312 param sum :: 13.821467399597168
2023-01-07 09:05:29,823 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,823 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,823 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:29,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,823 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -160.1973114013672
2023-01-07 09:05:29,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.312278747558594
2023-01-07 09:05:29,824 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -27.499126434326172 param sum :: 256.0
2023-01-07 09:05:29,824 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,824 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,825 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.297849655151367
2023-01-07 09:05:29,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,825 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -179.6358642578125
2023-01-07 09:05:29,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,825 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -858.9345092773438
2023-01-07 09:05:29,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.79682922363281
2023-01-07 09:05:29,826 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -178.22091674804688 param sum :: -16.297849655151367
2023-01-07 09:05:29,826 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,826 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,826 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:29,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,826 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -832.9545288085938
2023-01-07 09:05:29,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -65.74775695800781
2023-01-07 09:05:29,827 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -16.415271759033203 param sum :: 64.0
2023-01-07 09:05:29,827 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,827 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,827 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -1.7540884017944336
2023-01-07 09:05:29,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,828 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -840.68994140625
2023-01-07 09:05:29,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.7830867767334
2023-01-07 09:05:29,829 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -856.236083984375 param sum :: -1.7540884017944336
2023-01-07 09:05:29,829 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,829 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:05:29,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,829 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.8474178314208984
2023-01-07 09:05:29,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,829 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 7.324575901031494
2023-01-07 09:05:29,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,829 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 57.142127990722656
2023-01-07 09:05:29,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.85062313079834
2023-01-07 09:05:29,831 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.4592455625534058 param sum :: 64.0
2023-01-07 09:05:29,831 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,831 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,831 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:29,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,831 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 14.362358093261719
2023-01-07 09:05:29,832 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.11452865600586
2023-01-07 09:05:29,832 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 22.045618057250977 param sum :: -0.8034350872039795
2023-01-07 09:05:29,832 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,832 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,833 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -17.25152015686035
2023-01-07 09:05:29,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,833 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 48.00341796875
2023-01-07 09:05:29,833 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3482792377471924
2023-01-07 09:05:29,834 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 1.5265934467315674 param sum :: 256.0
2023-01-07 09:05:29,834 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,834 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,834 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:29,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,834 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2.6934947967529297
2023-01-07 09:05:29,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.982622146606445
2023-01-07 09:05:29,835 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 45.88261413574219 param sum :: -17.25152015686035
2023-01-07 09:05:29,835 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,835 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,835 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:29,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,836 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 35.598876953125
2023-01-07 09:05:29,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.656776428222656
2023-01-07 09:05:29,837 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -17.42014503479004 param sum :: 128.0
2023-01-07 09:05:29,837 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,837 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,837 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.020125389099121
2023-01-07 09:05:29,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,837 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -23.12040901184082
2023-01-07 09:05:29,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.51614761352539
2023-01-07 09:05:29,838 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -35.282474517822266 param sum :: -10.020125389099121
2023-01-07 09:05:29,838 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,838 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,838 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,838 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 1.146914005279541
2023-01-07 09:05:29,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,839 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1.4740867614746094
2023-01-07 09:05:29,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.5806884765625
2023-01-07 09:05:29,840 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.9023983478546143 param sum :: 128.0
2023-01-07 09:05:29,840 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,840 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,840 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -4.991181373596191
2023-01-07 09:05:29,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,840 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 13.458354949951172
2023-01-07 09:05:29,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.699030876159668
2023-01-07 09:05:29,841 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 7.634366989135742 param sum :: -4.991181373596191
2023-01-07 09:05:29,841 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,841 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,841 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:29,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 1.1553421020507812
2023-01-07 09:05:29,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -19.963163375854492
2023-01-07 09:05:29,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.23434591293335
2023-01-07 09:05:29,843 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.7395302057266235 param sum :: 512.0
2023-01-07 09:05:29,843 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,843 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,843 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -18.34212875366211
2023-01-07 09:05:29,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,843 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5141944885253906
2023-01-07 09:05:29,843 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.06195068359375
2023-01-07 09:05:29,844 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -23.157974243164062 param sum :: -18.34212875366211
2023-01-07 09:05:29,844 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,844 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,844 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:29,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,844 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -27.85983657836914
2023-01-07 09:05:29,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.288312911987305
2023-01-07 09:05:29,846 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -38.10065460205078 param sum :: 512.0
2023-01-07 09:05:29,846 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,846 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 37.5141487121582
2023-01-07 09:05:29,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,846 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.50363540649414
2023-01-07 09:05:29,846 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.559129476547241
2023-01-07 09:05:29,847 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -7.364831924438477 param sum :: 37.5141487121582
2023-01-07 09:05:29,847 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,847 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,847 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,847 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.56291264295578
2023-01-07 09:05:29,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,848 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2557.0732421875
2023-01-07 09:05:29,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.6946868896484375
2023-01-07 09:05:29,849 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.576384425163269 param sum :: 128.0
2023-01-07 09:05:29,849 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,849 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,849 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 8.519919395446777
2023-01-07 09:05:29,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,849 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2560.210205078125
2023-01-07 09:05:29,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46506237983703613
2023-01-07 09:05:29,850 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 2557.26318359375 param sum :: 8.519919395446777
2023-01-07 09:05:29,850 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,850 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,850 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,850 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.7882039546966553
2023-01-07 09:05:29,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,851 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2708.913330078125
2023-01-07 09:05:29,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.817037582397461
2023-01-07 09:05:29,852 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 1.4872314929962158 param sum :: 128.0
2023-01-07 09:05:29,852 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,852 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,852 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -17.605867385864258
2023-01-07 09:05:29,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,852 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2721.63671875
2023-01-07 09:05:29,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5124001502990723
2023-01-07 09:05:29,853 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 2719.534423828125 param sum :: -17.605867385864258
2023-01-07 09:05:29,853 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,853 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,853 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:29,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,853 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3885.45361328125
2023-01-07 09:05:29,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3324568271636963
2023-01-07 09:05:29,854 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -40.834590911865234 param sum :: 512.0
2023-01-07 09:05:29,855 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,855 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,855 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -58.115535736083984
2023-01-07 09:05:29,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,855 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3886.13720703125
2023-01-07 09:05:29,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.057737350463867
2023-01-07 09:05:29,856 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 3897.580810546875 param sum :: -58.115535736083984
2023-01-07 09:05:29,856 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,856 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,856 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 1.0606569051742554
2023-01-07 09:05:29,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,856 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -281.42938232421875
2023-01-07 09:05:29,857 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.321964263916016
2023-01-07 09:05:29,858 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.35466742515563965 param sum :: 128.0
2023-01-07 09:05:29,858 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,858 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,858 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -14.622330665588379
2023-01-07 09:05:29,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,858 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -266.3891906738281
2023-01-07 09:05:29,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.614116191864014
2023-01-07 09:05:29,859 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -273.90008544921875 param sum :: -14.622330665588379
2023-01-07 09:05:29,859 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,859 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,859 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,859 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.536974310874939
2023-01-07 09:05:29,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,860 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -47.159454345703125
2023-01-07 09:05:29,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.999868154525757
2023-01-07 09:05:29,861 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.15592876076698303 param sum :: 128.0
2023-01-07 09:05:29,861 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,861 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.833906173706055
2023-01-07 09:05:29,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,861 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -37.07810974121094
2023-01-07 09:05:29,861 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.332308292388916
2023-01-07 09:05:29,862 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -41.33369064331055 param sum :: 20.833906173706055
2023-01-07 09:05:29,862 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,862 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,862 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:29,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,862 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4162765145301819
2023-01-07 09:05:29,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,863 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -99.54415893554688
2023-01-07 09:05:29,863 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.81222152709961
2023-01-07 09:05:29,864 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.4434415102005005 param sum :: 512.0
2023-01-07 09:05:29,864 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,864 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,864 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -48.606842041015625
2023-01-07 09:05:29,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,864 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -105.15282440185547
2023-01-07 09:05:29,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.947248935699463
2023-01-07 09:05:29,865 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -102.38092041015625 param sum :: -48.606842041015625
2023-01-07 09:05:29,865 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,866 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.08187663555145264
2023-01-07 09:05:29,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,866 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -1.332026481628418
2023-01-07 09:05:29,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.34688138961792
2023-01-07 09:05:29,867 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.35912859439849854 param sum :: 128.0
2023-01-07 09:05:29,867 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,867 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,867 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -34.13706588745117
2023-01-07 09:05:29,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,867 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.6291942596435547
2023-01-07 09:05:29,867 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6736931800842285
2023-01-07 09:05:29,868 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 19.282958984375 param sum :: -34.13706588745117
2023-01-07 09:05:29,868 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,868 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,868 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:05:29,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,869 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.4067245125770569
2023-01-07 09:05:29,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,869 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -30.819917678833008
2023-01-07 09:05:29,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.081246852874756
2023-01-07 09:05:29,870 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.1551181674003601 param sum :: 128.0
2023-01-07 09:05:29,870 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,870 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,870 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 11.814714431762695
2023-01-07 09:05:29,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,870 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -24.24967384338379
2023-01-07 09:05:29,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9940290451049805
2023-01-07 09:05:29,871 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -20.690292358398438 param sum :: 11.814714431762695
2023-01-07 09:05:29,871 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,872 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,872 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:05:29,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,872 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.03390410542488098
2023-01-07 09:05:29,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 6.335599899291992
2023-01-07 09:05:29,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4351317882537842
2023-01-07 09:05:29,873 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.029633626341819763 param sum :: 512.0
2023-01-07 09:05:29,873 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,873 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,873 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -26.576377868652344
2023-01-07 09:05:29,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,873 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -11.361346244812012
2023-01-07 09:05:29,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9759459495544434
2023-01-07 09:05:29,874 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -12.113874435424805 param sum :: -26.576377868652344
2023-01-07 09:05:29,875 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,875 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,875 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:29,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.07182586193084717
2023-01-07 09:05:29,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -541.175048828125
2023-01-07 09:05:29,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6544407606124878
2023-01-07 09:05:29,876 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.4373911917209625 param sum :: 256.0
2023-01-07 09:05:29,876 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,876 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,877 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.7762942314147949
2023-01-07 09:05:29,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,877 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -548.5289306640625
2023-01-07 09:05:29,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.811565399169922
2023-01-07 09:05:29,878 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -533.296875 param sum :: -0.7762942314147949
2023-01-07 09:05:29,878 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,878 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,878 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:29,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 100.2472915649414
2023-01-07 09:05:29,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.944705963134766
2023-01-07 09:05:29,879 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -1.2117635011672974 param sum :: 256.0
2023-01-07 09:05:29,879 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,879 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,879 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.802434921264648
2023-01-07 09:05:29,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,880 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 104.69921875
2023-01-07 09:05:29,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9235191345214844
2023-01-07 09:05:29,881 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 104.62921142578125 param sum :: 10.802434921264648
2023-01-07 09:05:29,881 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,881 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,881 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:29,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 428.0872802734375
2023-01-07 09:05:29,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.457463264465332
2023-01-07 09:05:29,882 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 3.298635482788086 param sum :: 1024.0
2023-01-07 09:05:29,882 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,882 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,882 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.55324935913086
2023-01-07 09:05:29,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 427.189697265625
2023-01-07 09:05:29,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0392980575561523
2023-01-07 09:05:29,884 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 432.96343994140625 param sum :: -34.55324935913086
2023-01-07 09:05:29,884 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,884 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,884 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:29,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 255.38385009765625
2023-01-07 09:05:29,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1260509490966797
2023-01-07 09:05:29,885 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.779788494110107 param sum :: 1024.0
2023-01-07 09:05:29,885 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,885 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,885 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -82.27286529541016
2023-01-07 09:05:29,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,885 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 260.9693908691406
2023-01-07 09:05:29,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2497888803482056
2023-01-07 09:05:29,886 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 262.80902099609375 param sum :: -82.27286529541016
2023-01-07 09:05:29,886 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,886 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,887 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:05:29,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.08442914485931396
2023-01-07 09:05:29,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 592.380126953125
2023-01-07 09:05:29,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.437420606613159
2023-01-07 09:05:29,888 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.2053208351135254 param sum :: 256.0
2023-01-07 09:05:29,888 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,888 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,888 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -26.810012817382812
2023-01-07 09:05:29,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,889 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 600.1397705078125
2023-01-07 09:05:29,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4014992713928223
2023-01-07 09:05:29,890 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 597.1006469726562 param sum :: -26.810012817382812
2023-01-07 09:05:29,890 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,890 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,890 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:29,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 151.728271484375
2023-01-07 09:05:29,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0162858963012695
2023-01-07 09:05:29,891 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -0.9774186611175537 param sum :: 256.0
2023-01-07 09:05:29,891 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,891 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,891 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 23.764698028564453
2023-01-07 09:05:29,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,892 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 157.01170349121094
2023-01-07 09:05:29,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9460254907608032
2023-01-07 09:05:29,893 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 157.7050018310547 param sum :: 23.764698028564453
2023-01-07 09:05:29,893 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,893 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,893 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:29,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.9845499992370605
2023-01-07 09:05:29,893 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1246225833892822
2023-01-07 09:05:29,894 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.883141279220581 param sum :: 1024.0
2023-01-07 09:05:29,894 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,894 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,894 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -33.003875732421875
2023-01-07 09:05:29,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,895 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -2.301905632019043
2023-01-07 09:05:29,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4135780334472656
2023-01-07 09:05:29,896 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -2.4777040481567383 param sum :: -33.003875732421875
2023-01-07 09:05:29,896 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,896 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,896 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:29,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,896 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.907658576965332
2023-01-07 09:05:29,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.060248345136642456
2023-01-07 09:05:29,897 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -2.0331428050994873 param sum :: 256.0
2023-01-07 09:05:29,897 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,897 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,897 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 39.414920806884766
2023-01-07 09:05:29,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,897 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.43960952758789
2023-01-07 09:05:29,898 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.360814332962036
2023-01-07 09:05:29,899 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -6.121466636657715 param sum :: 39.414920806884766
2023-01-07 09:05:29,899 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,899 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,899 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:29,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4594826698303223
2023-01-07 09:05:29,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.157336950302124
2023-01-07 09:05:29,900 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -2.6758859157562256 param sum :: 256.0
2023-01-07 09:05:29,900 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,900 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,900 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 7.077023506164551
2023-01-07 09:05:29,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,900 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -2.1598286628723145
2023-01-07 09:05:29,901 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8124186396598816
2023-01-07 09:05:29,901 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -4.89012336730957 param sum :: 7.077023506164551
2023-01-07 09:05:29,902 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,902 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,902 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:29,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -12.6639986038208
2023-01-07 09:05:29,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.36716890335083
2023-01-07 09:05:29,903 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -4.520811557769775 param sum :: 1024.0
2023-01-07 09:05:29,903 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,903 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,903 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -1.5680561065673828
2023-01-07 09:05:29,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -12.485340118408203
2023-01-07 09:05:29,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.026129722595215
2023-01-07 09:05:29,904 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -12.633336067199707 param sum :: -1.5680561065673828
2023-01-07 09:05:29,905 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,905 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,905 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:29,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -614.301025390625
2023-01-07 09:05:29,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.435971736907959
2023-01-07 09:05:29,906 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -1.3718202114105225 param sum :: 256.0
2023-01-07 09:05:29,906 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,906 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,906 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -28.745033264160156
2023-01-07 09:05:29,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,906 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -620.275634765625
2023-01-07 09:05:29,907 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3903595209121704
2023-01-07 09:05:29,907 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -621.4905395507812 param sum :: -28.745033264160156
2023-01-07 09:05:29,907 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,908 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,908 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:29,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 0.42551136016845703
2023-01-07 09:05:29,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1246895790100098
2023-01-07 09:05:29,909 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -1.634080410003662 param sum :: 256.0
2023-01-07 09:05:29,909 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,909 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,909 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.068355560302734
2023-01-07 09:05:29,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,910 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 0.020430564880371094
2023-01-07 09:05:29,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12052270770072937
2023-01-07 09:05:29,911 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 0.16083812713623047 param sum :: -5.068355560302734
2023-01-07 09:05:29,911 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,911 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,911 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:29,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,911 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -23.868389129638672
2023-01-07 09:05:29,911 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.462585210800171
2023-01-07 09:05:29,912 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -1.9005893468856812 param sum :: 1024.0
2023-01-07 09:05:29,912 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,912 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,912 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -63.574039459228516
2023-01-07 09:05:29,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,913 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.917640686035156
2023-01-07 09:05:29,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.019223690032959
2023-01-07 09:05:29,914 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -25.712295532226562 param sum :: -63.574039459228516
2023-01-07 09:05:29,914 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,914 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,914 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:29,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,914 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -215.7605438232422
2023-01-07 09:05:29,914 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5190457105636597
2023-01-07 09:05:29,915 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.9821614027023315 param sum :: 256.0
2023-01-07 09:05:29,915 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,915 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,915 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 41.2066535949707
2023-01-07 09:05:29,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,916 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -214.65060424804688
2023-01-07 09:05:29,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.125389337539673
2023-01-07 09:05:29,917 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -216.21890258789062 param sum :: 41.2066535949707
2023-01-07 09:05:29,917 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,917 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,917 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:29,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,917 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1632.646728515625
2023-01-07 09:05:29,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1031993627548218
2023-01-07 09:05:29,918 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.6522670388221741 param sum :: 256.0
2023-01-07 09:05:29,918 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,918 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,918 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.652504920959473
2023-01-07 09:05:29,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,918 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1632.5860595703125
2023-01-07 09:05:29,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.541020154953003
2023-01-07 09:05:29,919 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1633.8104248046875 param sum :: -6.652504920959473
2023-01-07 09:05:29,919 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,920 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,920 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:29,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,920 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 382.41943359375
2023-01-07 09:05:29,920 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1317073106765747
2023-01-07 09:05:29,921 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.20786748826503754 param sum :: 1024.0
2023-01-07 09:05:29,921 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,921 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,921 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -52.449466705322266
2023-01-07 09:05:29,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,921 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 381.6011047363281
2023-01-07 09:05:29,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3056488037109375
2023-01-07 09:05:29,922 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 380.0491943359375 param sum :: -52.449466705322266
2023-01-07 09:05:29,922 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,922 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,923 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:29,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,923 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 683.0194091796875
2023-01-07 09:05:29,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1503517627716064
2023-01-07 09:05:29,924 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.03852224349975586 param sum :: 256.0
2023-01-07 09:05:29,924 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,924 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,924 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -38.22235870361328
2023-01-07 09:05:29,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,924 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 683.2705688476562
2023-01-07 09:05:29,924 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0787873268127441
2023-01-07 09:05:29,925 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 682.989990234375 param sum :: -38.22235870361328
2023-01-07 09:05:29,925 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,925 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,925 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:29,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 487.2298583984375
2023-01-07 09:05:29,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4144752025604248
2023-01-07 09:05:29,927 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.008105725049972534 param sum :: 256.0
2023-01-07 09:05:29,927 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,927 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,927 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -3.3066205978393555
2023-01-07 09:05:29,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,927 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 486.9619140625
2023-01-07 09:05:29,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24814027547836304
2023-01-07 09:05:29,928 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 486.66357421875 param sum :: -3.3066205978393555
2023-01-07 09:05:29,928 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,928 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,928 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:29,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,929 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1029.9246826171875
2023-01-07 09:05:29,929 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4450095891952515
2023-01-07 09:05:29,930 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.339801549911499 param sum :: 1024.0
2023-01-07 09:05:29,930 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,930 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,930 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 70.22351837158203
2023-01-07 09:05:29,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,930 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1029.5517578125
2023-01-07 09:05:29,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2178987264633179
2023-01-07 09:05:29,931 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1028.369873046875 param sum :: 70.22351837158203
2023-01-07 09:05:29,931 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,931 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,931 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:29,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,932 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -363.7452087402344
2023-01-07 09:05:29,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5788887739181519
2023-01-07 09:05:29,933 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5771065950393677 param sum :: 512.0
2023-01-07 09:05:29,933 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,933 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,933 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.939680099487305
2023-01-07 09:05:29,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,933 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -364.7186279296875
2023-01-07 09:05:29,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1344521045684814
2023-01-07 09:05:29,934 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -363.0988464355469 param sum :: 8.939680099487305
2023-01-07 09:05:29,934 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,934 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,934 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:29,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,934 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -536.41162109375
2023-01-07 09:05:29,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.165555715560913
2023-01-07 09:05:29,936 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.6716139316558838 param sum :: 512.0
2023-01-07 09:05:29,936 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,936 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,936 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.04633331298828
2023-01-07 09:05:29,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,936 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -535.3192749023438
2023-01-07 09:05:29,936 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1565658152103424
2023-01-07 09:05:29,937 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -535.3232421875 param sum :: 20.04633331298828
2023-01-07 09:05:29,937 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,937 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,937 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:29,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,937 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2613.8193359375
2023-01-07 09:05:29,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9152058362960815
2023-01-07 09:05:29,939 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.4040006101131439 param sum :: 2048.0
2023-01-07 09:05:29,939 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,939 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,939 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -41.879310607910156
2023-01-07 09:05:29,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,939 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2614.93359375
2023-01-07 09:05:29,939 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1899700164794922
2023-01-07 09:05:29,940 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2614.50732421875 param sum :: -41.879310607910156
2023-01-07 09:05:29,940 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,940 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,940 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:29,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,940 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1156.237060546875
2023-01-07 09:05:29,940 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.711530327796936
2023-01-07 09:05:29,941 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.20978692173957825 param sum :: 2048.0
2023-01-07 09:05:29,941 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,941 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,941 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 1.8718643188476562
2023-01-07 09:05:29,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,942 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1155.921630859375
2023-01-07 09:05:29,942 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2119855135679245
2023-01-07 09:05:29,943 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1155.285888671875 param sum :: 1.8718643188476562
2023-01-07 09:05:29,943 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,943 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,943 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:29,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,943 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3411.114990234375
2023-01-07 09:05:29,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16783785820007324
2023-01-07 09:05:29,944 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -74.50062561035156 param sum :: 512.0
2023-01-07 09:05:29,944 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,944 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,944 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -16.520038604736328
2023-01-07 09:05:29,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,945 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3410.521728515625
2023-01-07 09:05:29,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.69139164686203
2023-01-07 09:05:29,946 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -3409.74951171875 param sum :: -16.520038604736328
2023-01-07 09:05:29,946 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,946 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,946 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:29,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,946 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2292.557373046875
2023-01-07 09:05:29,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7504000663757324
2023-01-07 09:05:29,947 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.14223559200763702 param sum :: 512.0
2023-01-07 09:05:29,947 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,947 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,947 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -14.060759544372559
2023-01-07 09:05:29,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,948 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2292.046142578125
2023-01-07 09:05:29,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03382588550448418
2023-01-07 09:05:29,948 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2292.231689453125 param sum :: -14.060759544372559
2023-01-07 09:05:29,949 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,949 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,949 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:29,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,949 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8321.830078125
2023-01-07 09:05:29,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19800317287445068
2023-01-07 09:05:29,950 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -73.95060729980469 param sum :: 2048.0
2023-01-07 09:05:29,950 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,950 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,950 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 35.38753890991211
2023-01-07 09:05:29,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8321.6845703125
2023-01-07 09:05:29,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4558258056640625
2023-01-07 09:05:29,951 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8322.322265625 param sum :: 35.38753890991211
2023-01-07 09:05:29,951 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,951 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,952 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:29,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,952 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3851.20947265625
2023-01-07 09:05:29,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8426640629768372
2023-01-07 09:05:29,953 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -74.30178833007812 param sum :: 512.0
2023-01-07 09:05:29,953 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,953 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,953 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -33.75589370727539
2023-01-07 09:05:29,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3851.1953125
2023-01-07 09:05:29,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.003621429204940796
2023-01-07 09:05:29,954 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -3851.16259765625 param sum :: -33.75589370727539
2023-01-07 09:05:29,954 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,954 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,954 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:29,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,955 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1976.3768310546875
2023-01-07 09:05:29,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1182761862874031
2023-01-07 09:05:29,956 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -74.55844116210938 param sum :: 512.0
2023-01-07 09:05:29,956 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,956 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,956 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 18.47730255126953
2023-01-07 09:05:29,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,956 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1976.0322265625
2023-01-07 09:05:29,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01589900255203247
2023-01-07 09:05:29,957 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -1976.033935546875 param sum :: 18.47730255126953
2023-01-07 09:05:29,957 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,957 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,957 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:29,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,957 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13576.9111328125
2023-01-07 09:05:29,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.428370952606201
2023-01-07 09:05:29,959 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -319.9140625 param sum :: 2048.0
2023-01-07 09:05:29,959 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,959 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,959 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 9.326160430908203
2023-01-07 09:05:29,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,959 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13580.1533203125
2023-01-07 09:05:29,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7178257703781128
2023-01-07 09:05:29,960 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 13580.7060546875 param sum :: 9.326160430908203
2023-01-07 09:05:29,960 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:05:29,960 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:05:29,961 > [DEBUG] 0 :: 7.088278293609619
2023-01-07 09:05:29,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,963 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.048553466796875
2023-01-07 09:05:29,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -328.37310791015625
2023-01-07 09:05:29,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,966 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.21151043474674225
2023-01-07 09:05:29,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,966 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -252.3050537109375
2023-01-07 09:05:29,966 > [DEBUG] 0 :: before allreduce fusion buffer :: -321.31414794921875
2023-01-07 09:05:29,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,968 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2422261238098145
2023-01-07 09:05:29,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.014221741817891598
2023-01-07 09:05:29,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,969 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.00924717728048563
2023-01-07 09:05:29,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,970 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -186.12351989746094
2023-01-07 09:05:29,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5563676357269287
2023-01-07 09:05:29,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,971 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.1893415451049805
2023-01-07 09:05:29,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09645617008209229
2023-01-07 09:05:29,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,973 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.019078046083450317
2023-01-07 09:05:29,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,973 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -111.10385131835938
2023-01-07 09:05:29,973 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2929791808128357
2023-01-07 09:05:29,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,974 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 10.999263763427734
2023-01-07 09:05:29,975 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33697277307510376
2023-01-07 09:05:29,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,976 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2500405013561249
2023-01-07 09:05:29,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,976 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -101.28843688964844
2023-01-07 09:05:29,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7994301319122314
2023-01-07 09:05:29,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,978 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.192342758178711
2023-01-07 09:05:29,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4113525450229645
2023-01-07 09:05:29,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,979 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.018236123025417328
2023-01-07 09:05:29,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,979 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -114.45109558105469
2023-01-07 09:05:29,979 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2830519676208496
2023-01-07 09:05:29,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,980 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.7719273567199707
2023-01-07 09:05:29,980 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1252104640007019
2023-01-07 09:05:29,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,981 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.034332528710365295
2023-01-07 09:05:29,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,981 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.7719273567199707
2023-01-07 09:05:29,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18309396505355835
2023-01-07 09:05:29,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,983 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -3.384838819503784
2023-01-07 09:05:29,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3619631826877594
2023-01-07 09:05:29,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,983 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.03711726889014244
2023-01-07 09:05:29,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,984 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -116.54903411865234
2023-01-07 09:05:29,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.960890293121338
2023-01-07 09:05:29,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,985 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.1494827270507812
2023-01-07 09:05:29,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5629931092262268
2023-01-07 09:05:29,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,986 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.19011032581329346
2023-01-07 09:05:29,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,986 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.7190990447998047
2023-01-07 09:05:29,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0361952781677246
2023-01-07 09:05:29,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,987 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 11.886743545532227
2023-01-07 09:05:29,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2562149167060852
2023-01-07 09:05:29,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,988 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.0216215830296278
2023-01-07 09:05:29,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,988 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -100.41289520263672
2023-01-07 09:05:29,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1091837882995605
2023-01-07 09:05:29,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,989 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -8.739212036132812
2023-01-07 09:05:29,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9388435482978821
2023-01-07 09:05:29,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,990 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0651494488120079
2023-01-07 09:05:29,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,991 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -8.739212036132812
2023-01-07 09:05:29,991 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03622329235076904
2023-01-07 09:05:29,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,992 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -0.8269480466842651
2023-01-07 09:05:29,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1630181074142456
2023-01-07 09:05:29,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.02388954535126686
2023-01-07 09:05:29,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,993 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.1712266206741333
2023-01-07 09:05:29,993 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.937510073184967
2023-01-07 09:05:29,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,994 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.88351583480835
2023-01-07 09:05:29,994 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23734603822231293
2023-01-07 09:05:29,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.02212289720773697
2023-01-07 09:05:29,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,996 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.9333558082580566
2023-01-07 09:05:29,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03907635807991028
2023-01-07 09:05:29,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,997 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 20.88983154296875
2023-01-07 09:05:29,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4998496770858765
2023-01-07 09:05:29,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,998 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.09028560668230057
2023-01-07 09:05:29,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,998 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 20.818490982055664
2023-01-07 09:05:29,998 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4655519723892212
2023-01-07 09:05:29,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:29,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:29,999 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.6753392219543457
2023-01-07 09:05:29,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3270111083984375
2023-01-07 09:05:30,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,000 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.13068629801273346
2023-01-07 09:05:30,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,000 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 2.3766655921936035
2023-01-07 09:05:30,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9449424743652344
2023-01-07 09:05:30,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,001 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1.0168781280517578
2023-01-07 09:05:30,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4218890070915222
2023-01-07 09:05:30,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,002 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.020035594701766968
2023-01-07 09:05:30,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,002 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1.1967639923095703
2023-01-07 09:05:30,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.397704154253006
2023-01-07 09:05:30,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -16.89783477783203
2023-01-07 09:05:30,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1658257246017456
2023-01-07 09:05:30,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.114117830991745
2023-01-07 09:05:30,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -16.728675842285156
2023-01-07 09:05:30,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.547255039215088
2023-01-07 09:05:30,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -4.210647106170654
2023-01-07 09:05:30,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4870428740978241
2023-01-07 09:05:30,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,007 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.10024692118167877
2023-01-07 09:05:30,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,007 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1.0447505712509155
2023-01-07 09:05:30,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0255801677703857
2023-01-07 09:05:30,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,008 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 10.397794723510742
2023-01-07 09:05:30,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.152331829071045
2023-01-07 09:05:30,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,009 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.2354189157485962
2023-01-07 09:05:30,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,009 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 10.821820259094238
2023-01-07 09:05:30,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7860171794891357
2023-01-07 09:05:30,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,010 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -41.5826416015625
2023-01-07 09:05:30,011 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6783765554428101
2023-01-07 09:05:30,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,011 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.02377268671989441
2023-01-07 09:05:30,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,012 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -36.89027404785156
2023-01-07 09:05:30,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.6701507568359375
2023-01-07 09:05:30,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,013 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -8.316282272338867
2023-01-07 09:05:30,013 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.983431100845337
2023-01-07 09:05:30,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,013 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.29481449723243713
2023-01-07 09:05:30,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,014 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -2.2739570140838623
2023-01-07 09:05:30,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6105504035949707
2023-01-07 09:05:30,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,015 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.8857468366622925
2023-01-07 09:05:30,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7381244897842407
2023-01-07 09:05:30,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,016 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.00703960657119751
2023-01-07 09:05:30,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,016 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 2.411388874053955
2023-01-07 09:05:30,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3950344324111938
2023-01-07 09:05:30,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,017 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -33.00267028808594
2023-01-07 09:05:30,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2161054611206055
2023-01-07 09:05:30,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,018 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.09509104490280151
2023-01-07 09:05:30,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,018 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -25.903650283813477
2023-01-07 09:05:30,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8999731540679932
2023-01-07 09:05:30,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,019 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.126606464385986
2023-01-07 09:05:30,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3067881762981415
2023-01-07 09:05:30,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,020 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.159851536154747
2023-01-07 09:05:30,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,021 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.924163818359375
2023-01-07 09:05:30,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2242608070373535
2023-01-07 09:05:30,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,022 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -0.49936771392822266
2023-01-07 09:05:30,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40784943103790283
2023-01-07 09:05:30,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,023 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.0465509295463562
2023-01-07 09:05:30,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,023 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -10.01641845703125
2023-01-07 09:05:30,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0669729709625244
2023-01-07 09:05:30,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,024 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -59.54943084716797
2023-01-07 09:05:30,024 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2545638084411621
2023-01-07 09:05:30,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,025 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -63.07563018798828
2023-01-07 09:05:30,025 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.118820190429688
2023-01-07 09:05:30,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,026 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 31.26664924621582
2023-01-07 09:05:30,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.275242805480957
2023-01-07 09:05:30,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,027 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.3766826093196869
2023-01-07 09:05:30,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,027 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 33.66648864746094
2023-01-07 09:05:30,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2368319034576416
2023-01-07 09:05:30,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,029 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -10.938942909240723
2023-01-07 09:05:30,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9912418127059937
2023-01-07 09:05:30,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,029 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.43611854314804077
2023-01-07 09:05:30,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,030 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.025843620300293
2023-01-07 09:05:30,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5436179637908936
2023-01-07 09:05:30,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,031 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -17.05327606201172
2023-01-07 09:05:30,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9222452044487
2023-01-07 09:05:30,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,032 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.14756543934345245
2023-01-07 09:05:30,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,032 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -3.645598888397217
2023-01-07 09:05:30,032 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.768221855163574
2023-01-07 09:05:30,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,033 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 186.141845703125
2023-01-07 09:05:30,033 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.984775543212891
2023-01-07 09:05:30,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,034 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 194.95285034179688
2023-01-07 09:05:30,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.478878021240234
2023-01-07 09:05:30,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,035 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 39.297935485839844
2023-01-07 09:05:30,035 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.0559468269348145
2023-01-07 09:05:30,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,036 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,036 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 20.99277687072754
2023-01-07 09:05:30,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.361466407775879
2023-01-07 09:05:30,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -23.642858505249023
2023-01-07 09:05:30,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.102752923965454
2023-01-07 09:05:30,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,038 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -28.376750946044922
2023-01-07 09:05:30,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1747536659240723
2023-01-07 09:05:30,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 12.528152465820312
2023-01-07 09:05:30,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2377246618270874
2023-01-07 09:05:30,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,040 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.754188537597656
2023-01-07 09:05:30,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6306700706481934
2023-01-07 09:05:30,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,041 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 9.164508819580078
2023-01-07 09:05:30,041 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6374344825744629
2023-01-07 09:05:30,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,042 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 16.753795623779297
2023-01-07 09:05:30,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.634568214416504
2023-01-07 09:05:30,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,043 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -3.67171049118042
2023-01-07 09:05:30,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06519901752471924
2023-01-07 09:05:30,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -28.676788330078125
2023-01-07 09:05:30,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.316569805145264
2023-01-07 09:05:30,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,045 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -298.9686279296875
2023-01-07 09:05:30,045 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7606847286224365
2023-01-07 09:05:30,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,046 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -315.01373291015625
2023-01-07 09:05:30,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0188307762145996
2023-01-07 09:05:30,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,047 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3.7229464054107666
2023-01-07 09:05:30,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.73865008354187
2023-01-07 09:05:30,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,048 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.8344672322273254
2023-01-07 09:05:30,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,048 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -15.35069751739502
2023-01-07 09:05:30,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.508655548095703
2023-01-07 09:05:30,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,049 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -51.40156555175781
2023-01-07 09:05:30,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.011866569519043
2023-01-07 09:05:30,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,050 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -36.30675506591797
2023-01-07 09:05:30,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1863081455230713
2023-01-07 09:05:30,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,051 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -126.10826873779297
2023-01-07 09:05:30,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.475547790527344
2023-01-07 09:05:30,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,052 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -119.82245635986328
2023-01-07 09:05:30,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.85599136352539
2023-01-07 09:05:30,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 22.341541290283203
2023-01-07 09:05:30,054 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.895854115486145
2023-01-07 09:05:30,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,054 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.541731834411621
2023-01-07 09:05:30,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,055 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 52.48924255371094
2023-01-07 09:05:30,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.107540130615234
2023-01-07 09:05:30,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -38.94620132446289
2023-01-07 09:05:30,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.147291660308838
2023-01-07 09:05:30,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,057 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -18.647192001342773
2023-01-07 09:05:30,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.83480453491211
2023-01-07 09:05:30,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -69.50262451171875
2023-01-07 09:05:30,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.417939186096191
2023-01-07 09:05:30,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,059 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.2204132080078125
2023-01-07 09:05:30,059 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.500991821289062
2023-01-07 09:05:30,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,060 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -234.45062255859375
2023-01-07 09:05:30,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.819915771484375
2023-01-07 09:05:30,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,061 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.2018358707427979
2023-01-07 09:05:30,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,061 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -150.54666137695312
2023-01-07 09:05:30,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.39952850341797
2023-01-07 09:05:30,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.4622802734375
2023-01-07 09:05:30,062 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.245933532714844
2023-01-07 09:05:30,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,063 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.9565145969390869
2023-01-07 09:05:30,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -20.28318977355957
2023-01-07 09:05:30,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -246.21926879882812
2023-01-07 09:05:30,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.400875091552734
2023-01-07 09:05:30,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -246.21926879882812
2023-01-07 09:05:30,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.051788330078125
2023-01-07 09:05:30,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -162.2281036376953
2023-01-07 09:05:30,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.365772247314453
2023-01-07 09:05:30,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,067 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -166.6903076171875
2023-01-07 09:05:30,067 > [DEBUG] 0 :: before allreduce fusion buffer :: -77.40103149414062
2023-01-07 09:05:30,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,067 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 2.4669389724731445
2023-01-07 09:05:30,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,068 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.79788208007812
2023-01-07 09:05:30,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.37337875366211
2023-01-07 09:05:30,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,069 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -382.2737121582031
2023-01-07 09:05:30,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.620372772216797
2023-01-07 09:05:30,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,070 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.9054864645004272
2023-01-07 09:05:30,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,070 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -63.327545166015625
2023-01-07 09:05:30,070 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.51164627075195
2023-01-07 09:05:30,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,071 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -543.570556640625
2023-01-07 09:05:30,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.617942810058594
2023-01-07 09:05:30,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,072 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -125.25363159179688
2023-01-07 09:05:30,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,072 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -200.9771728515625
2023-01-07 09:05:30,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.647178649902344
2023-01-07 09:05:30,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,073 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -593.2543334960938
2023-01-07 09:05:30,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.157066345214844
2023-01-07 09:05:30,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,074 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 54.62349319458008
2023-01-07 09:05:30,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.90697479248047
2023-01-07 09:05:30,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,075 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -682.60107421875
2023-01-07 09:05:30,076 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.102073669433594
2023-01-07 09:05:30,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,077 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -2.4211840629577637
2023-01-07 09:05:30,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,077 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -119.76900482177734
2023-01-07 09:05:30,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,077 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -833.571044921875
2023-01-07 09:05:30,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -229.06851196289062
2023-01-07 09:05:30,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -101.61378479003906
2023-01-07 09:05:30,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,079 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -812.50537109375
2023-01-07 09:05:30,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.311006546020508
2023-01-07 09:05:30,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,080 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -67.91182708740234
2023-01-07 09:05:30,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.724824905395508
2023-01-07 09:05:30,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,081 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -900.39208984375
2023-01-07 09:05:30,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.77065086364746
2023-01-07 09:05:30,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,082 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -540.5798950195312
2023-01-07 09:05:30,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -230.863525390625
2023-01-07 09:05:30,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -88.7984848022461
2023-01-07 09:05:30,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,083 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -234.5243682861328
2023-01-07 09:05:30,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.59096908569336
2023-01-07 09:05:30,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,084 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 0.7444391250610352
2023-01-07 09:05:30,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,084 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -230.73153686523438
2023-01-07 09:05:30,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.777267456054688
2023-01-07 09:05:30,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,085 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -539.9155883789062
2023-01-07 09:05:30,086 > [DEBUG] 0 :: before allreduce fusion buffer :: -107.319580078125
2023-01-07 09:05:30,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:05:30,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:05:30,086 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -915.9730834960938
2023-01-07 09:05:30,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -95.6900634765625
