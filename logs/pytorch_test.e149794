[00;33m 	'cuda/11.3' does not supports the {CUDA_MPI}. [0m
[00;34m 	{CUDA_MPI} is only supported in cuda 11.4 version. [0m
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
Traceback (most recent call last):
  File "profile_gpt2.py", line 74, in <module>
    sizes, times  = comm_profiler.benchmark()
  File "/scratch/hpc72a03/shardscheduler/profiling.py", line 266, in benchmark
    sizes = small_sizes + large_sizes
NameError: name 'small_sizes' is not defined
srun: error: gpu20: tasks 0,2-3: Exited with exit code 1
srun: error: gpu21: tasks 4-5,7: Exited with exit code 1
srun: error: gpu20: task 1: Exited with exit code 1
srun: error: gpu21: task 6: Exited with exit code 1
