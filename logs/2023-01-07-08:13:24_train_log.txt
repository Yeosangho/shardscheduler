2023-01-07 08:13:31,083 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:13:31,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,122 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 1.1206388473510742
2023-01-07 08:13:31,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,122 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:13:31,122 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:31,122 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:31,122 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:13:31,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,966 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:31,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,967 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:31,967 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:31,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:31,967 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:13:31,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,969 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.129781723022461
2023-01-07 08:13:31,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,969 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:13:31,969 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:31,969 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:31,970 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:13:31,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,971 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:31,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,971 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:31,971 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:31,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:31,971 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:13:31,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,972 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 8.42106819152832
2023-01-07 08:13:31,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:31,973 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:13:31,973 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:31,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:31,973 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:13:31,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,011 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:32,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,011 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:32,011 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,011 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,011 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:13:32,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,013 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -26.32551383972168
2023-01-07 08:13:32,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,013 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,013 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,013 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,013 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:13:32,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,015 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,015 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,015 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,015 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,015 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:13:32,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,016 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 16.983993530273438
2023-01-07 08:13:32,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,017 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,017 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,017 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,017 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:13:32,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,018 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,018 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,018 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,018 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,018 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:13:32,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,020 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 1.9173755645751953
2023-01-07 08:13:32,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,020 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,020 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,020 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,020 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:13:32,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,021 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:32,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,022 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:32,022 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,022 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,022 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:13:32,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,023 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: 12.11391830444336
2023-01-07 08:13:32,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,023 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:13:32,023 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,024 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:13:32,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,025 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:32,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,025 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:32,025 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,025 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,025 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:13:32,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,026 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -2.4323062896728516
2023-01-07 08:13:32,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,027 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,027 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,027 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,027 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:13:32,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,028 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,028 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,029 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,029 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:13:32,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,030 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 3.261503219604492
2023-01-07 08:13:32,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,030 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,030 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,030 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,030 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:13:32,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,031 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:32,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,032 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:32,032 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,032 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:13:32,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,033 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -4.521993637084961
2023-01-07 08:13:32,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,034 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:13:32,034 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,034 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,034 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:13:32,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,035 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:13:32,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,035 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:13:32,035 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,035 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,035 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:13:32,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,037 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 28.01500701904297
2023-01-07 08:13:32,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,037 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:13:32,037 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,037 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,037 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:13:32,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,038 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,039 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,039 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,039 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,039 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:13:32,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,040 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -7.174853324890137
2023-01-07 08:13:32,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,040 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:13:32,040 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,040 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,041 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:13:32,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,042 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,042 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,042 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,042 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,042 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:13:32,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,044 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 15.166845321655273
2023-01-07 08:13:32,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,044 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:13:32,044 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,044 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,044 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:13:32,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,045 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,046 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,046 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,046 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,046 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:13:32,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,047 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -14.443331718444824
2023-01-07 08:13:32,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,048 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,048 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,048 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,048 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:13:32,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,049 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,049 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,049 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,049 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,049 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:13:32,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,050 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 9.496797561645508
2023-01-07 08:13:32,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,051 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:13:32,051 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,051 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,051 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:13:32,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,052 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,053 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,053 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,053 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,053 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:13:32,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,054 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -7.85957145690918
2023-01-07 08:13:32,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,054 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,054 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,054 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,055 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:13:32,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,056 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,056 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,056 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,056 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,056 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:13:32,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,057 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 41.2872428894043
2023-01-07 08:13:32,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,058 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:13:32,058 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,058 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,058 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:13:32,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,059 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,060 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,060 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,060 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,060 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:13:32,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,061 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: 7.092031002044678
2023-01-07 08:13:32,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,061 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,061 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,061 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,061 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:13:32,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,062 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,063 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,063 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,063 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,063 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:13:32,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,064 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 53.722877502441406
2023-01-07 08:13:32,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,065 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,065 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,065 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,065 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:13:32,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,066 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,066 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,066 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,066 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,066 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:13:32,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,068 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 4.542716979980469
2023-01-07 08:13:32,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,068 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:13:32,068 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,068 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,068 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:13:32,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,069 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,070 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,070 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,070 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,070 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:13:32,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,071 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.481887817382812
2023-01-07 08:13:32,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,071 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,071 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,071 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,071 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:13:32,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,072 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,073 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,073 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,073 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,073 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:13:32,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,074 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 29.27133560180664
2023-01-07 08:13:32,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,075 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,075 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,075 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,075 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:13:32,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,076 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,076 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,076 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,076 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,076 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:13:32,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,078 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -9.987442016601562
2023-01-07 08:13:32,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,078 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:13:32,078 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,078 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,078 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:13:32,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,079 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:13:32,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,080 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:13:32,080 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,080 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,080 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:13:32,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,081 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 28.147680282592773
2023-01-07 08:13:32,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,081 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:13:32,081 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,081 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,081 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:13:32,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,082 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,083 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,083 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,083 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,083 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:13:32,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,084 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -23.213808059692383
2023-01-07 08:13:32,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,085 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:13:32,085 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,085 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,085 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:13:32,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,086 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,086 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,086 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,086 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,087 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:13:32,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,088 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.716934204101562
2023-01-07 08:13:32,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,088 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,088 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,088 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,088 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:13:32,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,089 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,090 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,090 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,090 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,090 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:13:32,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,091 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 6.201474189758301
2023-01-07 08:13:32,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,092 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,092 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,092 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,092 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:13:32,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,093 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,093 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,093 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,093 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,093 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:13:32,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,094 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.194400787353516
2023-01-07 08:13:32,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,095 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:13:32,095 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,095 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,095 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:13:32,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,096 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,096 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,097 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,097 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,097 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:13:32,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,098 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -49.706459045410156
2023-01-07 08:13:32,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,098 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,098 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,098 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,098 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:13:32,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,099 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,100 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,100 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,100 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,100 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:13:32,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,101 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -47.76422119140625
2023-01-07 08:13:32,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,102 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,102 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,102 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,102 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:13:32,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,103 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,103 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,104 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,104 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,104 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:13:32,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,105 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 18.3553466796875
2023-01-07 08:13:32,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,105 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,105 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,105 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,105 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:13:32,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,106 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,107 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,107 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,107 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,107 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:13:32,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,108 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -61.87196350097656
2023-01-07 08:13:32,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,108 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,109 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,109 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,109 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:13:32,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,110 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,110 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,110 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,110 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,110 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:13:32,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,111 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: -32.59107971191406
2023-01-07 08:13:32,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,112 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,112 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,112 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,112 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:13:32,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,113 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,113 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,113 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,113 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,114 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:13:32,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,115 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 24.820240020751953
2023-01-07 08:13:32,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,115 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,115 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,115 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,115 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:13:32,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,116 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,117 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,117 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,117 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,117 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:13:32,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,118 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -93.63822937011719
2023-01-07 08:13:32,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,118 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,118 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,118 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,118 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:13:32,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,120 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,120 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,120 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,120 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,120 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:13:32,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,121 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: 3.948330879211426
2023-01-07 08:13:32,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,122 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,122 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,122 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,122 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:13:32,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,123 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,123 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,123 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,123 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,123 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:13:32,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,125 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -3.615861415863037
2023-01-07 08:13:32,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,125 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,125 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,125 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,125 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:13:32,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,126 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,127 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,127 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,127 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,127 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:13:32,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,128 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 2.705291748046875
2023-01-07 08:13:32,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,128 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,128 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,128 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,128 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:13:32,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,129 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,130 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,130 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,130 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,130 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:13:32,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,131 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 10.411664962768555
2023-01-07 08:13:32,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,132 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,132 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,132 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,132 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:13:32,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,133 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,133 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,133 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,133 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,133 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:13:32,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,134 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.172096252441406
2023-01-07 08:13:32,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,135 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,135 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,135 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,135 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:13:32,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,136 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,136 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,136 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,136 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,137 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:13:32,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,138 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -28.02915382385254
2023-01-07 08:13:32,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,138 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,138 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,138 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,138 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:13:32,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,139 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,140 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,140 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,140 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,140 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:13:32,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,141 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 6.629621982574463
2023-01-07 08:13:32,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,141 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:13:32,141 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,142 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,142 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:13:32,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,143 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:13:32,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,143 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:13:32,143 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,143 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,143 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:13:32,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,144 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -35.49064636230469
2023-01-07 08:13:32,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,145 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:13:32,145 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,145 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,145 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:13:32,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,146 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:13:32,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,146 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:13:32,146 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,146 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,146 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:13:32,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,148 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -34.04503631591797
2023-01-07 08:13:32,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,148 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:13:32,148 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,148 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,148 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:13:32,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,149 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,150 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,150 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,150 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,150 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:13:32,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,151 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -29.863983154296875
2023-01-07 08:13:32,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,151 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:13:32,151 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,151 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,151 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:13:32,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,153 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,153 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,153 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,153 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,153 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:13:32,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,154 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -60.7851448059082
2023-01-07 08:13:32,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,155 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:13:32,155 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,155 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,155 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:13:32,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,156 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:13:32,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,156 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:13:32,156 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,156 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,156 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:13:32,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,157 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 65.61173248291016
2023-01-07 08:13:32,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,158 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:13:32,158 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,158 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,158 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:13:32,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,159 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:13:32,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,159 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:13:32,159 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,159 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,160 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:13:32,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,161 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 20.577342987060547
2023-01-07 08:13:32,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,161 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:13:32,161 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,161 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,161 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:13:32,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,162 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,163 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,163 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,163 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,163 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:13:32,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,164 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: 12.426922798156738
2023-01-07 08:13:32,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,164 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:13:32,165 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,165 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,165 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:13:32,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,166 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,166 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,166 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,166 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,166 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:13:32,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,167 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.845649719238281
2023-01-07 08:13:32,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,168 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:13:32,168 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,168 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,168 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:13:32,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,169 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:13:32,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,169 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:13:32,170 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,170 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,170 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:13:32,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,171 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -17.466018676757812
2023-01-07 08:13:32,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,171 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:13:32,171 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,171 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,171 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:13:32,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,172 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,173 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,173 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,173 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,173 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:13:32,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,174 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 18.535490036010742
2023-01-07 08:13:32,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,174 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:13:32,175 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,175 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,175 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:13:32,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,176 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:13:32,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,176 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:13:32,176 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,176 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,176 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:13:32,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,177 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 4.210667133331299
2023-01-07 08:13:32,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,178 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:13:32,178 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,178 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,178 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:13:32,178 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,179 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:13:32,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,179 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:13:32,179 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,179 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,179 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:13:32,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,181 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: -1.2663183212280273
2023-01-07 08:13:32,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:13:32,181 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:13:32,181 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:13:32,181 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:13:32,181 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:13:32,183 > [DEBUG] 0 :: 7.553402423858643
2023-01-07 08:13:32,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:13:32,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:13:32,188 > [DEBUG] 0 :: before allreduce fusion buffer :: -364.4566345214844
