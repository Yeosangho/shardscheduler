CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 335.44 MiB already allocated; 7.57 GiB free; 409.60 MiB allowed; 406.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONFTraceback (most recent call last):
  File "/workspace/shardscheduler/torch_scheduler.py", line 312, in _poll_FSDP
    self.run_schedule(self.schedules, init=False)
  File "/workspace/shardscheduler/torch_scheduler.py", line 470, in run_schedule
    remains = self.bucket.push(params=grad_chunks,
  File "/workspace/shardscheduler/bucket.py", line 72, in push
    stacked_input = torch.stack(params).view(self.world_size, -1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 335.44 MiB already allocated; 7.57 GiB free; 409.60 MiB allowed; 406.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 341.10 MiB already allocated; 7.57 GiB free; 409.60 MiB allowed; 406.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONFTraceback (most recent call last):
  File "/workspace/shardscheduler/torch_scheduler.py", line 312, in _poll_FSDP
    self.run_schedule(self.schedules, init=False)
  File "/workspace/shardscheduler/torch_scheduler.py", line 470, in run_schedule
    remains = self.bucket.push(params=grad_chunks,
  File "/workspace/shardscheduler/bucket.py", line 72, in push
    stacked_input = torch.stack(params).view(self.world_size, -1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 341.10 MiB already allocated; 7.57 GiB free; 409.60 MiB allowed; 406.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 334.31 MiB already allocated; 7.59 GiB free; 409.60 MiB allowed; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONFTraceback (most recent call last):
  File "/workspace/shardscheduler/torch_scheduler.py", line 312, in _poll_FSDP
    self.run_schedule(self.schedules, init=False)
  File "/workspace/shardscheduler/torch_scheduler.py", line 470, in run_schedule
    remains = self.bucket.push(params=grad_chunks,
  File "/workspace/shardscheduler/bucket.py", line 72, in push
    stacked_input = torch.stack(params).view(self.world_size, -1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 334.31 MiB already allocated; 7.59 GiB free; 409.60 MiB allowed; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 343.96 MiB already allocated; 7.59 GiB free; 409.60 MiB allowed; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONFTraceback (most recent call last):
  File "/workspace/shardscheduler/torch_scheduler.py", line 312, in _poll_FSDP
    self.run_schedule(self.schedules, init=False)
  File "/workspace/shardscheduler/torch_scheduler.py", line 470, in run_schedule
    remains = self.bucket.push(params=grad_chunks,
  File "/workspace/shardscheduler/bucket.py", line 72, in push
    stacked_input = torch.stack(params).view(self.world_size, -1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.78 GiB total capacity; 343.96 MiB already allocated; 7.59 GiB free; 409.60 MiB allowed; 392.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
